{"pages":[{"url":"pages/deployment.html","text":"Simple Semantic Analysis ~ how happy are you? ~","tags":"pages","title":"Deployment"},{"url":"pages/quest.html","text":"15-day Gokyo Everest Base Camp Trek 2018","tags":"pages","title":"Quest"},{"url":"My-first-GAN-using-CelebA-data.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } Everyone has been lately talking about Generative Adversarial Networks, or more famously known as GAN. My colleagues, interview candidates, my manager, conferences literally EVERYONE. It seems like the GANs becomes required knowledge for data scientists in bay area. It also seems that GANs are cool: GANs can generate new celebility face images, generate creative arts or generate the next frame of the video images. AI can think by itself with the power of GAN. In this blog I will learn what's so great about GAN. The gif above is the outputed images from my first GAN. The reproduced images are blurr and seem to need more training; I only trained my GAN for about 1 hour. Nevertheless, this is a good start. My focus in this blog will be on its simple implementation rather than its theoretical degails. I will use famous and popular celebA data to train GANs, and generate celebrity face images. I got lots of good idea about its simple implemenation from GAN: A Beginner's Guide to Generative Adversarial Networks so I highly recommend you to read this great blog first. Reference GAN: A Beginner's Guide to Generative Adversarial Networks Generative Adversarial Nets in TensorFlow Photo Editing with Generative Adversarial Networks Part 1 Photo Editing with Generative Adversarial Networks Part 2 Generative Adversarial Networks and Their Applications In [1]: ## load modules import matplotlib.pyplot as plt import os , time import numpy as np from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array import tensorflow as tf from keras.backend.tensorflow_backend import set_session os . environ [ \"CUDA_DEVICE_ORDER\" ] = \"PCI_BUS_ID\" config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"1\" set_session ( tf . Session ( config = config )) /home/bur2pal/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Using TensorFlow backend. Load celebA data. As my previous post shows, celebA contains over 202,599 images. I will use 200,000 images to train GANs. The original image is of the shape (218, 178, 3). All images are resized to smaller shape for the sake of easier computation. I am shrinking the image size pretty small here because otherwise, GAN requires lots of computation time. In [2]: dir_data = \"data/img_align_celeba/\" Ntrain = 200000 Ntest = 100 nm_imgs = np . sort ( os . listdir ( dir_data )) ## name of the jpg files for training set nm_imgs_train = nm_imgs [: Ntrain ] ## name of the jpg files for the testing data nm_imgs_test = nm_imgs [ Ntrain : Ntrain + Ntest ] img_shape = ( 32 , 32 , 3 ) def get_npdata ( nm_imgs_train ): X_train = [] for i , myid in enumerate ( nm_imgs_train ): image = load_img ( dir_data + \"/\" + myid , target_size = img_shape [: 2 ]) image = img_to_array ( image ) / 255.0 X_train . append ( image ) X_train = np . array ( X_train ) return ( X_train ) X_train = get_npdata ( nm_imgs_train ) print ( \"X_train.shape = {}\" . format ( X_train . shape )) X_test = get_npdata ( nm_imgs_test ) print ( \"X_test.shape = {}\" . format ( X_test . shape )) X_train.shape = (200000, 32, 32, 3) X_test.shape = (100, 32, 32, 3) Plot the resized input images I hope that our generators can generate images similar to these! In [3]: fig = plt . figure ( figsize = ( 30 , 10 )) nplot = 7 for count in range ( 1 , nplot ): ax = fig . add_subplot ( 1 , nplot , count ) ax . imshow ( X_train [ count ]) plt . show () Define GAN GAN contains two networks which has two competing objectives: Generator : the generator generates new data instances that are \"similar\" to the training data, in our case celebA images. Generator takes random latent vector and output a \"fake\" image of the same size as our reshaped celebA image. Discriminator : the discriminator evaluate the authenticity of provided images; it classifies the images from the generator and the original image. Discriminator takes true of fake images and output the probability estimate ranging between 0 and 1. \\begin{array}{rcll} \\textrm{Generator(latent)} &\\rightarrow& \\textrm{image}\\\\ \\textrm{Discriminator(image)}&\\rightarrow& \\textrm{0 (fake) /1 (true)} \\end{array} Define generator In [4]: import numpy as np from keras import layers , models from keras.optimizers import Adam ## optimizer #optimizer = Adam(0.0002, 0.5) optimizer = Adam ( 0.00007 , 0.5 ) def build_generator ( img_shape , noise_shape = ( 100 ,)): ''' noise_shape : the dimension of the input vector for the generator img_shape : the dimension of the output ''' ## latent variable as input input_noise = layers . Input ( shape = noise_shape ) d = layers . Dense ( 1024 , activation = \"relu\" )( input_noise ) d = layers . Dense ( 1024 , activation = \"relu\" )( input_noise ) d = layers . Dense ( 128 * 8 * 8 , activation = \"relu\" )( d ) d = layers . Reshape (( 8 , 8 , 128 ))( d ) d = layers . Conv2DTranspose ( 128 , kernel_size = ( 2 , 2 ) , strides = ( 2 , 2 ) , use_bias = False )( d ) d = layers . Conv2D ( 64 , ( 1 , 1 ) , activation = 'relu' , padding = 'same' , name = \"block_4\" )( d ) ## 16,16 d = layers . Conv2DTranspose ( 32 , kernel_size = ( 2 , 2 ) , strides = ( 2 , 2 ) , use_bias = False )( d ) d = layers . Conv2D ( 64 , ( 1 , 1 ) , activation = 'relu' , padding = 'same' , name = \"block_5\" )( d ) ## 32,32 if img_shape [ 0 ] == 64 : d = layers . Conv2DTranspose ( 32 , kernel_size = ( 2 , 2 ) , strides = ( 2 , 2 ) , use_bias = False )( d ) d = layers . Conv2D ( 64 , ( 1 , 1 ) , activation = 'relu' , padding = 'same' , name = \"block_6\" )( d ) ## 64,64 img = layers . Conv2D ( 3 , ( 1 , 1 ) , activation = 'sigmoid' , padding = 'same' , name = \"final_block\" )( d ) ## 32, 32 model = models . Model ( input_noise , img ) model . summary () return ( model ) ## Set the dimension of latent variables to be 100 noise_shape = ( 100 ,) generator = build_generator ( img_shape , noise_shape = noise_shape ) generator . compile ( loss = 'binary_crossentropy' , optimizer = optimizer ) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 100) 0 _________________________________________________________________ dense_2 (Dense) (None, 1024) 103424 _________________________________________________________________ dense_3 (Dense) (None, 8192) 8396800 _________________________________________________________________ reshape_1 (Reshape) (None, 8, 8, 128) 0 _________________________________________________________________ conv2d_transpose_1 (Conv2DTr (None, 16, 16, 128) 65536 _________________________________________________________________ block_4 (Conv2D) (None, 16, 16, 64) 8256 _________________________________________________________________ conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32) 8192 _________________________________________________________________ block_5 (Conv2D) (None, 32, 32, 64) 2112 _________________________________________________________________ final_block (Conv2D) (None, 32, 32, 3) 195 ================================================================= Total params: 8,584,515 Trainable params: 8,584,515 Non-trainable params: 0 _________________________________________________________________ Take a look at the generatied images BEFORE any training. As expectedly, the image is nothing like celebA. Our generator knows nothing about it and it outputs some random noise in a weak attempt to trick the discriminator. Let's see how much generator can learn from the celebA training data to generate \"fake celebA images\"! In [5]: def get_noise ( nsample = 1 , nlatent_dim = 100 ): noise = np . random . normal ( 0 , 1 , ( nsample , nlatent_dim )) return ( noise ) def plot_generated_images ( noise , path_save = None , titleadd = \"\" ): imgs = generator . predict ( noise ) fig = plt . figure ( figsize = ( 40 , 10 )) for i , img in enumerate ( imgs ): ax = fig . add_subplot ( 1 , nsample , i + 1 ) ax . imshow ( img ) fig . suptitle ( \"Generated images \" + titleadd , fontsize = 30 ) if path_save is not None : plt . savefig ( path_save , bbox_inches = 'tight' , pad_inches = 0 ) plt . close () else : plt . show () nsample = 4 noise = get_noise ( nsample = nsample , nlatent_dim = noise_shape [ 0 ]) plot_generated_images ( noise ) Define discriminator In [6]: def build_discriminator ( img_shape , noutput = 1 ): input_img = layers . Input ( shape = img_shape ) x = layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block1_conv1' )( input_img ) x = layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block1_conv2' )( x ) x = layers . MaxPooling2D (( 2 , 2 ), strides = ( 2 , 2 ), name = 'block1_pool' )( x ) x = layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block2_conv1' )( x ) x = layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block2_conv2' )( x ) x = layers . MaxPooling2D (( 2 , 2 ), strides = ( 2 , 2 ), name = 'block2_pool' )( x ) x = layers . Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block4_conv1' )( x ) x = layers . Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block4_conv2' )( x ) x = layers . MaxPooling2D (( 2 , 2 ), strides = ( 1 , 1 ), name = 'block4_pool' )( x ) x = layers . Flatten ()( x ) x = layers . Dense ( 1024 , activation = \"relu\" )( x ) out = layers . Dense ( noutput , activation = 'sigmoid' )( x ) model = models . Model ( input_img , out ) return model discriminator = build_discriminator ( img_shape ) discriminator . compile ( loss = 'binary_crossentropy' , optimizer = optimizer , metrics = [ 'accuracy' ]) discriminator . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) (None, 32, 32, 3) 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ block1_conv2 (Conv2D) (None, 32, 32, 32) 9248 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 16, 16, 32) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ block2_conv2 (Conv2D) (None, 16, 16, 64) 36928 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 8, 8, 64) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 8, 8, 128) 73856 _________________________________________________________________ block4_conv2 (Conv2D) (None, 8, 8, 128) 147584 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 7, 7, 128) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 6272) 0 _________________________________________________________________ dense_4 (Dense) (None, 1024) 6423552 _________________________________________________________________ dense_5 (Dense) (None, 1) 1025 ================================================================= Total params: 6,711,585 Trainable params: 6,711,585 Non-trainable params: 0 _________________________________________________________________ Combined model In 32Ã—32 8-bit RGB images, there are $2&#94;{3x8x32x32}=2&#94;{24576}$ possible arrangements of the pixel values in those images. The number of parameters of Generator and Dicriminator are together much less than this number. noise -> generator -> discriminator This combined model will share the same weights as discriminator and generator. In [7]: z = layers . Input ( shape = noise_shape ) img = generator ( z ) # For the combined model we will only train the generator discriminator . trainable = False # The valid takes generated images as input and determines validity valid = discriminator ( img ) # The combined model (stacked generator and discriminator) takes # noise as input => generates images => determines validity combined = models . Model ( z , valid ) combined . compile ( loss = 'binary_crossentropy' , optimizer = optimizer ) combined . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) (None, 100) 0 _________________________________________________________________ model_1 (Model) (None, 32, 32, 3) 8584515 _________________________________________________________________ model_2 (Model) (None, 1) 6711585 ================================================================= Total params: 15,296,100 Trainable params: 8,584,515 Non-trainable params: 6,711,585 _________________________________________________________________ Training When you train the discriminator, hold the generator values constant; and when you train the generator, hold the discriminator constant. In [8]: def train ( models , X_train , noise_plot , dir_result = \"/result/\" , epochs = 10000 , batch_size = 128 ): ''' models : tuple containins three tensors, (combined, discriminator, generator) X_train : np.array containing images (Nsample, height, width, Nchannels) noise_plot : np.array of size (Nrandom_sample_to_plot, hidden unit length) dir_result : the location where the generated plots for noise_plot are saved ''' combined , discriminator , generator = models nlatent_dim = noise_plot . shape [ 1 ] half_batch = int ( batch_size / 2 ) history = [] for epoch in range ( epochs ): # --------------------- # Train Discriminator # --------------------- # Select a random half batch of images idx = np . random . randint ( 0 , X_train . shape [ 0 ], half_batch ) imgs = X_train [ idx ] noise = get_noise ( half_batch , nlatent_dim ) # Generate a half batch of new images gen_imgs = generator . predict ( noise ) # Train the discriminator q: better to mix them together? d_loss_real = discriminator . train_on_batch ( imgs , np . ones (( half_batch , 1 ))) d_loss_fake = discriminator . train_on_batch ( gen_imgs , np . zeros (( half_batch , 1 ))) d_loss = 0.5 * np . add ( d_loss_real , d_loss_fake ) # --------------------- # Train Generator # --------------------- noise = get_noise ( batch_size , nlatent_dim ) # The generator wants the discriminator to label the generated samples # as valid (ones) valid_y = ( np . array ([ 1 ] * batch_size )) . reshape ( batch_size , 1 ) # Train the generator g_loss = combined . train_on_batch ( noise , valid_y ) history . append ({ \"D\" : d_loss [ 0 ], \"G\" : g_loss }) if epoch % 100 == 0 : # Plot the progress print ( \"Epoch {:05.0f} [D loss: {:4.3f}, acc.: {:05.1f}%] [G loss: {:4.3f}]\" . format ( epoch , d_loss [ 0 ], 100 * d_loss [ 1 ], g_loss )) if epoch % int ( epochs / 100 ) == 0 : plot_generated_images ( noise_plot , path_save = dir_result + \"/image_{:05.0f}.png\" . format ( epoch ), titleadd = \"Epoch {}\" . format ( epoch )) if epoch % 1000 == 0 : plot_generated_images ( noise_plot , titleadd = \"Epoch {}\" . format ( epoch )) return ( history ) dir_result = \"./result_GAN/\" try : os . mkdir ( dir_result ) except : pass start_time = time . time () _models = combined , discriminator , generator history = train ( _models , X_train , noise , dir_result = dir_result , epochs = 20000 , batch_size = 128 * 8 ) end_time = time . time () print ( \"-\" * 10 ) print ( \"Time took: {:4.2f} min\" . format (( end_time - start_time ) / 60 )) /home/bur2pal/Modules/keras/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ? 'Discrepancy between trainable weights and collected trainable' Epoch 00000 [D loss: 0.732, acc.: 038.4%] [G loss: 0.648] Epoch 00100 [D loss: 0.098, acc.: 098.2%] [G loss: 3.399] Epoch 00200 [D loss: 0.272, acc.: 089.1%] [G loss: 2.390] Epoch 00300 [D loss: 0.271, acc.: 088.7%] [G loss: 2.464] Epoch 00400 [D loss: 0.282, acc.: 089.1%] [G loss: 2.234] Epoch 00500 [D loss: 0.185, acc.: 093.0%] [G loss: 2.780] Epoch 00600 [D loss: 0.213, acc.: 094.7%] [G loss: 3.204] Epoch 00700 [D loss: 0.161, acc.: 095.7%] [G loss: 3.229] Epoch 00800 [D loss: 0.168, acc.: 095.3%] [G loss: 4.831] Epoch 00900 [D loss: 0.189, acc.: 093.6%] [G loss: 2.705] Epoch 01000 [D loss: 0.295, acc.: 089.1%] [G loss: 2.081] Epoch 01100 [D loss: 0.232, acc.: 092.6%] [G loss: 2.619] Epoch 01200 [D loss: 0.158, acc.: 093.7%] [G loss: 3.485] Epoch 01300 [D loss: 0.284, acc.: 089.9%] [G loss: 3.901] Epoch 01400 [D loss: 0.129, acc.: 095.8%] [G loss: 3.299] Epoch 01500 [D loss: 0.345, acc.: 084.4%] [G loss: 3.596] Epoch 01600 [D loss: 0.346, acc.: 086.7%] [G loss: 2.375] Epoch 01700 [D loss: 0.151, acc.: 095.8%] [G loss: 3.398] Epoch 01800 [D loss: 0.169, acc.: 093.9%] [G loss: 3.201] Epoch 01900 [D loss: 0.176, acc.: 093.9%] [G loss: 2.934] Epoch 02000 [D loss: 0.169, acc.: 094.4%] [G loss: 3.022] Epoch 02100 [D loss: 0.170, acc.: 094.0%] [G loss: 3.029] Epoch 02200 [D loss: 0.346, acc.: 085.3%] [G loss: 2.539] Epoch 02300 [D loss: 0.308, acc.: 087.7%] [G loss: 2.795] Epoch 02400 [D loss: 0.372, acc.: 084.0%] [G loss: 2.897] Epoch 02500 [D loss: 0.303, acc.: 088.2%] [G loss: 2.476] Epoch 02600 [D loss: 0.331, acc.: 086.3%] [G loss: 2.522] Epoch 02700 [D loss: 0.430, acc.: 081.0%] [G loss: 2.143] Epoch 02800 [D loss: 0.409, acc.: 081.1%] [G loss: 2.220] Epoch 02900 [D loss: 0.440, acc.: 079.1%] [G loss: 2.105] Epoch 03000 [D loss: 0.419, acc.: 081.3%] [G loss: 2.262] Epoch 03100 [D loss: 0.487, acc.: 078.3%] [G loss: 1.833] Epoch 03200 [D loss: 0.497, acc.: 075.9%] [G loss: 1.826] Epoch 03300 [D loss: 0.433, acc.: 081.2%] [G loss: 1.919] Epoch 03400 [D loss: 0.371, acc.: 085.0%] [G loss: 2.259] Epoch 03500 [D loss: 0.453, acc.: 078.8%] [G loss: 2.058] Epoch 03600 [D loss: 0.471, acc.: 078.8%] [G loss: 1.663] Epoch 03700 [D loss: 0.439, acc.: 080.0%] [G loss: 1.733] Epoch 03800 [D loss: 0.499, acc.: 077.1%] [G loss: 1.587] Epoch 03900 [D loss: 0.452, acc.: 079.3%] [G loss: 1.616] Epoch 04000 [D loss: 0.502, acc.: 076.2%] [G loss: 1.670] Epoch 04100 [D loss: 0.524, acc.: 074.2%] [G loss: 1.557] Epoch 04200 [D loss: 0.468, acc.: 078.7%] [G loss: 1.523] Epoch 04300 [D loss: 0.525, acc.: 072.5%] [G loss: 1.548] Epoch 04400 [D loss: 0.502, acc.: 075.5%] [G loss: 1.492] Epoch 04500 [D loss: 0.473, acc.: 077.1%] [G loss: 1.679] Epoch 04600 [D loss: 0.478, acc.: 077.9%] [G loss: 1.628] Epoch 04700 [D loss: 0.548, acc.: 072.3%] [G loss: 1.359] Epoch 04800 [D loss: 0.538, acc.: 072.9%] [G loss: 1.341] Epoch 04900 [D loss: 0.519, acc.: 075.3%] [G loss: 1.320] Epoch 05000 [D loss: 0.452, acc.: 079.0%] [G loss: 1.521] Epoch 05100 [D loss: 0.497, acc.: 075.9%] [G loss: 1.427] Epoch 05200 [D loss: 0.497, acc.: 075.6%] [G loss: 1.370] Epoch 05300 [D loss: 0.481, acc.: 077.1%] [G loss: 1.441] Epoch 05400 [D loss: 0.514, acc.: 075.5%] [G loss: 1.377] Epoch 05500 [D loss: 0.450, acc.: 080.4%] [G loss: 1.443] Epoch 05600 [D loss: 0.489, acc.: 076.1%] [G loss: 1.415] Epoch 05700 [D loss: 0.491, acc.: 075.9%] [G loss: 1.376] Epoch 05800 [D loss: 0.479, acc.: 077.1%] [G loss: 1.531] Epoch 05900 [D loss: 0.503, acc.: 076.0%] [G loss: 1.383] Epoch 06000 [D loss: 0.476, acc.: 077.4%] [G loss: 1.395] Epoch 06100 [D loss: 0.476, acc.: 078.0%] [G loss: 1.432] Epoch 06200 [D loss: 0.505, acc.: 075.1%] [G loss: 1.374] Epoch 06300 [D loss: 0.489, acc.: 076.0%] [G loss: 1.416] Epoch 06400 [D loss: 0.484, acc.: 076.0%] [G loss: 1.459] Epoch 06500 [D loss: 0.472, acc.: 077.5%] [G loss: 1.460] Epoch 06600 [D loss: 0.554, acc.: 071.0%] [G loss: 1.387] Epoch 06700 [D loss: 0.473, acc.: 077.3%] [G loss: 1.447] Epoch 06800 [D loss: 0.529, acc.: 073.0%] [G loss: 1.378] Epoch 06900 [D loss: 0.524, acc.: 073.2%] [G loss: 1.350] Epoch 07000 [D loss: 0.495, acc.: 075.5%] [G loss: 1.321] Epoch 07100 [D loss: 0.514, acc.: 074.4%] [G loss: 1.349] Epoch 07200 [D loss: 0.468, acc.: 078.7%] [G loss: 1.469] Epoch 07300 [D loss: 0.474, acc.: 076.4%] [G loss: 1.497] Epoch 07400 [D loss: 0.442, acc.: 080.6%] [G loss: 1.465] Epoch 07500 [D loss: 0.478, acc.: 076.6%] [G loss: 1.431] Epoch 07600 [D loss: 0.504, acc.: 074.8%] [G loss: 1.375] Epoch 07700 [D loss: 0.514, acc.: 073.3%] [G loss: 1.393] Epoch 07800 [D loss: 0.511, acc.: 074.2%] [G loss: 1.376] Epoch 07900 [D loss: 0.510, acc.: 075.4%] [G loss: 1.378] Epoch 08000 [D loss: 0.508, acc.: 075.1%] [G loss: 1.388] Epoch 08100 [D loss: 0.498, acc.: 075.5%] [G loss: 1.425] Epoch 08200 [D loss: 0.477, acc.: 077.1%] [G loss: 1.425] Epoch 08300 [D loss: 0.491, acc.: 077.0%] [G loss: 1.364] Epoch 08400 [D loss: 0.521, acc.: 073.5%] [G loss: 1.331] Epoch 08500 [D loss: 0.455, acc.: 079.2%] [G loss: 1.392] Epoch 08600 [D loss: 0.496, acc.: 075.4%] [G loss: 1.421] Epoch 08700 [D loss: 0.473, acc.: 077.0%] [G loss: 1.396] Epoch 08800 [D loss: 0.494, acc.: 075.5%] [G loss: 1.427] Epoch 08900 [D loss: 0.508, acc.: 073.9%] [G loss: 1.384] Epoch 09000 [D loss: 0.502, acc.: 075.1%] [G loss: 1.369] Epoch 09100 [D loss: 0.460, acc.: 077.9%] [G loss: 1.337] Epoch 09200 [D loss: 0.492, acc.: 075.8%] [G loss: 1.401] Epoch 09300 [D loss: 0.465, acc.: 078.1%] [G loss: 1.371] Epoch 09400 [D loss: 0.506, acc.: 075.4%] [G loss: 1.365] Epoch 09500 [D loss: 0.472, acc.: 077.9%] [G loss: 1.315] Epoch 09600 [D loss: 0.453, acc.: 079.0%] [G loss: 1.366] Epoch 09700 [D loss: 0.482, acc.: 075.9%] [G loss: 1.353] Epoch 09800 [D loss: 0.505, acc.: 075.0%] [G loss: 1.372] Epoch 09900 [D loss: 0.511, acc.: 074.9%] [G loss: 1.356] Epoch 10000 [D loss: 0.477, acc.: 077.0%] [G loss: 1.396] Epoch 10100 [D loss: 0.476, acc.: 077.8%] [G loss: 1.324] Epoch 10200 [D loss: 0.475, acc.: 077.0%] [G loss: 1.377] Epoch 10300 [D loss: 0.507, acc.: 074.0%] [G loss: 1.361] Epoch 10400 [D loss: 0.466, acc.: 076.4%] [G loss: 1.414] Epoch 10500 [D loss: 0.492, acc.: 074.1%] [G loss: 1.365] Epoch 10600 [D loss: 0.469, acc.: 078.6%] [G loss: 1.340] Epoch 10700 [D loss: 0.474, acc.: 077.2%] [G loss: 1.371] Epoch 10800 [D loss: 0.492, acc.: 074.4%] [G loss: 1.348] Epoch 10900 [D loss: 0.471, acc.: 077.3%] [G loss: 1.349] Epoch 11000 [D loss: 0.472, acc.: 077.8%] [G loss: 1.342] Epoch 11100 [D loss: 0.447, acc.: 080.1%] [G loss: 1.408] Epoch 11200 [D loss: 0.473, acc.: 076.6%] [G loss: 1.367] Epoch 11300 [D loss: 0.465, acc.: 078.5%] [G loss: 1.395] Epoch 11400 [D loss: 0.479, acc.: 077.2%] [G loss: 1.352] Epoch 11500 [D loss: 0.509, acc.: 074.4%] [G loss: 1.306] Epoch 11600 [D loss: 0.490, acc.: 074.0%] [G loss: 1.323] Epoch 11700 [D loss: 0.490, acc.: 075.4%] [G loss: 1.321] Epoch 11800 [D loss: 0.506, acc.: 074.3%] [G loss: 1.315] Epoch 11900 [D loss: 0.497, acc.: 075.6%] [G loss: 1.325] Epoch 12000 [D loss: 0.491, acc.: 075.8%] [G loss: 1.317] Epoch 12100 [D loss: 0.501, acc.: 074.9%] [G loss: 1.371] Epoch 12200 [D loss: 0.488, acc.: 074.9%] [G loss: 1.391] Epoch 12300 [D loss: 0.527, acc.: 073.4%] [G loss: 1.332] Epoch 12400 [D loss: 0.486, acc.: 074.8%] [G loss: 1.350] Epoch 12500 [D loss: 0.482, acc.: 077.0%] [G loss: 1.321] Epoch 12600 [D loss: 0.511, acc.: 074.4%] [G loss: 1.325] Epoch 12700 [D loss: 0.507, acc.: 074.1%] [G loss: 1.329] Epoch 12800 [D loss: 0.501, acc.: 074.3%] [G loss: 1.338] Epoch 12900 [D loss: 0.482, acc.: 078.2%] [G loss: 1.367] Epoch 13000 [D loss: 0.488, acc.: 076.9%] [G loss: 1.386] Epoch 13100 [D loss: 0.505, acc.: 074.5%] [G loss: 1.335] Epoch 13200 [D loss: 0.509, acc.: 074.1%] [G loss: 1.318] Epoch 13300 [D loss: 0.496, acc.: 075.2%] [G loss: 1.379] Epoch 13400 [D loss: 0.475, acc.: 076.3%] [G loss: 1.392] Epoch 13500 [D loss: 0.508, acc.: 073.6%] [G loss: 1.338] Epoch 13600 [D loss: 0.461, acc.: 077.7%] [G loss: 1.342] Epoch 13700 [D loss: 0.485, acc.: 075.8%] [G loss: 1.350] Epoch 13800 [D loss: 0.462, acc.: 078.9%] [G loss: 1.358] Epoch 13900 [D loss: 0.486, acc.: 076.2%] [G loss: 1.383] Epoch 14000 [D loss: 0.475, acc.: 078.4%] [G loss: 1.410] Epoch 14100 [D loss: 0.511, acc.: 075.0%] [G loss: 1.326] Epoch 14200 [D loss: 0.487, acc.: 075.9%] [G loss: 1.301] Epoch 14300 [D loss: 0.510, acc.: 074.2%] [G loss: 1.372] Epoch 14400 [D loss: 0.510, acc.: 074.5%] [G loss: 1.395] Epoch 14500 [D loss: 0.506, acc.: 073.2%] [G loss: 1.350] Epoch 14600 [D loss: 0.478, acc.: 078.2%] [G loss: 1.402] Epoch 14700 [D loss: 0.497, acc.: 075.6%] [G loss: 1.374] Epoch 14800 [D loss: 0.468, acc.: 077.7%] [G loss: 1.382] Epoch 14900 [D loss: 0.470, acc.: 077.2%] [G loss: 1.337] Epoch 15000 [D loss: 0.475, acc.: 078.9%] [G loss: 1.326] Epoch 15100 [D loss: 0.477, acc.: 076.4%] [G loss: 1.347] Epoch 15200 [D loss: 0.463, acc.: 077.9%] [G loss: 1.386] Epoch 15300 [D loss: 0.484, acc.: 076.3%] [G loss: 1.326] Epoch 15400 [D loss: 0.494, acc.: 076.6%] [G loss: 1.282] Epoch 15500 [D loss: 0.483, acc.: 077.0%] [G loss: 1.364] Epoch 15600 [D loss: 0.502, acc.: 074.1%] [G loss: 1.387] Epoch 15700 [D loss: 0.478, acc.: 076.7%] [G loss: 1.359] Epoch 15800 [D loss: 0.489, acc.: 074.8%] [G loss: 1.339] Epoch 15900 [D loss: 0.504, acc.: 074.8%] [G loss: 1.362] Epoch 16000 [D loss: 0.480, acc.: 075.4%] [G loss: 1.354] Epoch 16100 [D loss: 0.486, acc.: 075.5%] [G loss: 1.381] Epoch 16200 [D loss: 0.491, acc.: 076.6%] [G loss: 1.374] Epoch 16300 [D loss: 0.496, acc.: 074.2%] [G loss: 1.421] Epoch 16400 [D loss: 0.468, acc.: 076.6%] [G loss: 1.416] Epoch 16500 [D loss: 0.492, acc.: 076.2%] [G loss: 1.359] Epoch 16600 [D loss: 0.500, acc.: 073.6%] [G loss: 1.341] Epoch 16700 [D loss: 0.475, acc.: 076.4%] [G loss: 1.409] Epoch 16800 [D loss: 0.476, acc.: 077.8%] [G loss: 1.377] Epoch 16900 [D loss: 0.482, acc.: 075.7%] [G loss: 1.397] Epoch 17000 [D loss: 0.465, acc.: 077.3%] [G loss: 1.371] Epoch 17100 [D loss: 0.492, acc.: 074.9%] [G loss: 1.340] Epoch 17200 [D loss: 0.486, acc.: 076.2%] [G loss: 1.383] Epoch 17300 [D loss: 0.458, acc.: 078.1%] [G loss: 1.411] Epoch 17400 [D loss: 0.476, acc.: 077.6%] [G loss: 1.413] Epoch 17500 [D loss: 0.501, acc.: 074.5%] [G loss: 1.401] Epoch 17600 [D loss: 0.500, acc.: 074.5%] [G loss: 1.396] Epoch 17700 [D loss: 0.470, acc.: 076.1%] [G loss: 1.365] Epoch 17800 [D loss: 0.489, acc.: 075.6%] [G loss: 1.415] Epoch 17900 [D loss: 0.485, acc.: 076.1%] [G loss: 1.412] Epoch 18000 [D loss: 0.478, acc.: 077.0%] [G loss: 1.437] Epoch 18100 [D loss: 0.473, acc.: 077.3%] [G loss: 1.396] Epoch 18200 [D loss: 0.436, acc.: 081.0%] [G loss: 1.382] Epoch 18300 [D loss: 0.500, acc.: 074.9%] [G loss: 1.466] Epoch 18400 [D loss: 0.485, acc.: 076.5%] [G loss: 1.415] Epoch 18500 [D loss: 0.454, acc.: 078.9%] [G loss: 1.432] Epoch 18600 [D loss: 0.484, acc.: 075.6%] [G loss: 1.444] Epoch 18700 [D loss: 0.435, acc.: 079.7%] [G loss: 1.391] Epoch 18800 [D loss: 0.481, acc.: 077.0%] [G loss: 1.427] Epoch 18900 [D loss: 0.488, acc.: 076.4%] [G loss: 1.428] Epoch 19000 [D loss: 0.466, acc.: 078.3%] [G loss: 1.380] Epoch 19100 [D loss: 0.473, acc.: 075.8%] [G loss: 1.427] Epoch 19200 [D loss: 0.462, acc.: 075.9%] [G loss: 1.478] Epoch 19300 [D loss: 0.458, acc.: 078.0%] [G loss: 1.508] Epoch 19400 [D loss: 0.483, acc.: 074.5%] [G loss: 1.402] Epoch 19500 [D loss: 0.458, acc.: 076.9%] [G loss: 1.442] Epoch 19600 [D loss: 0.477, acc.: 076.9%] [G loss: 1.392] Epoch 19700 [D loss: 0.436, acc.: 080.2%] [G loss: 1.453] Epoch 19800 [D loss: 0.469, acc.: 076.3%] [G loss: 1.426] Epoch 19900 [D loss: 0.464, acc.: 077.6%] [G loss: 1.457] ---------- Time took: 76.88 min Loss over epochs Notice that the losses from dicriminator are not dicreasing over epochs, which makes sense because the discriminator's good classification performance is always challenged by the improved \"fake images\" from the generator. In [9]: import pandas as pd hist = pd . DataFrame ( history ) plt . figure ( figsize = ( 20 , 5 )) for colnm in hist . columns : plt . plot ( hist [ colnm ], label = colnm ) plt . legend () plt . ylabel ( \"loss\" ) plt . xlabel ( \"epochs\" ) plt . show () Finally create gif of the generated images at every few epochs In [10]: def makegif ( dir_images ): import imageio filenames = np . sort ( os . listdir ( dir_images )) filenames = [ fnm for fnm in filenames if \".png\" in fnm ] with imageio . get_writer ( dir_images + '/image.gif' , mode = 'I' ) as writer : for filename in filenames : image = imageio . imread ( dir_images + filename ) writer . append_data ( image ) os . remove ( dir_images + filename ) makegif ( dir_result ) GAN Auto-Encoder GAN can transform the latent variable to an image. What about the other way? Can I map an image to a latent variable? If we can do such mapping, I can do interesting things e.g., the average latent variable values of female images may be decoded into an average female images. Photo Editing Generative Adversarial Networks Part 1 creates GAN-based encoder-decoder network: separately train an encoder while using generator as a fixed decoder. For the encoder network, I will use the discriminator with the number of output neurons in the last layer set to 100. This way, I can reversed the order of things in my GAN and created a GAN Auto-encoder. In the encoder network, the trainable parameters are only set to the ones from the encoder. In [11]: img_in = layers . Input ( shape = img_shape ) # discriminator with the final output layer = 100 network as encoder discriminator_encoder = build_discriminator ( img_shape , 100 ) # discriminator as encoder encoder = discriminator_encoder ( img_in ) # generator as decoder generator . trainable = False img_out = generator ( encoder ) encoder_decoder = models . Model ( img_in , img_out ) encoder_decoder . compile ( loss = 'mse' , optimizer = optimizer ) encoder_decoder . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_4 (InputLayer) (None, 32, 32, 3) 0 _________________________________________________________________ model_4 (Model) (None, 100) 6813060 _________________________________________________________________ model_1 (Model) (None, 32, 32, 3) 8584515 ================================================================= Total params: 15,397,575 Trainable params: 6,813,060 Non-trainable params: 8,584,515 _________________________________________________________________ Train encoders In [12]: start_time = time . time () history_ed = encoder_decoder . fit ( X_train , X_train , validation_data = ( X_test , X_test ), epochs = 10 , verbose = 2 ) end_time = time . time () print ( \"-\" * 10 ) print ( \"Time took: {:4.2f} min\" . format (( end_time - start_time ) / 60 )) Train on 200000 samples, validate on 100 samples Epoch 1/10 - 68s - loss: 0.0355 - val_loss: 0.0349 Epoch 2/10 - 66s - loss: 0.0328 - val_loss: 0.0340 Epoch 3/10 - 66s - loss: 0.0322 - val_loss: 0.0336 Epoch 4/10 - 66s - loss: 0.0318 - val_loss: 0.0332 Epoch 5/10 - 67s - loss: 0.0316 - val_loss: 0.0331 Epoch 6/10 - 66s - loss: 0.0314 - val_loss: 0.0330 Epoch 7/10 - 66s - loss: 0.0312 - val_loss: 0.0328 Epoch 8/10 - 67s - loss: 0.0311 - val_loss: 0.0327 Epoch 9/10 - 67s - loss: 0.0310 - val_loss: 0.0326 Epoch 10/10 - 66s - loss: 0.0309 - val_loss: 0.0326 ---------- Time took: 11.08 min Loss over epochs In [13]: plt . figure ( figsize = ( 10 , 5 )) for colnm in history_ed . history . keys (): plt . plot ( history_ed . history [ colnm ], label = colnm ) plt . legend () plt . show () Check the model performance of the encoder-decoder network using testing data In [14]: # discriminator_encoder.compile(loss='mse', optimizer=optimizer) X_pred = encoder_decoder . predict ( X_test ) ## z_pred = discriminator_encoder.predict(X_test) Plot the original image and reproduced image using encoder Some reproduced images are somewhat similar to the original images... But I clearly need more training. In [15]: Ntest = 10 for irow in range ( Ntest ): fig = plt . figure ( figsize = ( 10 , 5 )) ax = fig . add_subplot ( 1 , 2 , 1 ) ax . imshow ( X_test [ irow ]) ax . set_title ( \"original image\" ) ax = fig . add_subplot ( 1 , 2 , 2 ) ax . imshow ( X_pred [ irow ]) ax . set_title ( \"encoded image\" ) plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"My first GAN using CelebA data"},{"url":"Welcome-to-CelebA.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In this notebook, I will explore the CelebA dataset . In [1]: import pandas as pd import os import numpy as np import matplotlib.pyplot as plt from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array dir_anno = \"data/Anno-20180622T163917Z-001/Anno/\" dir_data = \"data/img_align_celeba/\" /home/bur2pal/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Using TensorFlow backend. Let's take a look at the available labels/annotations In [2]: ls $ dir_anno identity_CelebA.txt list_landmarks_align_celeba.txt list_attr_celeba.txt list_landmarks_celeba.txt list_bbox_celeba.txt ~$st_bbox_celeba.txt load annotations In [8]: def get_annotation ( fnmtxt , verbose = True ): if verbose : print ( \"_\" * 70 ) print ( fnmtxt ) rfile = open ( dir_anno + fnmtxt , 'r' ) texts = rfile . read () . split ( \" \\r\\n \" ) rfile . close () columns = np . array ( texts [ 1 ] . split ( \" \" )) columns = columns [ columns != \"\" ] df = [] for txt in texts [ 2 :]: txt = np . array ( txt . split ( \" \" )) txt = txt [ txt != \"\" ] df . append ( txt ) df = pd . DataFrame ( df ) if df . shape [ 1 ] == len ( columns ) + 1 : columns = [ \"image_id\" ] + list ( columns ) df . columns = columns df = df . dropna () if verbose : print ( \" Total number of annotations {} \\n \" . format ( df . shape )) print ( df . head ()) ## cast to integer for nm in df . columns : if nm != \"image_id\" : df [ nm ] = pd . to_numeric ( df [ nm ], downcast = \"integer\" ) return ( df ) attr = get_annotation ( \"list_attr_celeba.txt\" ) align = get_annotation ( \"list_landmarks_align_celeba.txt\" ) assert np . all ( align [ \"image_id\" ] == attr [ \"image_id\" ]) ______________________________________________________________________ list_attr_celeba.txt Total number of annotations (202599, 41) image_id 5_o_Clock_Shadow Arched_Eyebrows Attractive Bags_Under_Eyes \\ 0 000001.jpg -1 1 1 -1 1 000002.jpg -1 -1 -1 1 2 000003.jpg -1 -1 -1 -1 3 000004.jpg -1 -1 1 -1 4 000005.jpg -1 1 1 -1 Bald Bangs Big_Lips Big_Nose Black_Hair ... Sideburns Smiling \\ 0 -1 -1 -1 -1 -1 ... -1 1 1 -1 -1 -1 1 -1 ... -1 1 2 -1 -1 1 -1 -1 ... -1 -1 3 -1 -1 -1 -1 -1 ... -1 -1 4 -1 -1 1 -1 -1 ... -1 -1 Straight_Hair Wavy_Hair Wearing_Earrings Wearing_Hat Wearing_Lipstick \\ 0 1 -1 1 -1 1 1 -1 -1 -1 -1 -1 2 -1 1 -1 -1 -1 3 1 -1 1 -1 1 4 -1 -1 -1 -1 1 Wearing_Necklace Wearing_Necktie Young 0 -1 -1 1 1 -1 -1 1 2 -1 -1 1 3 1 -1 1 4 -1 -1 1 [5 rows x 41 columns] ______________________________________________________________________ list_landmarks_align_celeba.txt Total number of annotations (202599, 11) image_id lefteye_x lefteye_y righteye_x righteye_y nose_x nose_y \\ 0 000001.jpg 69 109 106 113 77 142 1 000002.jpg 69 110 107 112 81 135 2 000003.jpg 76 112 104 106 108 128 3 000004.jpg 72 113 108 108 101 138 4 000005.jpg 66 114 112 112 86 119 leftmouth_x leftmouth_y rightmouth_x rightmouth_y 0 73 152 108 154 1 70 151 108 153 2 74 156 98 158 3 71 155 101 151 4 71 147 104 150 Plot facial images with landmarks In [4]: def plot_image ( align , nrow = 2 ): figsize = ( 20 , 10 ) ncol = 5 fig = plt . figure ( figsize = figsize ) N = nrow * ncol for i , myid in enumerate ( align [ \"image_id\" ][: N ]): image = load_img ( dir_data + \"/\" + myid ) image = img_to_array ( image ) / 255.0 ( _ , lefteye_x , lefteye_y , righteye_x , righteye_y , nose_x , nose_y , leftmouth_x , leftmouth_y , rightmouth_x , rightmouth_y ) = align . iloc [ i ] ax = fig . add_subplot ( nrow , ncol , i + 1 ) ax . imshow ( image ) ax . set_title ( image . shape ) ax . scatter ( lefteye_x , lefteye_y ) ax . scatter ( righteye_x , righteye_y ) ax . scatter ( nose_x , nose_y ) ax . scatter ( leftmouth_x , leftmouth_y ) ax . scatter ( rightmouth_x , rightmouth_y ) plot_image ( align ) Plot all the (x,y) coordiantes of landmarks In [5]: landmarks = [ \"lefteye\" , \"righteye\" , \"nose\" , \"leftmouth\" , \"rightmouth\" ] plt . figure ( figsize = ( 10 , 10 )) for lmark in landmarks : plt . scatter ( align [ lmark + \"_x\" ], align [ lmark + \"_y\" ], alpha = 0.3 , label = lmark ) plt . legend () plt . gca () . invert_yaxis () plt . show () Plot the distribution of attributes In [6]: for colnm in attr . columns : if colnm != \"image_id\" : print ( \" {:20} {:5.2f}%\" . format ( colnm , 100 * np . mean ( attr [ colnm ] == 1 ))) 5_o_Clock_Shadow 11.11% Arched_Eyebrows 26.70% Attractive 51.25% Bags_Under_Eyes 20.46% Bald 2.24% Bangs 15.16% Big_Lips 24.08% Big_Nose 23.45% Black_Hair 23.93% Blond_Hair 14.80% Blurry 5.09% Brown_Hair 20.52% Bushy_Eyebrows 14.22% Chubby 5.76% Double_Chin 4.67% Eyeglasses 6.51% Goatee 6.28% Gray_Hair 4.19% Heavy_Makeup 38.69% High_Cheekbones 45.50% Male 41.68% Mouth_Slightly_Open 48.34% Mustache 4.15% Narrow_Eyes 11.51% No_Beard 83.49% Oval_Face 28.41% Pale_Skin 4.29% Pointy_Nose 27.74% Receding_Hairline 7.98% Rosy_Cheeks 6.57% Sideburns 5.65% Smiling 48.21% Straight_Hair 20.84% Wavy_Hair 31.96% Wearing_Earrings 18.89% Wearing_Hat 4.85% Wearing_Lipstick 47.24% Wearing_Necklace 12.30% Wearing_Necktie 7.27% Young 77.36% Plot the celebs with specific attributes In [7]: for attrnm in [ \"Bald\" , \"Bangs\" , \"Male\" , \"No_Beard\" , \"Pointy_Nose\" , \"Wearing_Earrings\" , \"Smiling\" , \"No_Beard\" ]: print ( attrnm ) plot_image ( align . loc [ attr [ attrnm ] == 1 ,:], nrow = 1 ) plt . show () Bald Bangs Male No_Beard Pointy_Nose Wearing_Earrings Smiling No_Beard if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Welcome to CelebA"},{"url":"Evaluate-uncertainty-using-ensemble-of-deep-learning-models-with-negative-log-likelihood-loss.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } Evaluating the quality of predictive uncertainties is challenging as \"ground truth\" uncertainty is usually not available. Yet, model's confidence about its estimation is often of interest for researchers. If the model can tell \"what it knows\" or what is \"out of distribution\", such infomation gives insights about when the researchers should take the point estimates as their face values. Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning recently proposed using Monte Carlo dropout to estimate predictive uncertainty by using Dropout at test time. Most of the recent work on uncertainty in deep learning is in this line. I study MC dropout method for evaluating model's confidence in Measure the uncertainty in deep learning models using dropout . I found that this MC dropout method requires the model to restrict the choice of network structures even for modeling simple data; activation functions seem to have to be relu and we seem to need to have quite a few dropout layers. I recently found a new paper Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles in this line of uncertainty research. Their methods do not take Bayesian approaches and it has a lot of similarity with the maximum likelihood approach. In this blog post, I will review this new method. MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}}); Model The approach assumes that given available input $X$, the target has a normal distribution with mean and variance dependent on the values of $X$: $$ Y | X \\overset{i.i.d.}{\\sim} N \\left(\\mu_m (X) , \\sigma_m&#94;2(X) \\right) $$ Here, $\\mu_m (X)$ and $\\sigma_m&#94;2(X)$ are modelled non-linearly using neural network. Then the maximum likelihood methods are used to estimate unknown parameters $\\mu_m (X)$, $\\sigma_m&#94;2(X)$. In practice, estimates become more accurate if $M$ models are ensembled together. Therefore, Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles also proposed to use the ensemble model for prediction. This essentially means that you are assuming that conditional distribution of $Y$ to be from the mixture: $$ Y \\left| X \\right. \\sim \\frac{1}{M}\\sum_{m=1}&#94;M N \\left(\\mu_m (X) , \\sigma_m&#94;2(X) \\right) $$ Under this ensemble model structure, mean $\\mu_{*} (X)$ and variance $\\sigma_{*}&#94;2(X)$ of the marginal distribution can be calcualted as: \\begin{array}{rcll} \\mu_{*} (X) &=& \\frac{1}{M}\\sum_{m=1}&#94;M \\mu_m (X)\\\\ \\sigma_{*}&#94;2 (X) &=& \\frac{1}{M} \\sum_{m=1}&#94;M \\left( \\sigma_m&#94;2(X) + \\mu_m (X)&#94;2 \\right) - \\mu_{*} (X)&#94;2\\\\ \\end{array} These are because: \\begin{array}{rcll} \\mu_{*} (X) & := & E(Y)\\\\ &=& \\sum_{m=1}&#94;M E(Y|m) \\\\ &=& \\frac{1}{M}\\sum_{m=1}&#94;M \\mu_m (X)\\\\ \\sigma_{*}&#94;2 (X) &:=& Var(Y) \\\\ &=& E_m(Var(Y|m)) + Var_m(E(Y|m)) \\\\ &=& \\frac{1}{M}\\sum_{m=1}&#94;M \\sigma_m&#94;2(X) + Var_m \\left( \\mu_m (X) \\right)\\\\ \\textrm{ where } Var_m \\left( \\mu_m (X) \\right) &=& E_m \\left( \\mu_m (X)&#94;2 \\right) - E_m \\left( \\mu_m (X) \\right)&#94;2\\\\ &=& \\frac{1}{M} \\sum_{m=1}&#94;M \\mu_m (X)&#94;2 - \\left( \\frac{1}{M}\\sum_{m=1}&#94;M \\mu_m (X) \\right)&#94;2\\\\ &=& \\frac{1}{M} \\sum_{m=1}&#94;M \\mu_m (X)&#94;2 - \\mu_{*} (X)&#94;2 \\end{array} See Wikipedia if you get lost dividing variance into its components. Finally, the authors proposed to use adversarial training to improve the estimation of the likelihood: Adversarial training can be interpreted as a computationally efficient solution to smooth the predictive distributions by increasing the likelihood of the target around an $\\epsilon$ neighboorhood. I previously studied adversarial examples. So I will not discuss its details here. See Generate adversarial examples using TensorFlow . In this blog, I would like to know how easy it is to implement this procedure and whether adversarial training is really necessary. In [1]: import matplotlib.pyplot as plt import pandas as pd import numpy as np import tensorflow as tf import os , sys os . environ [ \"CUDA_DEVICE_ORDER\" ] = \"PCI_BUS_ID\" config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"1\" tf . Session ( config = config ) print ( \"python {}\" . format ( sys . version )) print ( \"tensorflow version {}\" . format ( tf . __version__ )) python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] tensorflow version 1.2.0 Generate synthetic data I will use the same simulated data as the previous blog: TensorFlow newbie creates a neural net with a negative log likelihood as a loss . In [2]: ## Define x inc = 0.001 x_train = np . concatenate ([ np . arange ( - 2 , - 1.5 , inc ), np . arange ( - 1 , 2 , inc )]) x_train = x_train . reshape ( len ( x_train ), 1 ) ## Define y steps_per_cycle = 1 def sinfun ( xs , noise = 0.001 ): import random , math xs = xs . flatten () def randomNoise ( x ): ax = 2 - np . abs ( x ) wnoise = random . uniform ( - noise * ax , noise * ax ) return ( math . sin ( x * ( 2 * math . pi / steps_per_cycle ) ) + wnoise ) vec = [ randomNoise ( x ) - x for x in xs ] return ( np . array ( vec ) . flatten ()) y_train0 = sinfun ( x_train , noise = 0.5 ) y_train = y_train0 . reshape ( len ( y_train0 ), 1 ) print ( \" x_train.shape={}\" . format ( x_train . shape )) print ( \" y_train.shape={}\" . format ( y_train . shape )) ## Visualize the generated data (X,y) plt . figure ( figsize = ( 10 , 3 )) plt . scatter ( x_train , y_train , s = 0.5 ) plt . xlabel ( \"x_train\" ) plt . ylabel ( \"y_train\" ) plt . title ( \"The x values of the synthetic data ranges between {:4.3f} and {:4.3f}\" . format ( np . min ( x_train ), np . max ( x_train ))) plt . show () x_train.shape=(3500, 1) y_train.shape=(3500, 1) Define model I will consider 3-hidden layer NN for modeling this synthetic data. I will consider negative log-likelihood as a loss function. This model was also used in the blog post: TensorFlow newbie creates a neural net with a negative log likelihood as a loss . Notice that all functions are the same as ones in the previous blog, except \"define_model\" function. The \"define_model\" function now has tensors to calcualte the adversarial examples. The codes for calculating adversarial examples are mostly borrowed from the previous blog Generate adversarial examples using TensorFlow . In [3]: def weight_variable ( shape ): ## weight variable, initialized with truncated normal distribution initial = tf . truncated_normal ( shape , stddev = 0.01 , dtype = \"float32\" ) return tf . Variable ( initial ) def bias_variable ( shape ): initial = tf . constant ( 0.001 , shape = shape , dtype = \"float32\" ) return tf . Variable ( initial ) def fully_connected_layer ( h0 , n_h0 , n_h1 , verbose = True ): ''' h0 : tensor of shape (n_h0, n_h1) n_h0 : scalar n_h1 : scalar ''' W1 = weight_variable ([ n_h0 , n_h1 ]) b1 = bias_variable ([ n_h1 ]) if verbose : print ( \" h0.shape={}\" . format ( h0 . get_shape ())) print ( \" W1.shape={}\" . format ( W1 . get_shape ())) print ( \" b1.shape={}\" . format ( b1 . get_shape ())) h1 = tf . matmul ( h0 , W1 ) + b1 return ( h1 , ( W1 , b1 )) def nll_gaussian ( y_pred_mean , y_pred_sd , y_test ): ## element wise square square = tf . square ( y_pred_mean - y_test ) ## preserve the same shape as y_pred.shape ms = tf . add ( tf . divide ( square , y_pred_sd ), tf . log ( y_pred_sd )) ## axis = -1 means that we take mean across the last dimension ## the output keeps all but the last dimension ## ms = tf.reduce_mean(ms,axis=-1) ## return scalar ms = tf . reduce_mean ( ms ) return ( ms ) def mse ( y_pred , y_test , verbose = True ): ''' y_pred : tensor y_test : tensor having the same shape as y_pred ''' ## element wise square square = tf . square ( y_pred - y_test ) ## preserve the same shape as y_pred.shape ## mean across the final dimensions ms = tf . reduce_mean ( square ) return ( ms ) def define_model ( n_feature , n_hs , n_output , eps = 0.1 , verbose = True , NLL = True ): x_input_shape = [ None , n_feature ] x_input = tf . placeholder ( tf . float32 , x_input_shape ) y_input = tf . placeholder ( tf . float32 , [ None , 1 ]) h_previous = x_input n_h_previous = n_feature paras = [] for ilayer , n_h in enumerate ( n_hs , 1 ): if verbose : print ( \" layer:{}\" . format ( ilayer )) h , p = fully_connected_layer ( h_previous , n_h_previous , n_h , verbose ) h_previous = tf . nn . relu ( h ) n_h_previous = n_h paras . append ( p ) if verbose : print ( \" output layer for y_mean\" ) y_mean , p = fully_connected_layer ( h_previous , n_h_previous , n_output , verbose ) paras . append ( p ) if NLL : if verbose : print ( \" output layer for y_sigma\" ) y_sigma , p = fully_connected_layer ( h_previous , n_h_previous , n_output , verbose ) ## for numerical stability this enforce the variance to be more than 1E-1 y_sigma = tf . clip_by_value ( t = tf . exp ( y_sigma ), clip_value_min = tf . constant ( 1E-1 ), clip_value_max = tf . constant ( 1E+100 )) paras . append ( p ) loss = nll_gaussian ( y_mean , y_sigma , y_input ) y = [ y_mean , y_sigma ] else : loss = mse ( y_mean , y_input ) y = [ y_mean ] ## tensor to calculate the adversarial image eps_tf = tf . constant ( float ( eps ), name = \"epsilon\" ) grad_tf = tf . gradients ( loss ,[ x_input ]) grad_sign_tf = tf . sign ( grad_tf ) grad_sign_eps_tf = tf . scalar_mul ( eps_tf , grad_sign_tf ) aimage = tf . add ( grad_sign_eps_tf , x_input ) aimage = tf . reshape ( aimage ,[ - 1 , n_feature ]) x_input_a = tf . concat ([ aimage , x_input ], axis = 0 ) train_step = tf . train . GradientDescentOptimizer ( 0.01 ) . minimize ( loss ) inputs = ( x_input , y_input , x_input_a ) tensors = ( inputs , loss , train_step , y , paras ) return ( tensors ) Functions to train the model Notice that every batch is augmented by adversarial examples. In [4]: def train ( x_train , y_train , tensors , n_batch = 500 , n_epochs = 5000 , adversarialTF = True ): from sklearn.utils import shuffle import time inputs , loss , train_step , _ , _ = tensors x_input , y_input , x_input_a = inputs sess = tf . InteractiveSession () tf . global_variables_initializer () . run () lvalues = [] start = time . time () for i_epoch in range ( n_epochs ): x_shuffle , y_shuffle = shuffle ( x_train , y_train ) for i in range ( 0 , x_train . shape [ 0 ], n_batch ): batch_xs = x_shuffle [ i : i + n_batch ] batch_ys = y_shuffle [ i : i + n_batch ] ## obtain adversarial samples if adversarialTF : batch_xs = sess . run ( x_input_a , feed_dict = { x_input : batch_xs , y_input : batch_ys }) batch_ys = np . concatenate ([ batch_ys , batch_ys ], axis = 0 ) sess . run ( train_step , feed_dict = { x_input : batch_xs , y_input : batch_ys }) lv = sess . run ( loss , feed_dict = { x_input : x_train , y_input : y_train }) lvalues . append ( lv ) if ( i_epoch % 1000 ) - 999 == 0 : print ( \" epoch {: 5.0f} loss {: 5.4f} {:3.0f}sec\" . format ( i_epoch , lv , time . time () - start )) start = time . time () return ( sess , lvalues ) Training multiple models Here, I consider two training setting: training with adversarial examples training without adversarial examples It seems that the training without adversarial examples takes 2/3 less times than with adversarial examples. In [5]: result_NLL , result_NLL_adv = [], [] n_hs = [ 500 , 300 , 100 ] eps = 0.05 Nensemble = 5 n_epochs = 5000 for iens in range ( Nensemble ): print ( \"Adversarial training: {}th ensemble\" . format ( iens )) tensors = define_model ( n_feature = 1 , n_hs = n_hs , n_output = 1 , eps = eps , verbose = False ) sess , lvalues = train ( x_train , y_train , tensors , n_epochs = n_epochs , adversarialTF = True ) result_NLL_adv . append (( sess , lvalues , tensors )) print ( \"Regular training: {}th ensemble\" . format ( iens )) tensors = define_model ( n_feature = 1 , n_hs = n_hs , n_output = 1 , eps = eps , verbose = False ) sess , lvalues = train ( x_train , y_train , tensors , n_epochs = n_epochs , adversarialTF = False ) result_NLL . append (( sess , lvalues , tensors )) Adversarial training: 0th ensemble epoch 999 loss 0.1921 34sec epoch 1999 loss -0.5653 33sec epoch 2999 loss -0.9877 34sec epoch 3999 loss -1.1007 35sec epoch 4999 loss -1.1928 34sec Regular training: 0th ensemble epoch 999 loss 0.0817 19sec epoch 1999 loss -0.3772 19sec epoch 2999 loss -1.1049 19sec epoch 3999 loss -1.1557 18sec epoch 4999 loss -1.3086 19sec Adversarial training: 1th ensemble epoch 999 loss 0.1046 34sec epoch 1999 loss -0.3785 34sec epoch 2999 loss -0.8608 34sec epoch 3999 loss -1.1535 34sec epoch 4999 loss -1.2078 34sec Regular training: 1th ensemble epoch 999 loss 0.0923 19sec epoch 1999 loss -0.6870 19sec epoch 2999 loss -1.2025 19sec epoch 3999 loss -1.2581 19sec epoch 4999 loss -1.3139 18sec Adversarial training: 2th ensemble epoch 999 loss 0.1234 34sec epoch 1999 loss -0.5084 34sec epoch 2999 loss -0.8654 33sec epoch 3999 loss -1.1636 34sec epoch 4999 loss -1.2155 33sec Regular training: 2th ensemble epoch 999 loss 0.0873 19sec epoch 1999 loss -0.3271 19sec epoch 2999 loss -0.9850 18sec epoch 3999 loss -1.2101 19sec epoch 4999 loss -1.3164 17sec Adversarial training: 3th ensemble epoch 999 loss 0.1084 34sec epoch 1999 loss -0.2542 34sec epoch 2999 loss -1.0142 34sec epoch 3999 loss -1.1467 34sec epoch 4999 loss -1.2141 33sec Regular training: 3th ensemble epoch 999 loss 0.0915 19sec epoch 1999 loss -0.3137 19sec epoch 2999 loss -1.1367 19sec epoch 3999 loss -1.3088 19sec epoch 4999 loss -1.2444 19sec Adversarial training: 4th ensemble epoch 999 loss 0.1462 34sec epoch 1999 loss -0.6342 34sec epoch 2999 loss -1.1118 34sec epoch 3999 loss -1.1936 34sec epoch 4999 loss -1.1991 34sec Regular training: 4th ensemble epoch 999 loss 0.0635 19sec epoch 1999 loss -0.6976 18sec epoch 2999 loss -1.1754 18sec epoch 3999 loss -1.2974 19sec epoch 4999 loss -1.3013 19sec Prediction on testing data Here I estimate the mean $\\mu_m (X)$ and variance $\\sigma_m&#94;2(X)$ of each model. In [6]: x_test = np . arange ( - 2.5 , 2.5 , inc ) . reshape ( - 1 , 1 ) y_test = sinfun ( x_test , noise = 0 ) def get_predicted_values ( x_test , result_NLL_adv ): y_mean_pred , y_sigma_pred = [], [] for iens , mod in enumerate ( result_NLL_adv ): ( sess , lvalues , tensors ) = mod ( inputs , _ , _ , y , _ ) = tensors ( x_input , _ , _ ) = inputs ( y_mean , y_sigma ) = y y_mean_pred . append ( sess . run ( y_mean , feed_dict = { x_input : x_test })) y_sigma_pred . append ( sess . run ( y_sigma , feed_dict = { x_input : x_test })) y_mean_pred = np . array ( y_mean_pred ) . squeeze () y_sigma_pred = np . array ( y_sigma_pred ) . squeeze () return ( y_mean_pred , y_sigma_pred ) prediction_adv = get_predicted_values ( x_test , result_NLL_adv ) prediction_ = get_predicted_values ( x_test , result_NLL ) From mean $\\mu_m (X)$ and variance $\\sigma_m&#94;2(X)$, I will estimate the aggregate estimates: \\begin{array}{rcll} \\mu_{*} (X) &=& \\frac{1}{M}\\sum_{m=1}&#94;M \\mu_m (X)\\\\ \\sigma_{*}&#94;2 (X) &=& \\frac{1}{M} \\sum_{m=1}&#94;M \\left( \\sigma_m&#94;2(X) + \\mu_m (X)&#94;2 \\right) - \\mu_{*} (X)&#94;2\\\\ \\end{array} In [7]: def get_marginal_var ( y_mean , y_sigma ): ''' y_mean : M by N y_sigma : M by N ''' y_mean_mar = np . mean ( y_mean , axis = 0 ) ## N by 1 y_sigma_mar = np . mean ( y_mean ** 2 + y_sigma , axis = 0 ) - y_mean_mar ** 2 return ( y_mean_mar , y_sigma_mar ) y_mean_adv , y_sigma_adv = get_marginal_var ( * prediction_adv ) y_mean_ , y_sigma_ = get_marginal_var ( * prediction_ ) Visualize the model performance Results Ensemble is neccesary to have a sensible error bound. With only single model, the variance seems too small. Adversarial samples seem not to affect the error bound and estiamtes much for this simple data analysis. In [8]: def plot ( x_train , y_train , prediction_adv , iensemble = 3 ): ''' y_mean_mar, y_sigma_mar = prediction_adv ''' def plotdata ( axs ): axs . scatter ( x_train , y_train , s = 0.5 , alpha = 0.5 ) def ploterrorbar ( x_test , y_test , y_sigma ): axs . plot ( x_test , y_test ) axs . errorbar ( x_test , y_test , yerr = 1.96 * np . sqrt ( y_sigma ), capsize = 0 , alpha = 0.1 , label = \"estimated_y +- 1.96*sigma\" ) fig = plt . figure ( figsize = ( 15 , 15 )) ## Plot training data with estimated y from each model axs = fig . add_subplot ( 4 , 1 , 1 ) plotdata ( axs ) for i , y_pre in enumerate ( prediction_adv [ 0 ]): axs . plot ( x_test , y_pre , label = \" Ensemble {:}\" . format ( i )) axs . set_title ( \"Estimated y from each ensemble model\" ) axs . legend () ## Plot estimated sigma axs = fig . add_subplot ( 4 , 1 , 2 ) for i , y_sig in enumerate ( prediction_adv [ 1 ]): axs . plot ( x_test , y_sig , label = \" Ensemble {:}\" . format ( i )) axs . set_title ( \"Estimated sigma from each ensemble model\" ) ## Plot a single estimated y with error bar axs = fig . add_subplot ( 4 , 1 , 3 ) plotdata ( axs ) ploterrorbar ( x_test , prediction_adv [ 0 ][ iensemble ], prediction_adv [ 1 ][ iensemble ]) axs . set_title ( \"Estimated y with estimated sigma from one of the ensemble models\" ) ## Plot axs = fig . add_subplot ( 4 , 1 , 4 ) axs . plot ( x_test , y_mean_adv , label = \" Marginal Mean {:}\" . format ( i )) plotdata ( axs ) ploterrorbar ( x_test , y_mean_adv , y_sigma_adv ) axs . set_title ( \"Aggregate Prediction\" ) plt . legend () plt . show () const = 80 iensemble = 1 print ( \"~\" * const ) print ( \"Visualization of results with adversarial training\" ) print ( \"~\" * const ) plot ( x_train , y_train , prediction_adv , iensemble = iensemble ) print ( \"~\" * const ) print ( \"Visualization of results without adversarial training\" ) print ( \"~\" * const ) plot ( x_train , y_train , prediction_ , iensemble = iensemble ) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Visualization of results with adversarial training ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Visualization of results without adversarial training ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Evaluate uncertainty using ensemble models with likelihood loss and adverserial training"},{"url":"Generate-adversarial-examples-using-TensorFlow.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In this blog post, we will generate adversarial images for a pre-trained Keras facial keypoint detection model. What is adversarial images? According to OpenAI , adversarial examples are: inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake; they're like optical illusions for machines. Explaining and Harnessing Adversarial Examples proposed the fast gradient sign method that generates an adversarial example as: $$ X' = X + \\epsilon sign \\left(\\frac{d \\textrm{loss}(X)}{dX} \\right) $$ where $\\epsilon$ is a small value such that the max-norm of the perturbation is bounded. This means that the adversarial perturbation creates a new training example by adding a perturbation along a direction which the network is likely to increase the loss. Explaining and Harnessing Adversarial Examples mentioned that by including the adversarial examples in the training data, the classifier becomes more robust. This is adversarial training. Adversarial examples using TensorFlow The goal of this blog is to understand and create adversarial examples using TensorFlow. I will use TensorFlow rather than Keras as writing it in Keras requires Keras's backend functions which essentially requires using Tensorflow backend functions. Rather than mixing up the two frameworks, I will stick to TensorFlow. I will use my facial keypoint detection model developed using Kaggle's facial keypoint detection data as a base model for which I will create adversarial examples. See Achieving Top 23% in Kaggle's Facial Keypoints Detection with Keras + Tensorflow to learn how I developed this model. As this model is developed in Keras, the first half of the blog discusses how to read in the Keras's pre-trained model, and load TensorFlow's model. If you already have a TensorFlow model in hand, I recommend you to start reading it from the section \"Create a class for adversarial examples with TensorFlow deep learning model\". Reference Explaining and Harnessing Adversarial Examples In [1]: import matplotlib.pyplot as plt import pandas as pd import numpy as np import tensorflow as tf import os , sys os . environ [ \"CUDA_DEVICE_ORDER\" ] = \"PCI_BUS_ID\" config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"1\" tf . Session ( config = config ) print ( \"python {} \" . format ( sys . version )) print ( \"tensorflow version {} \" . format ( tf . __version__ )) ## change the directory os . chdir ( \"../FacialKeypoint/\" ) python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] tensorflow version 1.2.0 Read in a Keras's pre-trained model and save it as a TensorFlow model This deep learning model takes input image of size (94,94,1) and output the standardized (x,y)-coordinates of the 15 landmark. That is, there are 30 targets to estimate (x-coordinate of left eye, y-coordinate of left eye, x-coordinate of nose,....). The model was developed at Achieving Top 23% in Kaggle's Facial Keypoints Detection with Keras + Tensorflow . In [2]: from keras.models import model_from_json def load_model ( name ): model = model_from_json ( open ( name + '_architecture.json' ) . read ()) model . load_weights ( name + '_weights.h5' ) return ( model ) model = load_model ( \"model4\" ) model . summary () Using TensorFlow backend. _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_22 (Conv2D) (None, 94, 94, 32) 320 _________________________________________________________________ activation_37 (Activation) (None, 94, 94, 32) 0 _________________________________________________________________ max_pooling2d_22 (MaxPooling (None, 47, 47, 32) 0 _________________________________________________________________ conv2d_23 (Conv2D) (None, 46, 46, 64) 8256 _________________________________________________________________ activation_38 (Activation) (None, 46, 46, 64) 0 _________________________________________________________________ max_pooling2d_23 (MaxPooling (None, 23, 23, 64) 0 _________________________________________________________________ conv2d_24 (Conv2D) (None, 22, 22, 128) 32896 _________________________________________________________________ activation_39 (Activation) (None, 22, 22, 128) 0 _________________________________________________________________ max_pooling2d_24 (MaxPooling (None, 11, 11, 128) 0 _________________________________________________________________ flatten_8 (Flatten) (None, 15488) 0 _________________________________________________________________ dense_31 (Dense) (None, 500) 7744500 _________________________________________________________________ activation_40 (Activation) (None, 500) 0 _________________________________________________________________ dense_32 (Dense) (None, 500) 250500 _________________________________________________________________ activation_41 (Activation) (None, 500) 0 _________________________________________________________________ dense_33 (Dense) (None, 30) 15030 ================================================================= Total params: 8,051,502 Trainable params: 8,051,502 Non-trainable params: 0 _________________________________________________________________ Create a directory to save a TensorFlow model. In [3]: outdir = \"model4_tf\" try : os . mkdir ( outdir ) except : pass The codes in the next block are mostly based on amir-abdi's Github code . In [4]: # Write the graph in binary .pb file from tensorflow.python.framework import graph_util from tensorflow.python.framework import graph_io from keras import backend as K prefix = \"simple_cnn\" name = 'output_graph.pb' # Alias the outputs in the model - this sometimes makes them easier to access in TF pred = [] pred_node_names = [] for i , o in enumerate ( model . outputs ): pred_node_names . append ( prefix + '_' + str ( i )) pred . append ( tf . identity ( o , name = pred_node_names [ i ])) print ( 'Output nodes names are: ' , pred_node_names ) sess = K . get_session () # Write the graph in human readable # f = 'graph_def_for_reference.pb.ascii' # tf.train.write_graph(sess.graph.as_graph_def(), outdir, f, as_text=True) # print('Saved the graph definition in ascii format at: ', os.path.join(outdir, f)) constant_graph = graph_util . convert_variables_to_constants ( sess , sess . graph . as_graph_def (), pred_node_names ) graph_io . write_graph ( constant_graph , outdir , name , as_text = False ) ## Finally delete the Keras's session K . clear_session () ('Output nodes names are: ', ['simple_cnn_0']) INFO:tensorflow:Froze 12 variables. Converted 12 variables to const ops. Create a class for adversarial examples with TensorFlow deep learning model Read in TensorFlow's model In [5]: outdir = \"../FacialKeypoint/model4_tf\" name = 'output_graph.pb' ## Make sure that there are no defult graph tf . reset_default_graph () def load_graph ( model_name ): #graph = tf.Graph() graph = tf . get_default_graph () graph_def = tf . GraphDef () with open ( model_name , \"rb\" ) as f : graph_def . ParseFromString ( f . read ()) with graph . as_default (): tf . import_graph_def ( graph_def ) return graph my_graph = load_graph ( model_name = os . path . join ( outdir , name )) In order to generate adversarial examples, I need to calculate the gradient of loss with respect to the image as: $$ \\frac{d \\textrm{loss}(y,X)}{dX} $$ where my loss function for the landmark detection model was MSE: $$ \\textrm{loss}(y,X) = (y - f(X))&#94;2 $$ For the gradient calculation, I need a input tensor (import/conv2d_22_input) and output tensor (import/simple_cnn_0) The following codes show all the operations' names in the Tensor's graph. We can find the tensors that we need: $f(X)$ = import/simple_cnn_0 $X$ = import/conv2d_22_input In [6]: for i , op in enumerate ( tf . get_default_graph () . get_operations ()): print \" {: 3.0f} : {} \" . format ( i , op . name ) 0: import/conv2d_22_input 1: import/conv2d_22/kernel 2: import/conv2d_22/kernel/read 3: import/conv2d_22/bias 4: import/conv2d_22/bias/read 5: import/conv2d_22/convolution 6: import/conv2d_22/BiasAdd 7: import/activation_37/Relu 8: import/max_pooling2d_22/MaxPool 9: import/conv2d_23/kernel 10: import/conv2d_23/kernel/read 11: import/conv2d_23/bias 12: import/conv2d_23/bias/read 13: import/conv2d_23/convolution 14: import/conv2d_23/BiasAdd 15: import/activation_38/Relu 16: import/max_pooling2d_23/MaxPool 17: import/conv2d_24/kernel 18: import/conv2d_24/kernel/read 19: import/conv2d_24/bias 20: import/conv2d_24/bias/read 21: import/conv2d_24/convolution 22: import/conv2d_24/BiasAdd 23: import/activation_39/Relu 24: import/max_pooling2d_24/MaxPool 25: import/flatten_8/Shape 26: import/flatten_8/strided_slice/stack 27: import/flatten_8/strided_slice/stack_1 28: import/flatten_8/strided_slice/stack_2 29: import/flatten_8/strided_slice 30: import/flatten_8/Const 31: import/flatten_8/Prod 32: import/flatten_8/stack/0 33: import/flatten_8/stack 34: import/flatten_8/Reshape 35: import/dense_31/kernel 36: import/dense_31/kernel/read 37: import/dense_31/bias 38: import/dense_31/bias/read 39: import/dense_31/MatMul 40: import/dense_31/BiasAdd 41: import/activation_40/Relu 42: import/dense_32/kernel 43: import/dense_32/kernel/read 44: import/dense_32/bias 45: import/dense_32/bias/read 46: import/dense_32/MatMul 47: import/dense_32/BiasAdd 48: import/activation_41/Relu 49: import/dense_33/kernel 50: import/dense_33/kernel/read 51: import/dense_33/bias 52: import/dense_33/bias/read 53: import/dense_33/MatMul 54: import/dense_33/BiasAdd 55: import/simple_cnn_0 Extract only tensors that are needed for calculating the gradients: In [7]: input_op = my_graph . get_operation_by_name ( \"import/conv2d_22_input\" ) output_op = my_graph . get_operation_by_name ( \"import/simple_cnn_0\" ) ops = ( input_op , output_op ) Following class computes the adversarial examples for given value of $\\epsilon$, image $X$ and true landmark coordinate vector $y$. In [8]: class AdversarialImage ( object ): def __init__ ( self , inp , out , eps = 0.01 ): ''' inp : input tensor (image) out : output tensor (y_pred) eps : scalar ''' self . inp = inp . outputs [ 0 ] self . out = out . outputs [ 0 ] self . define_aimage_tensor ( float ( eps )) def mse_tf ( self , y_pred , y_test , verbose = True ): ''' y_pred : tensor y_test : tensor having the same shape as y_pred ''' ## element wise square minus = tf . constant ( - 1.0 ) m_y_test = tf . scalar_mul ( minus , y_test ) square = tf . square ( tf . add ( y_pred , m_y_test )) ## preserve the same shape as y_pred.shape ## mean across the final dimensions ms = tf . reduce_mean ( square ) return ( ms ) def define_aimage_tensor ( self , eps ): ''' Define a graph to output adversarial image Xnew = X + eps * sign(dX) X : np.array of image of shape (None,height, width,n_channel) y : np.array containing the true landmark coordinates (None, 30) ''' ## get list of target yshape = [ None ] + [ int ( i ) for i in self . out . get_shape ()[ 1 :]] eps_tf = tf . constant ( eps , name = \"epsilon\" ) y_true_tf = tf . placeholder ( tf . float32 , yshape ) y_pred_tf = self . out loss = self . mse_tf ( y_pred_tf , y_true_tf ) ## tensor that calculate the gradient of loss with respect to image i.e., dX grad_tf = tf . gradients ( loss ,[ self . inp ]) grad_sign_tf = tf . sign ( grad_tf ) grad_sign_eps_tf = tf . scalar_mul ( eps_tf , grad_sign_tf ) new_image_tf = tf . add ( grad_sign_eps_tf , self . inp ) self . y_true = y_true_tf self . eps = eps_tf self . aimage = new_image_tf self . added_noise = grad_sign_eps_tf def predict ( self , X ): with tf . Session () as sess : y_pred = sess . run ( self . out , feed_dict = { self . inp : X }) return ( y_pred ) def get_aimage ( self , X , y , added_noise = False ): tensor2eval = [ self . aimage ] if added_noise : tensor2eval . append ( self . added_noise ) with tf . Session () as sess : result = sess . run ( tensor2eval , feed_dict = { self . inp : X , self . y_true : y }) for i in range ( len ( result )): result [ i ] = result [ i ] . reshape ( * X . shape ) return ( result ) Load Kaggle's facial landmark detection data Finally, let's generate the adversarial examples using Kaggle's facial landmark detection data. The data extraction and transformation process is the same as my previous blogs: Achieving Top 23% in Kaggle's Facial Keypoints Detection with Keras + Tensorflow Achieving top 5 in Kaggle's facial keypoints detection using FCN . I will use the same ETL functions as before. In [9]: def plot_sample ( X , y , axs ): ''' kaggle picture is 96 by 96 y is rescaled to range between -1 and 1 ''' axs . imshow ( X . reshape ( 96 , 96 ), cmap = \"gray\" ) axs . scatter ( 48 * y [ 0 :: 2 ] + 48 , 48 * y [ 1 :: 2 ] + 48 ) def load ( test = False , cols = None ): \"\"\" load test/train data cols : a list containing landmark label names. If this is specified, only the subset of the landmark labels are extracted. for example, cols could be: [left_eye_center_x, left_eye_center_y] return: X: 2-d numpy array (Nsample, Ncol*Nrow) y: 2-d numpy array (Nsample, Nlandmarks*2) In total there are 15 landmarks. As x and y coordinates are recorded, u.shape = (Nsample,30) \"\"\" from pandas import read_csv from sklearn.utils import shuffle fname = FTEST if test else FTRAIN df = read_csv ( os . path . expanduser ( fname )) df [ 'Image' ] = df [ 'Image' ] . apply ( lambda im : np . fromstring ( im , sep = ' ' )) if cols : df = df [ list ( cols ) + [ 'Image' ]] myprint = df . count () myprint = myprint . reset_index () print ( myprint ) ## row with at least one NA columns are removed! df = df . dropna () X = np . vstack ( df [ 'Image' ] . values ) / 255. # changes valeus between 0 and 1 X = X . astype ( np . float32 ) if not test : # labels only exists for the training data ## standardization of the response y = df [ df . columns [: - 1 ]] . values y = ( y - 48 ) / 48 # y values are between [-1,1] X , y = shuffle ( X , y , random_state = 42 ) # shuffle data y = y . astype ( np . float32 ) else : y = None return X , y def load2d ( test = False , cols = None ): re = load ( test , cols ) X = re [ 0 ] . reshape ( - 1 , 96 , 96 , 1 ) y = re [ 1 ] return X , y FTRAIN = 'data/training.csv' FTEST = 'data/test.csv' FIdLookup = 'data/IdLookupTable.csv' X , y = load2d ( test = False ) index 0 0 left_eye_center_x 7039 1 left_eye_center_y 7039 2 right_eye_center_x 7036 3 right_eye_center_y 7036 4 left_eye_inner_corner_x 2271 5 left_eye_inner_corner_y 2271 6 left_eye_outer_corner_x 2267 7 left_eye_outer_corner_y 2267 8 right_eye_inner_corner_x 2268 9 right_eye_inner_corner_y 2268 10 right_eye_outer_corner_x 2268 11 right_eye_outer_corner_y 2268 12 left_eyebrow_inner_end_x 2270 13 left_eyebrow_inner_end_y 2270 14 left_eyebrow_outer_end_x 2225 15 left_eyebrow_outer_end_y 2225 16 right_eyebrow_inner_end_x 2270 17 right_eyebrow_inner_end_y 2270 18 right_eyebrow_outer_end_x 2236 19 right_eyebrow_outer_end_y 2236 20 nose_tip_x 7049 21 nose_tip_y 7049 22 mouth_left_corner_x 2269 23 mouth_left_corner_y 2269 24 mouth_right_corner_x 2270 25 mouth_right_corner_y 2270 26 mouth_center_top_lip_x 2275 27 mouth_center_top_lip_y 2275 28 mouth_center_bottom_lip_x 7016 29 mouth_center_bottom_lip_y 7016 30 Image 7049 Let's instantiate two Adversarial Image class objects with two different values of epsilon. Notice that I instantiated the objects with positive and negative epsilon values. By changing the direction of sign to negative I can create \"good\" or \"friendly\" or \"anti-adversarial\" examples that can decrease the loss. In [10]: AIbad = AdversarialImage ( * ops , eps = 0.01 ) AIgood = AdversarialImage ( * ops , eps =- 0.01 ) Generate adversarial images as well as good images, and visualize them together with RMSE. Adversarial images increase the RMSE while \"good\" images decreases the RMSE. However, these images look identical to the original image to me! In [11]: def getrmse ( y_pred , y_true ): return ( np . sqrt ( np . mean (( y_pred - y_true ) ** 2 ))) def getRMSE ( y_pred , y_true ): return ( \"RMSE: {:4.3f} \" . format ( getrmse ( y_pred , y_true ))) Nplot = 5 inds = np . random . choice ( X . shape [ 0 ], Nplot , replace = False ) count = 1 plt . close () fig = plt . figure ( figsize = ( 20 , 20 )) for irow in inds : Xi , yi = X [[ irow ]], y [[ irow ]] ## original image y_predi = AIgood . predict ( Xi ) ## Good image ( X_good , noise_good ) = AIgood . get_aimage ( Xi , yi , added_noise = True ) y_predi_good = AIgood . predict ( X_good ) ## Adversarial image ( X_bad , noise_bad ) = AIbad . get_aimage ( Xi , yi , added_noise = True ) y_predi_bad = AIbad . predict ( X_bad ) ## ======== ## ## Plotting ## ======== ## ## original image axs = fig . add_subplot ( Nplot , 5 , count ) plot_sample ( Xi [ 0 ], y_predi [ 0 ], axs ) axs . set_title ( \"original\" + getRMSE ( y_predi , yi )) count += 1 ## noise for bad image axs = fig . add_subplot ( Nplot , 5 , count ) axs . imshow ( noise_bad . reshape ( 96 , 96 ), cmap = \"gray\" ) axs . set_title ( \"Noise for adversarial image\" ) count += 1 ## bad image axs = fig . add_subplot ( Nplot , 5 , count ) plot_sample ( X_bad [ 0 ], y_predi_bad [ 0 ], axs ) axs . set_title ( \"Adversarial image: \" + getRMSE ( y_predi_bad , yi )) count += 1 ## noise for good image axs = fig . add_subplot ( Nplot , 5 , count ) axs . imshow ( noise_good . reshape ( 96 , 96 ), cmap = \"gray\" ) axs . set_title ( \"Noise for good image\" ) count += 1 ## good image axs = fig . add_subplot ( Nplot , 5 , count ) plot_sample ( X_good [ 0 ], y_predi_good [ 0 ], axs ) axs . set_title ( \"Good image: \" + getRMSE ( y_predi_good , yi )) count += 1 plt . show () Think about the effect of $\\epsilon$ on adversarial images To create adversarial example, it is very important to choose \"good\" value of $\\epsilon$, that determines the amount of noise added to the original image $X$. The maginitude of $\\epsilon$ should depend on the range of X. Our pixcel values in X ranges from 0 to 1 after standardization. See histogram below. In [12]: vecX = X . flatten () plt . hist ( vecX [ ~ np . isnan ( vecX )]) plt . xlabel ( \"X\" ) plt . show () Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles previously give recommendation to the default values of $\\epsilon$ in his adversarial training. Recommended default values are... $\\epsilon = 1\\%$ of input range of the corresponding dimension (e.g. 2.55 if input range is [0,255]). Therefore in our data, it would be interesting to see the effect of $\\epsilon$ around the values of 0.01. Let's create gif that shows the effect of $\\epsilon$ on adversarial image and the RMSE. This will be the gif shown at the beggining of the blog. In [13]: dir_images = \"adversarial_image_gif/\" try : os . mkdir ( dir_images ) except : pass In [14]: irow = 1072 Xi , yi = X [[ irow ]], y [[ irow ]] ## original image y_predi = AIbad . predict ( Xi ) rmse_base = getrmse ( y_predi , yi ) ## range of epsilon xs = np . arange ( 0 , 0.05 , 0.0005 ) rmses = np . array ([ np . NaN ] * len ( xs )) for ieps , eps in enumerate ( xs ): AIbad = AdversarialImage ( * ops , eps = eps ) ## Adversarial image ( X_bad , noise_bad ) = AIbad . get_aimage ( Xi , yi , added_noise = True ) y_predi_bad = AIbad . predict ( X_bad ) ## ======== ## ## Plotting ## ======== ## count = 1 fig = plt . figure ( figsize = ( 15 , 8 )) ## noise for bad image rmses [ ieps ] = getrmse ( y_predi_bad , yi ) axs = fig . add_subplot ( 1 , 2 , count ) axs . imshow ( X_bad [ 0 ] . reshape ( 96 , 96 ), cmap = \"gray\" ) axs . scatter ( 48 * y_predi_bad [ 0 ][ 0 :: 2 ] + 48 , 48 * y_predi_bad [ 0 ][ 1 :: 2 ] + 48 , label = \"y_pred\" ) axs . scatter ( 48 * yi [ 0 ][ 0 :: 2 ] + 48 , 48 * yi [ 0 ][ 1 :: 2 ] + 48 , label = \"true\" ) axs . set_title ( \"Adversarial image: RMSE {:5.4f} , eps: {:5.4f} \" . format ( rmses [ ieps ], eps )) plt . legend () count += 1 ## noise for bad image axs = fig . add_subplot ( 1 , 2 , count ) axs . plot ( xs , rmses ) axs . set_xlim ([ np . min ( xs ), np . max ( xs )]) axs . set_ylim ([ rmse_base , rmse_base * 3 ]) axs . set_ylabel ( \"RMSEs\" ) axs . set_xlabel ( \"epsilon\" ) axs . set_title ( \"RMSE without noise = {:5.4f} \" . format ( rmses [ 0 ])) count += 1 plt . savefig ( dir_images + '/ {:05.0f} .png' . format ( ieps ), bbox_inches = 'tight' , pad_inches = 0 ) /home/fairy/anaconda2/lib/python2.7/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). max_open_warning, RuntimeWarning) Create a gif presented at the beginning of this blog In [15]: def makegif ( dir_images ): import imageio filenames = np . sort ( os . listdir ( dir_images )) filenames = [ fnm for fnm in filenames if \".png\" in fnm ] with imageio . get_writer ( dir_images + '/image.gif' , mode = 'I' ) as writer : for filename in filenames : image = imageio . imread ( dir_images + filename ) writer . append_data ( image ) os . remove ( dir_images + filename ) makegif ( dir_images ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Generate adversarial examples using TensorFlow"},{"url":"Create-a-neural-net-with-a-negative-log-likelihood-as-a-loss.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In this blog, I will create a deep learning model that uses the negative log-likelihood of Gaussian distribution as a loss. For this purpose, I will use Tensorflow. Why not Keras? Keras has been my first-choice deep learning framework in the last 1 year. However, if you want to create personal loss functions or layers, Keras requires to use backend functions written in either TensorFlow or Theano. As the negative log-likelihood of Gaussian distribution is not one of the available loss in Keras, I need to implement it in Tensorflow which is often my backend. So this motivated me to learn Tensorflow and write everything in Tensorflow rather than mixing up two frameworks. So this blog assumes that this is your first time using Tensorflow. Reference Tensorflow tutorial In [1]: import matplotlib.pyplot as plt import pandas as pd import numpy as np import tensorflow as tf import os , sys os . environ [ \"CUDA_DEVICE_ORDER\" ] = \"PCI_BUS_ID\" config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"1\" tf . Session ( config = config ) print ( \"python {}\" . format ( sys . version )) print ( \"tensorflow version {}\" . format ( tf . __version__ )) python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] tensorflow version 1.2.1 For those familiar with Keras's functional API, one of the roadblocks in using Tensorflow is understanding the concept of \"Tensor\". In Keras, defining a deep learning model in Sequential or Functional APIs do not require new data structure concept. When training the model, the input data is a numpy array and output from Keras model is also numpy array. So there is no concept of the \"Tensor\". However, in Tensorflow, the computational graph or networks are defined using Tensor data structure: TensorFlow programs use a tensor data structure to represent all data â€” only tensors are passed between operations in the computation graph. You can think of a TensorFlow tensor as an n-dimensional array or list. As the purpose of the tensors are to define the graph, and it is an arbitrary array or matrix that represent the graph connections, it does not hold actual values. Therefore, you cannot print the Tensor like the numpy array. To understand, let's start with creating our familiar numpy array, and convert it to Tensor. Let's look at an example. In [2]: y1_np = np . arange ( 3 * 2 * 1 ) . reshape ( 3 , 2 , 1 ) y1_np *= y1_np y2_np = np . arange ( 3 * 2 * 1 ) . reshape ( 3 , 2 , 1 ) print ( \"-\" * 10 ) print ( \"y1_np.shape={}\" . format ( y1_np . shape )) print ( y1_np ) print ( \"-\" * 10 ) print ( \"y2_np.shape={}\" . format ( y2_np . shape )) print ( y2_np ) ---------- y1_np.shape=(3, 2, 1) [[[ 0] [ 1]] [[ 4] [ 9]] [[16] [25]]] ---------- y2_np.shape=(3, 2, 1) [[[0] [1]] [[2] [3]] [[4] [5]]] I will create a very simple computational graph which simply convert a numpy array to constant immutable tensor. You need to specify the data type and shape of the tensor. You see that printing the tensors do not show any actual array values! In [3]: y1_tf = tf . constant ( y1_np , shape = y1_np . shape , dtype = \"float64\" ) y2_tf = tf . constant ( y2_np , shape = y2_np . shape , dtype = \"float64\" ) print ( \"-\" * 10 ) print ( \"y1_tf.shape={}\" . format ( y1_tf . get_shape ())) print ( y1_tf ) print ( \"-\" * 10 ) print ( \"y2_tf.shape={}\" . format ( y2_tf . get_shape ())) print ( y2_tf ) ---------- y1_tf.shape=(3, 2, 1) Tensor(\"Const:0\", shape=(3, 2, 1), dtype=float64) ---------- y2_tf.shape=(3, 2, 1) Tensor(\"Const_1:0\", shape=(3, 2, 1), dtype=float64) In order to see the values of Tensor, you need to start the session, and execute our graph (which is simply defining a constant tensor). The output of the session are numpy arrays. A session allows to execute graphs or part of graphs. It allocates resources (on one or more machines) for that and holds the actual values of intermediate results and variables. In [4]: sess = tf . InteractiveSession () with tf . Session () as sess : y1_tf_back_to_np = sess . run ( y1_tf ) y2_tf_back_to_np = sess . run ( y2_tf ) print ( \"-\" * 10 ) print ( \"y1_tf_back_to_np.shape={}\" . format ( y1_tf_back_to_np . shape )) print ( y1_tf_back_to_np ) print ( \"-\" * 10 ) print ( \"y2_tf_back_to_np.shape={}\" . format ( y2_tf_back_to_np . shape )) print ( y2_tf_back_to_np ) ---------- y1_tf_back_to_np.shape=(3, 2, 1) [[[ 0.] [ 1.]] [[ 4.] [ 9.]] [[ 16.] [ 25.]]] ---------- y2_tf_back_to_np.shape=(3, 2, 1) [[[ 0.] [ 1.]] [[ 2.] [ 3.]] [[ 4.] [ 5.]]] Calculate mean square error using Tensorflow tf.constant tensor as input OK. Let's make our graph more complicated and calculate the mean square error. Make sure to use special tensorflow functions (e.g., tf.square and tf.reduce_mean) to do the calculation with Tensor. In [5]: def mse ( y_pred , y_test , verbose = True ): ''' y_pred : tensor y_test : tensor having the same shape as y_pred ''' ## element wise square square = tf . square ( y_pred - y_test ) ## preserve the same shape as y_pred.shape ## mean across the final dimensions ms = tf . reduce_mean ( square ) return ( ms ) y1_tf = tf . constant ( y1_np , shape = y1_np . shape , dtype = \"float64\" ) y2_tf = tf . constant ( y2_np , shape = y2_np . shape , dtype = \"float64\" ) mse_tf = mse ( y1_tf , y2_tf ) with tf . Session () as sess : mse_np = sess . run ( mse_tf ) print ( \"======\" ) print ( \"Result\" ) print ( \"======\" ) print ( \"MSE={}\" . format ( mse_np )) print ( \"the numpy calculation must be the same as the tensor flow calculation:\" ) print ( mse_np == np . mean (( y1_np - y2_np ) ** 2 )) ====== Result ====== MSE=97.3333333333 the numpy calculation must be the same as the tensor flow calculation: True tf.placeholder tensor as input You can also define the input to the tensor within session by using tf.placeholder: A placeholder is simply a variable that we will assign data to at a later date. It allows us to create our operations and build our computation graph, without needing the data. In TensorFlow terminology, we then feed data into the graph through these placeholders. The 1st dimension of the placeholder can be defined during session. In [6]: Nsample = 10 n_feature = 2 ## Define Graph y1_tf_var = tf . placeholder ( tf . float32 ,[ None , n_feature ]) y2_tf_var = tf . placeholder ( tf . float32 ,[ None , n_feature ]) mse_tf = mse ( y1_tf_var , y2_tf_var ) ## Input y1_np = np . random . rand ( * [ Nsample , n_feature ]) y2_np = np . random . rand ( * [ Nsample , n_feature ]) ## Define session with tf . Session () as sess : mse_np = sess . run ( mse_tf , feed_dict = { y1_tf_var : y1_np , y2_tf_var : y2_np }) print ( \"======\" ) print ( \"Result\" ) print ( \"======\" ) print ( \"MSE={}\" . format ( mse_np )) print ( \"The numpy calculation must be the same as the tensor flow calculation:\" ) print ( np . abs ( mse_np - np . mean (( y1_np - y2_np ) ** 2 )) < 1E-6 ) ====== Result ====== MSE=0.200229600072 The numpy calculation must be the same as the tensor flow calculation: True A least square regrssion (feed forward network) Before diving into a deep learning model, let's solve simpler problem and fit a simple least squarea regression model to very small data. A simple least square regression can be thought as a feed-foward neural network model with no hidden layer. I will consider only 5 data points with 2 features. Generate sample points In [7]: ## input numpy array N = 5 n_feature = 2 X_np = np . random . rand ( N , n_feature ) y_np = ( X_np [:, 0 ] * ( - 3 ) + X_np [:, 1 ] * 2 ) . reshape ( - 1 , 1 ) fig = plt . figure ( figsize = ( 10 , 5 )) for i in range ( X_np . shape [ 1 ]): ax = fig . add_subplot ( 1 , X_np . shape [ 1 ], i + 1 ) ax . plot ( X_np [:, i ], y_np , \"p\" ) ax . set_xlabel ( \"x{}\" . format ( i )) ax . set_ylabel ( \"y\" ) plt . show () We will use tf.Variable for the learning parameters so that the tensors are mutable. $ X_i \\in R&#94;{(1,2)}$ : input tf.placeholder $ w \\in R&#94;{(2,1)}$ : tf.Variable $ b \\in R&#94;{(1,)}$ : tf.Variable $ y_i \\in R&#94;{(1,)}$ : input tf.placeholder $ h_i \\in R&#94;{(1,)}$ $$ h_i = X_i w + b $$ $$ RMSE = \\sum_i (y_i - h_i)&#94;2 $$ In [8]: def weight_variable ( shape ): ## weight variable, initialized with truncated normal distribution initial = tf . truncated_normal ( shape , stddev = 0.01 , dtype = \"float32\" ) return tf . Variable ( initial ) def bias_variable ( shape ): initial = tf . constant ( 0.001 , shape = shape , dtype = \"float32\" ) return tf . Variable ( initial ) def fully_connected_layer ( h0 , n_h0 , n_h1 , verbose = True ): ''' h0 : tensor of shape (n_h0, n_h1) n_h0 : scalar n_h1 : scalar ''' W1 = weight_variable ([ n_h0 , n_h1 ]) b1 = bias_variable ([ n_h1 ]) if verbose : print ( \" h0.shape={}\" . format ( h0 . get_shape ())) print ( \" W1.shape={}\" . format ( W1 . get_shape ())) print ( \" b1.shape={}\" . format ( b1 . get_shape ())) h1 = tf . matmul ( h0 , W1 ) + b1 return ( h1 , ( W1 , b1 )) Define a graph In [9]: n_output = 1 ## Defines Graph x_tf = tf . placeholder ( tf . float32 ,[ None , n_feature ]) y_tf = tf . placeholder ( tf . float32 ,[ None , n_output ]) h_tf , ( W1_tf , b1_tf ) = fully_connected_layer ( x_tf , n_feature , n_output , verbose = True ) mse_tf = mse ( y_tf , h_tf ) ## a single step of the gradient descent to minimize the loss train_step = tf . train . GradientDescentOptimizer ( 0.01 ) . minimize ( mse_tf ) h0.shape=(?, 2) W1.shape=(2, 1) b1.shape=(1,) Define a session In [10]: Nepochs = 500 mses = [] with tf . Session () as sess : ## This line is necessary because the weights need to be initialized. tf . global_variables_initializer () . run () for _ in range ( Nepochs ): sess . run ( train_step , feed_dict = { x_tf : X_np , y_tf : y_np }) mse_np = sess . run ( mse_tf , feed_dict = { x_tf : X_np , y_tf : y_np }) mses . append ( mse_np ) h_np = sess . run ( h_tf , feed_dict = { x_tf : X_np }) W1_np = sess . run ( W1_tf , feed_dict = { x_tf : X_np }) b1_np = sess . run ( b1_tf , feed_dict = { x_tf : X_np }) y_est_np = np . dot ( X_np , W1_np ) + b1_np def check_equal ( A , B , small = 1E-5 ): TF = np . all ( np . abs ( A - B ) < small ) print ( \"Numerical Check, 2 objects are numerically the same: {}\" . format ( TF )) print ( \"======\" ) print ( \"Result\" ) print ( \"======\" ) print ( \"MSE is almost zero:\" ) print ( \"MSE={} \\n \" . format ( mse_np )) print ( \"Parameter: \\n W1={} \\n b1={}\" . format ( W1_np , b1_np )) print ( \"Estimate: \\n h={}\" . format ( h_np )) check_equal ( h_np , y_est_np ) plt . plot ( mses ) plt . title ( \"mean square loss\" ) plt . show () ====== Result ====== MSE is almost zero: MSE=0.197435796261 Parameter: W1=[[-1.89675987] [ 0.84892249]] b1=[-0.02030724] Estimate: h=[[-0.99182582] [-1.52101552] [ 0.17919284] [-1.13890231] [-0.02388124]] Numerical Check, 2 objects are numerically the same: True Evaluate the gradient of the MSE with respect to weight, bias and the input data It seems like you can readily evaluate gradients with Tensorflow! In [11]: grad_tf = tf . gradients ( mse_tf ,[ h_tf , W1_tf , b1_tf , x_tf ]) feed_dict = { x_tf : X_np , y_tf : y_np , W1_tf : W1_np , b1_tf : b1_np } with tf . Session () as sess : dh , dW1 , db1 , dx = sess . run ( grad_tf , feed_dict = feed_dict ) h_np = sess . run ( h_tf , feed_dict = feed_dict ) print ( \"==\" * 10 ) print ( \"dh:\" ) print ( dh ) dh_np = 2 * ( h_np - y_np ) / 5.0 check_equal ( dh , dh_np ) print ( \"==\" * 10 ) print ( \"dW1:\" ) print ( dW1 ) dW1_np = np . dot ( X_np . T , dh_np ) check_equal ( dW1 , dW1_np ) print ( \"==\" * 10 ) print ( \"db1:\" ) print ( db1 ) db1_np = np . sum ( dh_np ) check_equal ( db1 , db1_np ) print ( \"==\" * 10 ) print ( \"dx:\" ) print ( dx ) dx_np = np . dot ( W1_np , dh_np . T ) . T check_equal ( dx , dx_np ) ==================== dh: [[-0.0336736 ] [ 0.27773347] [-0.2658242 ] [ 0.05955472] [-0.07395665]] Numerical Check, 2 objects are numerically the same: True ==================== dW1: [[ 0.1964612 ] [-0.15411061]] Numerical Check, 2 objects are numerically the same: True ==================== db1: [-0.03616625] Numerical Check, 2 objects are numerically the same: True ==================== dx: [[ 0.06387073 -0.02858627] [-0.52679372 0.23577419] [ 0.50420469 -0.22566414] [-0.11296101 0.05055734] [ 0.14027801 -0.06278346]] Numerical Check, 2 objects are numerically the same: True Fit feed foward network with negative log likelihood as a loss Now, let's generate more complex data and fit more complex model on it. The input is a one dimensional sequence ranging between -2 and 2 with a jump between -1.5 and -1. The output time series is a function of the input at the corresponding time point with some noise. The noise level is high around input = 0. In [12]: ## Define x inc = 0.001 x_train = np . concatenate ([ np . arange ( - 2 , - 1.5 , inc ), np . arange ( - 1 , 2 , inc )]) x_train = x_train . reshape ( len ( x_train ), 1 ) ## Define y steps_per_cycle = 1 def sinfun ( xs , noise = 0.001 ): import random , math xs = xs . flatten () def randomNoise ( x ): ax = 2 - np . abs ( x ) wnoise = random . uniform ( - noise * ax , noise * ax ) return ( math . sin ( x * ( 2 * math . pi / steps_per_cycle ) ) + wnoise ) vec = [ randomNoise ( x ) - x for x in xs ] return ( np . array ( vec ) . flatten ()) y_train0 = sinfun ( x_train , noise = 0.5 ) y_train = y_train0 . reshape ( len ( y_train0 ), 1 ) print ( \" x_train.shape={}\" . format ( x_train . shape )) print ( \" y_train.shape={}\" . format ( y_train . shape )) ## Visualize the generated data (X,y) plt . figure ( figsize = ( 10 , 3 )) plt . scatter ( x_train , y_train , s = 0.5 ) plt . xlabel ( \"x_train\" ) plt . ylabel ( \"y_train\" ) plt . title ( \"The x values of the synthetic data ranges between {:4.3f} and {:4.3f}\" . format ( np . min ( x_train ), np . max ( x_train ))) plt . show () x_train.shape=(3500, 1) y_train.shape=(3500, 1) Model Now that we know Tensorflow, we are free to create and use any loss function for our model! To model this data, I will use a neural network that outputs two values in the final layer, corresponding to the predicted mean $\\mu(x)$ and $\\sigma&#94;2(x) > 0$. This essentially means that I am treating the observed value as a sample from a (heteroscedastic) Gaussian distribution with the predicted mean and variance as: $$ y_i | x_i \\overset{i.i.d.}{\\sim} N\\left( \\mu(x_i), \\sigma&#94;2(x_i)\\right) $$ Then we minimize the negative log-likelihood criterion, instead of using MSE as a loss: $$ NLL = \\sum_i \\frac{ \\textrm{log} \\left(\\sigma&#94;2(x_i)\\right) }{2} + \\frac{ \\left(y_i - \\mu(x_i) \\right)&#94;2 }{ 2 \\sigma&#94;2(x_i) } $$ Notice that when $\\sigma&#94;2(x_i)=1$, the first term of NLL becomes constant, and this loss function becomes essentially the same as the MSE. By modeling $\\sigma&#94;2(x_i)$, in theory, our model can weight more on the data points with less noise. For example, my data above has more variability when input variable is around origin. So ideally we want our model to weight less on this region. Use of NLL as a loss function is discussed at: Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles Estimating the mean and variance of the target probability distribution For compaison purpose, we will fit two models with two loss: MSE (i.e., $\\sigma&#94;2(x_i)=1$) NLL In [14]: def nll_gaussian ( y_pred_mean , y_pred_sd , y_test ): ## element wise square square = tf . square ( y_pred_mean - y_test ) ## preserve the same shape as y_pred.shape ms = tf . add ( tf . divide ( square , y_pred_sd ), tf . log ( y_pred_sd )) ## axis = -1 means that we take mean across the last dimension ## the output keeps all but the last dimension ## ms = tf.reduce_mean(ms,axis=-1) ## return scalar ms = tf . reduce_mean ( ms ) return ( ms ) def define_model ( n_feature , n_hs , n_output , verbose = True , NLL = True ): x_input = tf . placeholder ( tf . float32 , [ None , n_feature ]) y_input = tf . placeholder ( tf . float32 , [ None , 1 ]) h_previous = x_input n_h_previous = n_feature paras = [] for ilayer , n_h in enumerate ( n_hs , 1 ): if verbose : print ( \" layer:{}\" . format ( ilayer )) h , p = fully_connected_layer ( h_previous , n_h_previous , n_h , verbose ) h_previous = tf . nn . relu ( h ) n_h_previous = n_h paras . append ( p ) if verbose : print ( \" output layer for y_mean\" ) y_mean , p = fully_connected_layer ( h_previous , n_h_previous , n_output , verbose ) paras . append ( p ) if NLL : if verbose : print ( \" output layer for y_sigma\" ) y_sigma , p = fully_connected_layer ( h_previous , n_h_previous , n_output , verbose ) ## for numerical stability this enforce the variance to be more than 1E-4 y_sigma = tf . clip_by_value ( t = tf . exp ( y_sigma ), clip_value_min = tf . constant ( 1E-4 ), clip_value_max = tf . constant ( 1E+100 )) paras . append ( p ) loss = nll_gaussian ( y_mean , y_sigma , y_input ) y = [ y_mean , y_sigma ] else : loss = mse ( y_mean , y_input ) y = [ y_mean ] train_step = tf . train . GradientDescentOptimizer ( 0.01 ) . minimize ( loss ) inputs = [ x_input , y_input ] return ( loss , train_step , inputs , y , paras ) print ( \"___\" * 10 ) print ( \"loss=NLL\" ) loss , train_step , inputs , y , _ = define_model ( n_feature = 1 , n_hs = [ 500 , 300 , 100 ], n_output = 1 ) [ x_input , _ ] = inputs [ y_mean , y_sigma ] = y print ( \"___\" * 10 ) print ( \"loss=MSE\" ) ## For comparison purpose, we also consider MSE as a loss loss_mse , train_step_mse , inputs_mse , y_mse , _ = define_model ( n_feature = 1 , n_hs = [ 500 , 300 , 100 ], n_output = 1 , NLL = False ) [ x_input_mse , _ ] = inputs_mse [ y_mean_mse ] = y_mse ______________________________ loss=NLL layer:1 h0.shape=(?, 1) W1.shape=(1, 500) b1.shape=(500,) layer:2 h0.shape=(?, 500) W1.shape=(500, 300) b1.shape=(300,) layer:3 h0.shape=(?, 300) W1.shape=(300, 100) b1.shape=(100,) output layer for y_mean h0.shape=(?, 100) W1.shape=(100, 1) b1.shape=(1,) output layer for y_sigma h0.shape=(?, 100) W1.shape=(100, 1) b1.shape=(1,) ______________________________ loss=MSE layer:1 h0.shape=(?, 1) W1.shape=(1, 500) b1.shape=(500,) layer:2 h0.shape=(?, 500) W1.shape=(500, 300) b1.shape=(300,) layer:3 h0.shape=(?, 300) W1.shape=(300, 100) b1.shape=(100,) output layer for y_mean h0.shape=(?, 100) W1.shape=(100, 1) b1.shape=(1,) Training starts here In [15]: from sklearn.utils import shuffle n_epochs = 8000 n_batch = 500 def train ( train_step , loss , inputs , x_train , y_train , n_epochs , n_batch ): x_input , y_input = inputs sess = tf . InteractiveSession () tf . global_variables_initializer () . run () lvalues = [] for count in range ( n_epochs ): x_shuffle , y_shuffle = shuffle ( x_train , y_train ) for i in range ( 0 , x_train . shape [ 0 ], n_batch ): batch_xs = x_shuffle [ i : i + n_batch ] batch_ys = y_shuffle [ i : i + n_batch ] sess . run ( train_step , feed_dict = { x_input : batch_xs , y_input : batch_ys }) lv = sess . run ( loss , feed_dict = { x_input : x_train , y_input : y_train }) lvalues . append ( lv ) if count % 1000 == 1 : print ( \" epoch={:05.0f}: {:5.3f}\" . format ( count , lv )) return ( sess , lvalues , lv ) print ( \"loss=NLL\" ) sess , lvalues , lv = train ( train_step , loss , inputs , x_train , y_train , n_epochs , n_batch ) print ( \"loss=MSE\" ) sess_mse , lvalues_mse , lv_mse = train ( train_step_mse , loss_mse , inputs_mse , x_train , y_train , n_epochs , n_batch ) loss=NLL epoch=00001: 2.265 epoch=01001: 0.046 epoch=02001: -0.871 epoch=03001: -1.193 epoch=04001: -1.664 epoch=05001: -1.314 epoch=06001: -1.319 epoch=07001: -1.658 loss=MSE epoch=00001: 2.499 epoch=01001: 0.476 epoch=02001: 0.206 epoch=03001: 0.148 epoch=04001: 0.158 epoch=05001: 0.135 epoch=06001: 0.133 epoch=07001: 0.132 Training loss over epochs MSE can be thought as NLL with fixed variance 1. The model with NLL loss returns smaller NLL than the model with MSE loss. In [16]: plt . plot ( lvalues , label = \"loss=NLL\" ) plt . plot ( lvalues_mse , label = \"loss=MSE\" ) plt . title ( \"loss\" ) plt . xlabel ( \"epochs\" ) plt . legend () plt . show () Prediction with testing set The input of the testing set is a sequence ranging between -2.5 and 2.5 with increment of 0.01. Notice that the RMSE on the testset is smaller by the model with NLL loss than the model with MSE as a loss This is because the model with NLL loss has more reasonable assumption; variance depends on the input value. In [17]: x_test = np . arange ( - 2.5 , 2.5 , inc ) . reshape ( - 1 , 1 ) y_test = sinfun ( x_test , noise = 0 ) print ( \"**\" ) print ( \"Model with NLL as a loss\" ) y_mean_pred = sess . run ( y_mean , feed_dict = { x_input : x_test }) y_sigma_pred = sess . run ( y_sigma , feed_dict = { x_input : x_test }) print ( \"RMSE: {}\" . format ( np . sqrt ( np . mean (( y_test - y_mean_pred ) ** 2 )))) print ( \"**\" ) print ( \"Model with MSE as a loss\" ) y_mean_pred_mse = sess_mse . run ( y_mean_mse , feed_dict = { x_input_mse : x_test }) print ( \"RMSE: {}\" . format ( np . sqrt ( np . mean (( y_test - y_mean_pred_mse ) ** 2 )))) ** Model with NLL as a loss RMSE: 2.14780967639 ** Model with MSE as a loss RMSE: 2.27695851403 Plot of estimated y The NLL model can return not only the estimated mean but also the estimated variance, which can tells the noise in the training data. In [18]: xlim = ( - 2.5 , 2.5 ) fig = plt . figure ( figsize = ( 15 , 10 )) ax = fig . add_subplot ( 4 , 1 , 1 ) ax . plot ( x_train , y_train , \"p\" , alpha = 0.1 , c = \"red\" , label = \"data\" ) ax . plot ( x_test , y_mean_pred_mse , label = \"estimated_y (loss=MSE)\" ) ax . plot ( x_test , y_test , label = \"true\" ) ax . set_xlabel ( \"x\" ) ax . set_xlim ( * xlim ) ax . legend () ax = fig . add_subplot ( 4 , 1 , 2 ) ax . plot ( x_train , y_train , \"p\" , alpha = 0.1 , c = \"red\" , label = \"data\" ) ax . plot ( x_test , y_mean_pred , label = \"estimated_y (loss=NLL)\" ) ax . plot ( x_test , y_test , label = \"true\" ) ax . set_xlabel ( \"x\" ) ax . set_xlim ( * xlim ) ax . legend () ax = fig . add_subplot ( 4 , 1 , 3 ) ax . plot ( x_test , y_sigma_pred , label = \"estimated_sigma2\" ) ax . set_xlabel ( \"x\" ) ax . set_xlim ( * xlim ) ax . legend () ax = fig . add_subplot ( 4 , 1 , 4 ) ax . plot ( x_train , y_train , \"p\" , alpha = 0.1 , label = \"data\" , c = \"red\" ) ax . errorbar ( x_test , y_mean_pred , yerr = 1.96 * np . sqrt ( y_sigma_pred ), capsize = 0 , label = \"estimated_y +- 1.96*sigma\" ) ax . set_xlabel ( \"x\" ) ax . set_xlim ( * xlim ) ax . legend () plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"TensorFlow newbie creates a neural net with a negative log likelihood as a loss"},{"url":"Achieving-top-5-in-Kaggles-facial-keypoints-detection-using-FCN.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } My Kaggle's final score using the model explained in this ipython notebook. In [1]: from IPython.display import IFrame src = \"https://www.youtube.com/embed/8FdSHl4oNIM\" IFrame ( src , width = 990 / 2 , height = 800 / 2 ) Out[1]: A few days ago, I saw this very awesome youtube video demonstrating the performance of the state-of-art facial keypoint detection. This video was a part of ICCV 2017's presentation How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks) . This paper says that the current state-of-art landmark localization problem uses Hour-Glass network of Stacked Hourglass Networks for Human Pose Estimation . Hour-Glass network uses the idea from the fully convolutional networks, which is often used in object segmentation task. I also studied FCN in previous blog using object segmentation data . Here, I was originally confused about how to apply the model from the object segmentation task to the facial landmark detection task. The data structure of the two problems are quite different: facial landmark detection: input: image output: x,y coordinate of landmarks object segmentation: input: image output: the object's class at every pixcel For example in my previous blog, Achieving Top 23% in Kaggle's Facial Keypoints Detection with Keras + Tensorflow , I did a facial keypoints (landmarks) detection using Kaggle's facial keypoints detection data . In this post, I used CNN to extract features and then regress on the x,y coordinates of the landmarks. So how can we apply the model from the object segmentation to the facial landmark detection problem? The answer was in the preprocessing of images: the (x,y)-coordinates of the landmarks are transformed to \"heatmap\" using some kernels e.g. Gaussian kernel. Then the problem becomes estimating the value of the heatmap at every pixcel just like object detection problem where the goal is to estimate the object's class at every pixcel. Interesting! In this blog post, I will revisit Kaggle's facial keypoints detection data to learn the performance of simple FCN-like model for the facial keypoint detection problem. I was able to improve the private and public scores of this competition from my previous model . The script here can yield: Private score : 1.45920 Public score : 1.87379 This means that I am \"roughly\" in Top 5 in the private score and Top 6 in the public score out of 175 teams. I use the word \"roughly\" because the competition has ended in January 2017, and final scores are not available for me. (The final offical scores are evaluated using 50% of the testing data while the results here are based on ALL the testing data. Nevertheless, the model performance is pretty good. Import libraries In [2]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras , sys , time , os , warnings , cv2 from keras.models import * from keras.layers import * import numpy as np import pandas as pd warnings . filterwarnings ( \"ignore\" ) os . environ [ \"CUDA_DEVICE_ORDER\" ] = \"PCI_BUS_ID\" config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"1\" set_session ( tf . Session ( config = config )) print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )); del keras print ( \"tensorflow version {}\" . format ( tf . __version__ )) Using TensorFlow backend. python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.0.6 tensorflow version 1.2.1 The data is downloaded from Kaggle's facial keypoints detection , and saved under data folder. In [3]: FTRAIN = \"data/training.csv\" FTEST = \"data/test.csv\" FIdLookup = 'data/IdLookupTable.csv' Data importing and preprocessing Create a Gaussian heatmap I will use the data loading functions from Achieving Top 23% in Kaggle's Facial Keypoints Detection with Keras + Tensorflow with some modifications that transform each (x,y)-coordinate facial keypoint to a single heatmap using Gaussian kernel. For example, in Kaggle's facial keypoints detection problem, there are 15 facial keypoints to estimate. So this means that I will create 15 heatmaps. In the case of left eye landmark, the Gaussian kernel centered around left eye landmark $( x_{\\textrm{left eye landmark}}, y_{\\textrm{left eye landmark}})$ has a value at ($x,y$) as: $$ \\frac{1}{2\\pi \\sigma&#94;2 } exp \\left({-\\frac{(x-x_{\\textrm{left eye landmark}})&#94;2 + (y-y_{\\textrm{left eye landmark}})&#94;2}{2 \\sigma&#94;2}}\\right) = constant * \\left({-\\frac{(x-x_{\\textrm{left eye landmark}})&#94;2 + (y-y_{\\textrm{left eye landmark}})&#94;2}{2 \\sigma&#94;2}}\\right) $$ I need to pre-specify $\\sigma&#94;2$ as a hyper parameter. I found that the choice of $\\sigma&#94;2$ is important to get sensible results. If $\\sigma&#94;2$ is too low, the heatmap becomes too sparse (mostly zero) for a model to train. If $\\sigma&#94;2$ is too high, the trained model focuses too much on estimating the magnitude of non-landmark coordinates. The constant scale is another hyper parameter that also needs to be adjusted via, e.g., cross-validation. The following functions are for transforming (x,y)-coordinate of a landmark to a heatmap. In [4]: def gaussian_k ( x0 , y0 , sigma , width , height ): \"\"\" Make a square gaussian kernel centered at (x0, y0) with sigma as SD. \"\"\" x = np . arange ( 0 , width , 1 , float ) ## (width,) y = np . arange ( 0 , height , 1 , float )[:, np . newaxis ] ## (height,1) return np . exp ( - (( x - x0 ) ** 2 + ( y - y0 ) ** 2 ) / ( 2 * sigma ** 2 )) def generate_hm ( height , width , landmarks , s = 3 ): \"\"\" Generate a full Heap Map for every landmarks in an array Args: height : The height of Heat Map (the height of target output) width : The width of Heat Map (the width of target output) joints : [(x1,y1),(x2,y2)...] containing landmarks maxlenght : Lenght of the Bounding Box \"\"\" Nlandmarks = len ( landmarks ) hm = np . zeros (( height , width , Nlandmarks ), dtype = np . float32 ) for i in range ( Nlandmarks ): if not np . array_equal ( landmarks [ i ], [ - 1 , - 1 ]): hm [:,:, i ] = gaussian_k ( landmarks [ i ][ 0 ], landmarks [ i ][ 1 ], s , height , width ) else : hm [:,:, i ] = np . zeros (( height , width )) return hm def get_y_as_heatmap ( df , height , width , sigma ): columns_lmxy = df . columns [: - 1 ] ## the last column contains Image columns_lm = [] for c in columns_lmxy : c = c [: - 2 ] if c not in columns_lm : columns_lm . extend ([ c ]) y_train = [] for i in range ( df . shape [ 0 ]): landmarks = [] for colnm in columns_lm : x = df [ colnm + \"_x\" ] . iloc [ i ] y = df [ colnm + \"_y\" ] . iloc [ i ] if np . isnan ( x ) or np . isnan ( y ): x , y = - 1 , - 1 landmarks . append ([ x , y ]) y_train . append ( generate_hm ( height , width , landmarks , sigma )) y_train = np . array ( y_train ) return ( y_train , df [ columns_lmxy ], columns_lmxy ) Functions to extract, transfer and load data These functions are very similar to the ones from Achieving Top 23% in Kaggle's Facial Keypoints Detection with Keras + Tensorflow . In [5]: def load ( test = False , width = 96 , height = 96 , sigma = 5 ): \"\"\" load test/train data cols : a list containing landmark label names. If this is specified, only the subset of the landmark labels are extracted. for example, cols could be: [left_eye_center_x, left_eye_center_y] return: X: 2-d numpy array (Nsample, Ncol*Nrow) y: 2-d numpy array (Nsample, Nlandmarks*2) In total there are 15 landmarks. As x and y coordinates are recorded, u.shape = (Nsample,30) y0: panda dataframe containins the landmarks \"\"\" from sklearn.utils import shuffle fname = FTEST if test else FTRAIN df = pd . read_csv ( os . path . expanduser ( fname )) df [ 'Image' ] = df [ 'Image' ] . apply ( lambda im : np . fromstring ( im , sep = ' ' )) myprint = df . count () myprint = myprint . reset_index () print ( myprint ) ## row with at least one NA columns are removed! ## df = df.dropna() df = df . fillna ( - 1 ) X = np . vstack ( df [ 'Image' ] . values ) / 255. # changes valeus between 0 and 1 X = X . astype ( np . float32 ) if not test : # labels only exists for the training data y , y0 , nm_landmark = get_y_as_heatmap ( df , height , width , sigma ) X , y , y0 = shuffle ( X , y , y0 , random_state = 42 ) # shuffle data y = y . astype ( np . float32 ) else : y , y0 , nm_landmark = None , None , None return X , y , y0 , nm_landmark def load2d ( test = False , width = 96 , height = 96 , sigma = 5 ): re = load ( test , width , height , sigma ) X = re [ 0 ] . reshape ( - 1 , width , height , 1 ) y , y0 , nm_landmarks = re [ 1 :] return X , y , y0 , nm_landmarks Import training data and testing data In [6]: sigma = 5 X_train , y_train , y_train0 , nm_landmarks = load2d ( test = False , sigma = sigma ) X_test , y_test , _ , _ = load2d ( test = True , sigma = sigma ) print X_train . shape , y_train . shape , y_train0 . shape print X_test . shape , y_test index 0 0 left_eye_center_x 7039 1 left_eye_center_y 7039 2 right_eye_center_x 7036 3 right_eye_center_y 7036 4 left_eye_inner_corner_x 2271 5 left_eye_inner_corner_y 2271 6 left_eye_outer_corner_x 2267 7 left_eye_outer_corner_y 2267 8 right_eye_inner_corner_x 2268 9 right_eye_inner_corner_y 2268 10 right_eye_outer_corner_x 2268 11 right_eye_outer_corner_y 2268 12 left_eyebrow_inner_end_x 2270 13 left_eyebrow_inner_end_y 2270 14 left_eyebrow_outer_end_x 2225 15 left_eyebrow_outer_end_y 2225 16 right_eyebrow_inner_end_x 2270 17 right_eyebrow_inner_end_y 2270 18 right_eyebrow_outer_end_x 2236 19 right_eyebrow_outer_end_y 2236 20 nose_tip_x 7049 21 nose_tip_y 7049 22 mouth_left_corner_x 2269 23 mouth_left_corner_y 2269 24 mouth_right_corner_x 2270 25 mouth_right_corner_y 2270 26 mouth_center_top_lip_x 2275 27 mouth_center_top_lip_y 2275 28 mouth_center_bottom_lip_x 7016 29 mouth_center_bottom_lip_y 7016 30 Image 7049 index 0 0 ImageId 1783 1 Image 1783 (7049, 96, 96, 1) (7049, 96, 96, 15) (7049, 30) (1783, 96, 96, 1) None Visualize original gray scale image together with heatmap Some of the heatmaps are just black indicating that some landmarks are not recorded (missing), and all the pixcels from this heatmap are zero. In principle, my deep learning model can accept all-zero heatmap and handle the missing landmarks with no extra effort. Such approach may be approriate if the landmark does not exist in the image: For example, if the left eye is outside of the image, the heatmap for the left eye should be all zero (while the heatmaps of other landmarks from the same image may or may not be all zeros). However, in this Kaggle's data, I see that all landmarks exist within the image, but for some reasons, some landmark's (x,y) coordinates are not recorded. See plotted images below. So, using all-zero heatmap in training gives misleading infomation to the model; there is no landmark in the image while there is! We should treat these \"missing\" (x,y)-coordinate landmarks as mis-labeled or contaminated data, and simply do not use such landmarks in training. This can be achieved using weights, and presented later. In [7]: Nplot = y_train . shape [ 3 ] + 1 for i in range ( 3 ): fig = plt . figure ( figsize = ( 20 , 6 )) ax = fig . add_subplot ( 2 , Nplot / 2 , 1 ) ax . imshow ( X_train [ i ,:,:, 0 ], cmap = \"gray\" ) ax . set_title ( \"input\" ) for j , lab in enumerate ( nm_landmarks [:: 2 ]): ax = fig . add_subplot ( 2 , Nplot / 2 , j + 2 ) ax . imshow ( y_train [ i ,:,:, j ], cmap = \"gray\" ) ax . set_title ( str ( j ) + \" \\n \" + lab [: - 2 ] ) plt . show () Data augmentation My previous blog showed that it is very important to augment the available training images to improve the model performance on future, validation data. The code below can do wide range of affine transformation and horizontal flipping. Note that the horizontal flipping in the landmark detection problem needs to make sure that the left-XX label and rand-XX labels are also flipped. In this Kaggle competition, we have 15 landmarks. For example, when horizontal flip happens the left_eye_center (0th landmark) needs to be swapped to the right_eye_center. To keep track on which pairs of landmarks to be swapped, we introduce a dictionary recording the original and new landmark's index: landmark_order = { \"orig\" : [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 11 , 12 ], \"new\" : [ 1 , 0 , 4 , 5 , 2 , 3 , 8 , 9 , 6 , 7 , 12 , 11 ]} In my model, I uses weights and horizontal swap will affects the order of weights. This point will become more clear later in the code. Note: the data augmentation function is not optimized and it sustantially increase the training time :\\ Functions In [8]: from skimage import transform from skimage.transform import SimilarityTransform , AffineTransform import random def transform_img ( data , loc_w_batch = 2 , max_rotation = 0.01 , max_shift = 2 , max_shear = 0 , max_scale = 0.01 , mode = \"edge\" ): ''' data : list of numpy arrays containing a single image e.g., data = [X, y, w] or data = [X, y] X.shape = (height, width, NfeatX) y.shape = (height, width, Nfeaty) w.shape = (height, width, Nfeatw) NfeatX, Nfeaty and Nfeatw can be different affine transformation for a single image loc_w_batch : the location of the weights in the fourth dimention [,,,loc_w_batch] ''' scale = ( np . random . uniform ( 1 - max_scale , 1 + max_scale ), np . random . uniform ( 1 - max_scale , 1 + max_scale )) rotation_tmp = np . random . uniform ( - 1 * max_rotation , max_rotation ) translation = ( np . random . uniform ( - 1 * max_shift , max_shift ), np . random . uniform ( - 1 * max_shift , max_shift )) shear = np . random . uniform ( - 1 * max_shear , max_shear ) tform = AffineTransform ( scale = scale , #, ## Convert angles from degrees to radians. rotation = np . deg2rad ( rotation_tmp ), translation = translation , shear = np . deg2rad ( shear ) ) for idata , d in enumerate ( data ): if idata != loc_w_batch : ## We do NOT need to do affine transformation for weights ## as weights are fixed for each (image,landmark) combination data [ idata ] = transform . warp ( d , tform , mode = mode ) return data def transform_imgs ( data , lm , loc_y_batch = 1 , loc_w_batch = 2 ): ''' data : list of numpy arrays containing a single image e.g., data = [X, y, w] or data = [X, y] X.shape = (height, width, NfeatX) y.shape = (height, width, Nfeaty) w.shape = (height, width, Nfeatw) NfeatX, Nfeaty and Nfeatw can be different affine transformation for a single image ''' Nrow = data [ 0 ] . shape [ 0 ] Ndata = len ( data ) data_transform = [[] for i in range ( Ndata )] for irow in range ( Nrow ): data_row = [] for idata in range ( Ndata ): data_row . append ( data [ idata ][ irow ]) ## affine transformation data_row_transform = transform_img ( data_row , loc_w_batch ) ## horizontal flip data_row_transform = horizontal_flip ( data_row_transform , lm , loc_y_batch , loc_w_batch ) for idata in range ( Ndata ): data_transform [ idata ] . append ( data_row_transform [ idata ]) for idata in range ( Ndata ): data_transform [ idata ] = np . array ( data_transform [ idata ]) return ( data_transform ) def horizontal_flip ( data , lm , loc_y_batch = 1 , loc_w_batch = 2 ): ''' flip the image with 50% chance lm is a dictionary containing \"orig\" and \"new\" key This must indicate the potitions of heatmaps that need to be flipped landmark_order = {\"orig\" : [0,1,2,3,4,5,6,7,8,9,11,12], \"new\" : [1,0,4,5,2,3,8,9,6,7,12,11]} data = [X, y, w] w is optional and if it is in the code, the position needs to be specified with loc_w_batch X.shape (height,width,n_channel) y.shape (height,width,n_landmarks) w.shape (height,width,n_landmarks) ''' lo , ln = np . array ( lm [ \"orig\" ]), np . array ( lm [ \"new\" ]) assert len ( lo ) == len ( ln ) if np . random . choice ([ 0 , 1 ]) == 1 : return ( data ) for i , d in enumerate ( data ): d = d [:, :: - 1 ,:] data [ i ] = d data [ loc_y_batch ] = swap_index_for_horizontal_flip ( data [ loc_y_batch ], lo , ln ) # when horizontal flip happens to image, we need to heatmap (y) and weights y and w # do this if loc_w_batch is within data length if loc_w_batch < len ( data ): data [ loc_w_batch ] = swap_index_for_horizontal_flip ( data [ loc_w_batch ], lo , ln ) return ( data ) def swap_index_for_horizontal_flip ( y_batch , lo , ln ): ''' lm = {\"orig\" : [0,1,2,3,4,5,6,7,8,9,11,12], \"new\" : [1,0,4,5,2,3,8,9,6,7,12,11]} lo, ln = np.array(lm[\"orig\"]), np.array(lm[\"new\"]) ''' y_orig = y_batch [:,:, lo ] y_batch [:,:, lo ] = y_batch [:,:, ln ] y_batch [:,:, ln ] = y_orig return ( y_batch ) Visualizing augmented images Notice that the image is shifted and also horizontally flipped in random fashion. When horizontal flip happens, the right eye is labeled as left eye and vise versa. In [9]: ## example image to visualize the data augmentation iexample = 139 ## Show the first 13 heatmaps Nhm = 10 plt . imshow ( X_train [ iexample ,:,:, 0 ], cmap = \"gray\" ) plt . title ( \"original\" ) plt . axis ( \"off\" ) plt . show () Nplot = 5 fig = plt . figure ( figsize = [ Nhm * 2.5 , 2 * Nplot ]) landmark_order = { \"orig\" : [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 11 , 12 ], \"new\" : [ 1 , 0 , 4 , 5 , 2 , 3 , 8 , 9 , 6 , 7 , 12 , 11 ]} count = 1 for _ in range ( Nplot ): x_batch , y_batch = transform_imgs ([ X_train [[ iexample ]], y_train [[ iexample ]]], landmark_order ) ax = fig . add_subplot ( Nplot , Nhm + 1 , count ) ax . imshow ( x_batch [ 0 ,:,:, 0 ], cmap = \"gray\" ) ax . axis ( \"off\" ) count += 1 for ifeat in range ( Nhm ): ax = fig . add_subplot ( Nplot , Nhm + 1 , count ) ax . imshow ( y_batch [ 0 ,:,:, ifeat ], cmap = \"gray\" ) ax . axis ( \"off\" ) if count < Nhm + 2 : ax . set_title ( nm_landmarks [ ifeat * 2 ][: - 2 ]) count += 1 plt . show () Split data into training and validation data In [10]: prop_train = 0.9 Ntrain = int ( X_train . shape [ 0 ] * prop_train ) X_tra , y_tra , X_val , y_val = X_train [: Ntrain ], y_train [: Ntrain ], X_train [ Ntrain :], y_train [ Ntrain :] del X_train , y_train Define FCN-like model This model is simplified version of FCN8. See my previous blog post about FCN8 . In [11]: input_height , input_width = 96 , 96 ## output shape is the same as input output_height , output_width = input_height , input_width n = 32 * 5 nClasses = 15 nfmp_block1 = 64 nfmp_block2 = 128 IMAGE_ORDERING = \"channels_last\" img_input = Input ( shape = ( input_height , input_width , 1 )) # Encoder Block 1 x = Conv2D ( nfmp_block1 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block1_conv1' , data_format = IMAGE_ORDERING )( img_input ) x = Conv2D ( nfmp_block1 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block1_conv2' , data_format = IMAGE_ORDERING )( x ) block1 = MaxPooling2D (( 2 , 2 ), strides = ( 2 , 2 ), name = 'block1_pool' , data_format = IMAGE_ORDERING )( x ) # Encoder Block 2 x = Conv2D ( nfmp_block2 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block2_conv1' , data_format = IMAGE_ORDERING )( block1 ) x = Conv2D ( nfmp_block2 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block2_conv2' , data_format = IMAGE_ORDERING )( x ) x = MaxPooling2D (( 2 , 2 ), strides = ( 2 , 2 ), name = 'block2_pool' , data_format = IMAGE_ORDERING )( x ) ## bottoleneck o = ( Conv2D ( n , ( input_height / 4 , input_width / 4 ), activation = 'relu' , padding = 'same' , name = \"bottleneck_1\" , data_format = IMAGE_ORDERING ))( x ) o = ( Conv2D ( n , ( 1 , 1 ) , activation = 'relu' , padding = 'same' , name = \"bottleneck_2\" , data_format = IMAGE_ORDERING ))( o ) ## upsamping to bring the feature map size to be the same as the one from block1 ## o_block1 = Conv2DTranspose(nfmp_block1, kernel_size=(2,2), strides=(2,2), use_bias=False, name='upsample_1', data_format=IMAGE_ORDERING )(o) ## o = Add()([o_block1,block1]) ## output = Conv2DTranspose(nClasses, kernel_size=(2,2), strides=(2,2), use_bias=False, name='upsample_2', data_format=IMAGE_ORDERING )(o) ## Decoder Block ## upsampling to bring the feature map size to be the same as the input image i.e., heatmap size output = Conv2DTranspose ( nClasses , kernel_size = ( 4 , 4 ), strides = ( 4 , 4 ), use_bias = False , name = 'upsample_2' , data_format = IMAGE_ORDERING )( o ) ## Reshaping is necessary to use sample_weight_mode=\"temporal\" which assumes 3 dimensional output shape ## See below for the discussion of weights output = Reshape (( output_width * input_height * nClasses , 1 ))( output ) model = Model ( img_input , output ) model . summary () model . compile ( loss = 'mse' , optimizer = \"rmsprop\" , sample_weight_mode = \"temporal\" ) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 96, 96, 1) 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 96, 96, 64) 640 _________________________________________________________________ block1_conv2 (Conv2D) (None, 96, 96, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 48, 48, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 48, 48, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 48, 48, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 24, 24, 128) 0 _________________________________________________________________ bottleneck_1 (Conv2D) (None, 24, 24, 160) 11796640 _________________________________________________________________ bottleneck_2 (Conv2D) (None, 24, 24, 160) 25760 _________________________________________________________________ upsample_2 (Conv2DTranspose) (None, 96, 96, 15) 38400 _________________________________________________________________ reshape_1 (Reshape) (None, 138240, 1) 0 ================================================================= Total params: 12,119,808 Trainable params: 12,119,808 Non-trainable params: 0 _________________________________________________________________ Sample weights We will only include the annotated landmarks to calculate the loss. To do this, I will define the weight matrix (N_image, height, width, N_landmark). In this weight matrix, if the (x,y)-coordinate of the landmark of the image is recorded, all the height x width pixcels have value 1, and if the landmark is not recorded, all the pixcels have value 0. Keras acepts the weight matrix to be at most 2 dimensional at this moment (May 2018) with the first dimension corresponding to the image sample. We reshape the weight matrix into the size (N_image, height x width x N_landmark). In [12]: def find_weight ( y_tra ): ''' :::input::: y_tra : np.array of shape (N_image, height, width, N_landmark) :::output::: weights : np.array of shape (N_image, height, width, N_landmark) weights[i_image, :, :, i_landmark] = 1 if the (x,y) coordinate of the landmark for this image is recorded. else weights[i_image, :, :, i_landmark] = 0 ''' weight = np . zeros_like ( y_tra ) count0 , count1 = 0 , 0 for irow in range ( y_tra . shape [ 0 ]): for ifeat in range ( y_tra . shape [ - 1 ]): if np . all ( y_tra [ irow ,:,:, ifeat ] == 0 ): value = 0 count0 += 1 else : value = 1 count1 += 1 weight [ irow ,:,:, ifeat ] = value print ( \"N landmarks={:5.0f}, N missing landmarks={:5.0f}, weight.shape={}\" . format ( count0 , count1 , weight . shape )) return ( weight ) def flatten_except_1dim ( weight , ndim = 2 ): ''' change the dimension from: (a,b,c,d,..) to (a, b*c*d*..) if ndim = 2 (a,b,c,d,..) to (a, b*c*d*..,1) if ndim = 3 ''' n = weight . shape [ 0 ] if ndim == 2 : shape = ( n , - 1 ) elif ndim == 3 : shape = ( n , - 1 , 1 ) else : print ( \"Not implemented!\" ) weight = weight . reshape ( * shape ) return ( weight ) Define weights for training, and validation data. In [13]: w_tra = find_weight ( y_tra ) weight_val = find_weight ( y_val ) weight_val = flatten_except_1dim ( weight_val ) y_val_fla = flatten_except_1dim ( y_val , ndim = 3 ) ## print(\"weight_tra.shape={}\".format(weight_tra.shape)) print ( \"weight_val.shape={}\" . format ( weight_val . shape )) print ( \"y_val_fla.shape={}\" . format ( y_val_fla . shape )) print ( model . output . shape ) N landmarks=47185, N missing landmarks=47975, weight.shape=(6344, 96, 96, 15) N landmarks= 5521, N missing landmarks= 5054, weight.shape=(705, 96, 96, 15) weight_val.shape=(705, 138240) y_val_fla.shape=(705, 138240, 1) (?, 138240, 1) Training starts here! In [14]: nb_epochs = 300 batch_size = 32 const = 10 history = { \"loss\" :[], \"val_loss\" :[]} for iepoch in range ( nb_epochs ): start = time . time () x_batch , y_batch , w_batch = transform_imgs ([ X_tra , y_tra , w_tra ], landmark_order ) # If you want no data augementation, comment out the line above and uncomment the comment below: # x_batch, y_batch, w_batch = X_tra,y_tra, w_batch w_batch_fla = flatten_except_1dim ( w_batch , ndim = 2 ) y_batch_fla = flatten_except_1dim ( y_batch , ndim = 3 ) hist = model . fit ( x_batch , y_batch_fla * const , sample_weight = w_batch_fla , validation_data = ( X_val , y_val_fla * const , weight_val ), batch_size = batch_size , epochs = 1 , verbose = 0 ) history [ \"loss\" ] . append ( hist . history [ \"loss\" ][ 0 ]) history [ \"val_loss\" ] . append ( hist . history [ \"val_loss\" ][ 0 ]) end = time . time () print ( \"Epoch {:03}: loss {:6.4f} val_loss {:6.4f} {:4.1f}sec\" . format ( iepoch + 1 , history [ \"loss\" ][ - 1 ], history [ \"val_loss\" ][ - 1 ], end - start )) Epoch 001: loss 0.4608 val_loss 0.2802 89.4sec Epoch 002: loss 0.2289 val_loss 0.2765 80.2sec Epoch 003: loss 0.1768 val_loss 0.1417 81.1sec Epoch 004: loss 0.1492 val_loss 0.2057 83.9sec Epoch 005: loss 0.1332 val_loss 0.1253 79.8sec Epoch 006: loss 0.1221 val_loss 0.1518 81.5sec Epoch 007: loss 0.1125 val_loss 0.1188 80.1sec Epoch 008: loss 0.1056 val_loss 0.1164 81.4sec Epoch 009: loss 0.0998 val_loss 0.0939 81.5sec Epoch 010: loss 0.0940 val_loss 0.0916 82.0sec Epoch 011: loss 0.0903 val_loss 0.0937 79.0sec Epoch 012: loss 0.0858 val_loss 0.1134 80.8sec Epoch 013: loss 0.0825 val_loss 0.1103 80.1sec Epoch 014: loss 0.0791 val_loss 0.0719 79.8sec Epoch 015: loss 0.0756 val_loss 0.0909 80.1sec Epoch 016: loss 0.0734 val_loss 0.0917 80.2sec Epoch 017: loss 0.0709 val_loss 0.0763 79.9sec Epoch 018: loss 0.0686 val_loss 0.0823 79.2sec Epoch 019: loss 0.0664 val_loss 0.0753 80.6sec Epoch 020: loss 0.0642 val_loss 0.0891 81.2sec Epoch 021: loss 0.0626 val_loss 0.0768 80.1sec Epoch 022: loss 0.0609 val_loss 0.0812 80.3sec Epoch 023: loss 0.0592 val_loss 0.0765 80.6sec Epoch 024: loss 0.0576 val_loss 0.0755 82.7sec Epoch 025: loss 0.0562 val_loss 0.0698 79.8sec Epoch 026: loss 0.0545 val_loss 0.0773 82.5sec Epoch 027: loss 0.0532 val_loss 0.0711 85.0sec Epoch 028: loss 0.0522 val_loss 0.0747 82.3sec Epoch 029: loss 0.0510 val_loss 0.0751 81.4sec Epoch 030: loss 0.0496 val_loss 0.0743 81.3sec Epoch 031: loss 0.0485 val_loss 0.0926 81.4sec Epoch 032: loss 0.0475 val_loss 0.0765 80.4sec Epoch 033: loss 0.0465 val_loss 0.0747 80.9sec Epoch 034: loss 0.0458 val_loss 0.0723 81.7sec Epoch 035: loss 0.0448 val_loss 0.0720 81.7sec Epoch 036: loss 0.0441 val_loss 0.0715 82.3sec Epoch 037: loss 0.0432 val_loss 0.0759 81.7sec Epoch 038: loss 0.0424 val_loss 0.0796 81.7sec Epoch 039: loss 0.0417 val_loss 0.0741 81.8sec Epoch 040: loss 0.0406 val_loss 0.0709 81.0sec Epoch 041: loss 0.0400 val_loss 0.0727 81.8sec Epoch 042: loss 0.0394 val_loss 0.0722 82.0sec Epoch 043: loss 0.0391 val_loss 0.0759 82.5sec Epoch 044: loss 0.0384 val_loss 0.0787 82.0sec Epoch 045: loss 0.0378 val_loss 0.0726 81.7sec Epoch 046: loss 0.0370 val_loss 0.0775 86.2sec Epoch 047: loss 0.0370 val_loss 0.0753 82.1sec Epoch 048: loss 0.0364 val_loss 0.0780 82.0sec Epoch 049: loss 0.0356 val_loss 0.0732 81.6sec Epoch 050: loss 0.0351 val_loss 0.0707 81.7sec Epoch 051: loss 0.0347 val_loss 0.0749 81.5sec Epoch 052: loss 0.0343 val_loss 0.0781 82.6sec Epoch 053: loss 0.0336 val_loss 0.0764 81.3sec Epoch 054: loss 0.0334 val_loss 0.0897 82.1sec Epoch 055: loss 0.0333 val_loss 0.0705 81.7sec Epoch 056: loss 0.0328 val_loss 0.0752 83.1sec Epoch 057: loss 0.0325 val_loss 0.0741 81.3sec Epoch 058: loss 0.0320 val_loss 0.0715 81.8sec Epoch 059: loss 0.0318 val_loss 0.0709 81.7sec Epoch 060: loss 0.0313 val_loss 0.0678 81.8sec Epoch 061: loss 0.0311 val_loss 0.0710 81.8sec Epoch 062: loss 0.0308 val_loss 0.0707 82.8sec Epoch 063: loss 0.0306 val_loss 0.0723 83.9sec Epoch 064: loss 0.0301 val_loss 0.0771 81.0sec Epoch 065: loss 0.0299 val_loss 0.0726 81.7sec Epoch 066: loss 0.0296 val_loss 0.0691 82.6sec Epoch 067: loss 0.0296 val_loss 0.0743 82.2sec Epoch 068: loss 0.0290 val_loss 0.0762 82.4sec Epoch 069: loss 0.0287 val_loss 0.0749 81.8sec Epoch 070: loss 0.0288 val_loss 0.0741 81.1sec Epoch 071: loss 0.0284 val_loss 0.0707 82.8sec Epoch 072: loss 0.0284 val_loss 0.0711 81.5sec Epoch 073: loss 0.0280 val_loss 0.0765 83.4sec Epoch 074: loss 0.0276 val_loss 0.0736 82.0sec Epoch 075: loss 0.0274 val_loss 0.0705 83.8sec Epoch 076: loss 0.0276 val_loss 0.0692 83.0sec Epoch 077: loss 0.0273 val_loss 0.0682 85.6sec Epoch 078: loss 0.0270 val_loss 0.0747 86.8sec Epoch 079: loss 0.0267 val_loss 0.0687 87.8sec Epoch 080: loss 0.0266 val_loss 0.0699 84.7sec Epoch 081: loss 0.0265 val_loss 0.0687 86.5sec Epoch 082: loss 0.0261 val_loss 0.0754 86.9sec Epoch 083: loss 0.0262 val_loss 0.0674 85.4sec Epoch 084: loss 0.0259 val_loss 0.0728 84.8sec Epoch 085: loss 0.0258 val_loss 0.0686 84.7sec Epoch 086: loss 0.0257 val_loss 0.0697 83.6sec Epoch 087: loss 0.0253 val_loss 0.0681 85.3sec Epoch 088: loss 0.0255 val_loss 0.0697 84.4sec Epoch 089: loss 0.0253 val_loss 0.0728 82.5sec Epoch 090: loss 0.0252 val_loss 0.0743 82.7sec Epoch 091: loss 0.0248 val_loss 0.0781 82.4sec Epoch 092: loss 0.0249 val_loss 0.0668 81.5sec Epoch 093: loss 0.0247 val_loss 0.0706 82.4sec Epoch 094: loss 0.0247 val_loss 0.0706 82.3sec Epoch 095: loss 0.0246 val_loss 0.0687 82.4sec Epoch 096: loss 0.0244 val_loss 0.0702 84.1sec Epoch 097: loss 0.0242 val_loss 0.0661 84.7sec Epoch 098: loss 0.0240 val_loss 0.0759 84.1sec Epoch 099: loss 0.0240 val_loss 0.0693 84.7sec Epoch 100: loss 0.0238 val_loss 0.0734 84.5sec Epoch 101: loss 0.0237 val_loss 0.0706 83.8sec Epoch 102: loss 0.0237 val_loss 0.0796 83.6sec Epoch 103: loss 0.0235 val_loss 0.0712 85.4sec Epoch 104: loss 0.0234 val_loss 0.0700 82.8sec Epoch 105: loss 0.0234 val_loss 0.0703 83.5sec Epoch 106: loss 0.0233 val_loss 0.0712 85.7sec Epoch 107: loss 0.0232 val_loss 0.0769 86.7sec Epoch 108: loss 0.0231 val_loss 0.0694 85.4sec Epoch 109: loss 0.0231 val_loss 0.0709 87.7sec Epoch 110: loss 0.0228 val_loss 0.0726 85.4sec Epoch 111: loss 0.0227 val_loss 0.0720 85.7sec Epoch 112: loss 0.0227 val_loss 0.0724 85.6sec Epoch 113: loss 0.0228 val_loss 0.0708 84.7sec Epoch 114: loss 0.0225 val_loss 0.0731 88.5sec Epoch 115: loss 0.0224 val_loss 0.0735 86.0sec Epoch 116: loss 0.0223 val_loss 0.0678 90.7sec Epoch 117: loss 0.0224 val_loss 0.0721 100.5sec Epoch 118: loss 0.0222 val_loss 0.0729 83.7sec Epoch 119: loss 0.0221 val_loss 0.0738 83.5sec Epoch 120: loss 0.0220 val_loss 0.0745 82.0sec Epoch 121: loss 0.0221 val_loss 0.0696 86.6sec Epoch 122: loss 0.0217 val_loss 0.0713 86.5sec Epoch 123: loss 0.0219 val_loss 0.0739 91.5sec Epoch 124: loss 0.0218 val_loss 0.0691 83.4sec Epoch 125: loss 0.0218 val_loss 0.0697 83.6sec Epoch 126: loss 0.0214 val_loss 0.0729 81.3sec Epoch 127: loss 0.0215 val_loss 0.0688 83.6sec Epoch 128: loss 0.0215 val_loss 0.0705 82.7sec Epoch 129: loss 0.0214 val_loss 0.0764 84.0sec Epoch 130: loss 0.0214 val_loss 0.0788 82.9sec Epoch 131: loss 0.0213 val_loss 0.0717 82.4sec Epoch 132: loss 0.0211 val_loss 0.0745 82.8sec Epoch 133: loss 0.0211 val_loss 0.0733 83.7sec Epoch 134: loss 0.0210 val_loss 0.0762 83.6sec Epoch 135: loss 0.0211 val_loss 0.0776 82.3sec Epoch 136: loss 0.0209 val_loss 0.0712 83.9sec Epoch 137: loss 0.0209 val_loss 0.0669 84.3sec Epoch 138: loss 0.0209 val_loss 0.0735 83.7sec Epoch 139: loss 0.0208 val_loss 0.0899 83.3sec Epoch 140: loss 0.0207 val_loss 0.0775 85.2sec Epoch 141: loss 0.0207 val_loss 0.0739 85.2sec Epoch 142: loss 0.0207 val_loss 0.0704 85.2sec Epoch 143: loss 0.0206 val_loss 0.0755 85.0sec Epoch 144: loss 0.0206 val_loss 0.0725 97.5sec Epoch 145: loss 0.0203 val_loss 0.0721 86.3sec Epoch 146: loss 0.0203 val_loss 0.0704 85.8sec Epoch 147: loss 0.0201 val_loss 0.0750 84.8sec Epoch 148: loss 0.0201 val_loss 0.0736 82.8sec Epoch 149: loss 0.0202 val_loss 0.0691 85.8sec Epoch 150: loss 0.0203 val_loss 0.0697 84.9sec Epoch 151: loss 0.0201 val_loss 0.0732 84.0sec Epoch 152: loss 0.0201 val_loss 0.0703 83.7sec Epoch 153: loss 0.0200 val_loss 0.0720 83.1sec Epoch 154: loss 0.0202 val_loss 0.0717 83.4sec Epoch 155: loss 0.0199 val_loss 0.0714 83.6sec Epoch 156: loss 0.0201 val_loss 0.0709 82.9sec Epoch 157: loss 0.0198 val_loss 0.0714 83.4sec Epoch 158: loss 0.0196 val_loss 0.0688 83.4sec Epoch 159: loss 0.0198 val_loss 0.0713 83.9sec Epoch 160: loss 0.0196 val_loss 0.0697 85.6sec Epoch 161: loss 0.0196 val_loss 0.0714 83.0sec Epoch 162: loss 0.0197 val_loss 0.0721 91.1sec Epoch 163: loss 0.0196 val_loss 0.0708 81.3sec Epoch 164: loss 0.0195 val_loss 0.0683 80.6sec Epoch 165: loss 0.0194 val_loss 0.0711 82.9sec Epoch 166: loss 0.0195 val_loss 0.0705 81.8sec Epoch 167: loss 0.0193 val_loss 0.0707 81.9sec Epoch 168: loss 0.0193 val_loss 0.0699 83.3sec Epoch 169: loss 0.0192 val_loss 0.0766 82.4sec Epoch 170: loss 0.0193 val_loss 0.0719 82.5sec Epoch 171: loss 0.0194 val_loss 0.0726 81.0sec Epoch 172: loss 0.0191 val_loss 0.0730 81.5sec Epoch 173: loss 0.0190 val_loss 0.0724 85.1sec Epoch 174: loss 0.0192 val_loss 0.0718 80.3sec Epoch 175: loss 0.0190 val_loss 0.0715 79.3sec Epoch 176: loss 0.0190 val_loss 0.0733 81.4sec Epoch 177: loss 0.0191 val_loss 0.0807 80.6sec Epoch 178: loss 0.0190 val_loss 0.0694 80.7sec Epoch 179: loss 0.0189 val_loss 0.0727 80.8sec Epoch 180: loss 0.0187 val_loss 0.0767 81.3sec Epoch 181: loss 0.0190 val_loss 0.0686 81.5sec Epoch 182: loss 0.0188 val_loss 0.0726 80.7sec Epoch 183: loss 0.0187 val_loss 0.0759 82.5sec Epoch 184: loss 0.0186 val_loss 0.0707 82.0sec Epoch 185: loss 0.0187 val_loss 0.0757 82.4sec Epoch 186: loss 0.0188 val_loss 0.0701 80.9sec Epoch 187: loss 0.0187 val_loss 0.0689 82.1sec Epoch 188: loss 0.0187 val_loss 0.0690 80.9sec Epoch 189: loss 0.0186 val_loss 0.0714 81.2sec Epoch 190: loss 0.0186 val_loss 0.0723 83.0sec Epoch 191: loss 0.0187 val_loss 0.0715 84.4sec Epoch 192: loss 0.0185 val_loss 0.0728 82.9sec Epoch 193: loss 0.0184 val_loss 0.0708 84.5sec Epoch 194: loss 0.0185 val_loss 0.0817 82.0sec Epoch 195: loss 0.0183 val_loss 0.0767 82.5sec Epoch 196: loss 0.0182 val_loss 0.0689 83.9sec Epoch 197: loss 0.0183 val_loss 0.0703 83.1sec Epoch 198: loss 0.0182 val_loss 0.0755 83.8sec Epoch 199: loss 0.0183 val_loss 0.0708 83.5sec Epoch 200: loss 0.0182 val_loss 0.0693 82.9sec Epoch 201: loss 0.0180 val_loss 0.0733 81.8sec Epoch 202: loss 0.0180 val_loss 0.0670 83.7sec Epoch 203: loss 0.0180 val_loss 0.0772 83.6sec Epoch 204: loss 0.0181 val_loss 0.0699 83.6sec Epoch 205: loss 0.0181 val_loss 0.0699 82.5sec Epoch 206: loss 0.0180 val_loss 0.0680 84.7sec Epoch 207: loss 0.0181 val_loss 0.0705 84.6sec Epoch 208: loss 0.0180 val_loss 0.0722 82.0sec Epoch 209: loss 0.0178 val_loss 0.0734 81.8sec Epoch 210: loss 0.0179 val_loss 0.0701 81.4sec Epoch 211: loss 0.0177 val_loss 0.0720 80.1sec Epoch 212: loss 0.0180 val_loss 0.0708 80.0sec Epoch 213: loss 0.0179 val_loss 0.0692 80.3sec Epoch 214: loss 0.0177 val_loss 0.0791 79.3sec Epoch 215: loss 0.0179 val_loss 0.0699 82.6sec Epoch 216: loss 0.0178 val_loss 0.0710 86.8sec Epoch 217: loss 0.0177 val_loss 0.0737 84.5sec Epoch 218: loss 0.0176 val_loss 0.0693 83.8sec Epoch 219: loss 0.0176 val_loss 0.0748 86.1sec Epoch 220: loss 0.0177 val_loss 0.0728 85.4sec Epoch 221: loss 0.0175 val_loss 0.0701 86.1sec Epoch 222: loss 0.0175 val_loss 0.0709 86.4sec Epoch 223: loss 0.0175 val_loss 0.0735 83.6sec Epoch 224: loss 0.0176 val_loss 0.0770 86.0sec Epoch 225: loss 0.0175 val_loss 0.0702 86.8sec Epoch 226: loss 0.0173 val_loss 0.0708 85.0sec Epoch 227: loss 0.0175 val_loss 0.0736 86.8sec Epoch 228: loss 0.0172 val_loss 0.0702 86.3sec Epoch 229: loss 0.0173 val_loss 0.0698 87.2sec Epoch 230: loss 0.0174 val_loss 0.0702 87.8sec Epoch 231: loss 0.0174 val_loss 0.0712 85.7sec Epoch 232: loss 0.0173 val_loss 0.0731 86.0sec Epoch 233: loss 0.0172 val_loss 0.0700 82.5sec Epoch 234: loss 0.0174 val_loss 0.0708 80.7sec Epoch 235: loss 0.0173 val_loss 0.0707 80.9sec Epoch 236: loss 0.0174 val_loss 0.0683 80.2sec Epoch 237: loss 0.0172 val_loss 0.0697 80.5sec Epoch 238: loss 0.0171 val_loss 0.0705 80.7sec Epoch 239: loss 0.0173 val_loss 0.0705 80.3sec Epoch 240: loss 0.0174 val_loss 0.0707 80.2sec Epoch 241: loss 0.0171 val_loss 0.0704 80.7sec Epoch 242: loss 0.0171 val_loss 0.0720 82.0sec Epoch 243: loss 0.0172 val_loss 0.0715 80.2sec Epoch 244: loss 0.0170 val_loss 0.0705 80.4sec Epoch 245: loss 0.0171 val_loss 0.0730 80.4sec Epoch 246: loss 0.0169 val_loss 0.0722 80.6sec Epoch 247: loss 0.0171 val_loss 0.0689 80.0sec Epoch 248: loss 0.0169 val_loss 0.0702 81.3sec Epoch 249: loss 0.0170 val_loss 0.0704 80.9sec Epoch 250: loss 0.0169 val_loss 0.0805 79.8sec Epoch 251: loss 0.0170 val_loss 0.0704 80.0sec Epoch 252: loss 0.0169 val_loss 0.0693 81.0sec Epoch 253: loss 0.0169 val_loss 0.0703 80.8sec Epoch 254: loss 0.0168 val_loss 0.0705 79.7sec Epoch 255: loss 0.0168 val_loss 0.0694 80.4sec Epoch 256: loss 0.0167 val_loss 0.0715 79.2sec Epoch 257: loss 0.0167 val_loss 0.0724 80.4sec Epoch 258: loss 0.0169 val_loss 0.0713 79.6sec Epoch 259: loss 0.0167 val_loss 0.0702 80.7sec Epoch 260: loss 0.0167 val_loss 0.0714 98.1sec Epoch 261: loss 0.0166 val_loss 0.0694 79.1sec Epoch 262: loss 0.0166 val_loss 0.0704 79.2sec Epoch 263: loss 0.0165 val_loss 0.0674 78.9sec Epoch 264: loss 0.0166 val_loss 0.0696 79.6sec Epoch 265: loss 0.0168 val_loss 0.0706 78.3sec Epoch 266: loss 0.0166 val_loss 0.0702 77.7sec Epoch 267: loss 0.0166 val_loss 0.0707 78.7sec Epoch 268: loss 0.0165 val_loss 0.0679 78.6sec Epoch 269: loss 0.0166 val_loss 0.0720 78.9sec Epoch 270: loss 0.0166 val_loss 0.0710 89.0sec Epoch 271: loss 0.0165 val_loss 0.0707 78.5sec Epoch 272: loss 0.0164 val_loss 0.0703 78.5sec Epoch 273: loss 0.0164 val_loss 0.0738 78.3sec Epoch 274: loss 0.0163 val_loss 0.0703 78.3sec Epoch 275: loss 0.0164 val_loss 0.0720 79.8sec Epoch 276: loss 0.0164 val_loss 0.0714 78.5sec Epoch 277: loss 0.0163 val_loss 0.0718 79.0sec Epoch 278: loss 0.0163 val_loss 0.0667 78.6sec Epoch 279: loss 0.0163 val_loss 0.0713 79.1sec Epoch 280: loss 0.0164 val_loss 0.0702 81.3sec Epoch 281: loss 0.0164 val_loss 0.0680 77.7sec Epoch 282: loss 0.0162 val_loss 0.0711 78.2sec Epoch 283: loss 0.0163 val_loss 0.0687 78.5sec Epoch 284: loss 0.0162 val_loss 0.0714 78.9sec Epoch 285: loss 0.0163 val_loss 0.0742 78.3sec Epoch 286: loss 0.0165 val_loss 0.0702 78.6sec Epoch 287: loss 0.0163 val_loss 0.0707 79.1sec Epoch 288: loss 0.0163 val_loss 0.0715 81.1sec Epoch 289: loss 0.0161 val_loss 0.0694 79.2sec Epoch 290: loss 0.0162 val_loss 0.0743 79.0sec Epoch 291: loss 0.0161 val_loss 0.0688 80.0sec Epoch 292: loss 0.0162 val_loss 0.0702 79.4sec Epoch 293: loss 0.0162 val_loss 0.0709 78.9sec Epoch 294: loss 0.0162 val_loss 0.0716 78.7sec Epoch 295: loss 0.0162 val_loss 0.0699 80.2sec Epoch 296: loss 0.0161 val_loss 0.0678 79.2sec Epoch 297: loss 0.0160 val_loss 0.0716 80.1sec Epoch 298: loss 0.0161 val_loss 0.0716 78.0sec Epoch 299: loss 0.0160 val_loss 0.0696 79.6sec Epoch 300: loss 0.0161 val_loss 0.0684 78.9sec Model performance evaluation Plot loss over epochs In [15]: for label in [ \"val_loss\" , \"loss\" ]: plt . plot ( history [ label ], label = label ) plt . legend () plt . show () Model performance on example training images The plot shows that when the facial keypoints are not recorded, the estimated model's heatmap is generally low. In [16]: y_pred = model . predict ( X_tra ) y_pred = y_pred . reshape ( - 1 , output_height , output_width , nClasses ) Nlandmark = y_pred . shape [ - 1 ] for i in range ( 96 , 100 ): fig = plt . figure ( figsize = ( 3 , 3 )) ax = fig . add_subplot ( 1 , 1 , 1 ) ax . imshow ( X_tra [ i ,:,:, 0 ], cmap = \"gray\" ) ax . axis ( \"off\" ) fig = plt . figure ( figsize = ( 20 , 3 )) count = 1 for j , lab in enumerate ( nm_landmarks [:: 2 ]): ax = fig . add_subplot ( 2 , Nlandmark , count ) ax . set_title ( lab [: 10 ] + \" \\n \" + lab [ 10 : - 2 ]) ax . axis ( \"off\" ) count += 1 ax . imshow ( y_pred [ i ,:,:, j ]) if j == 0 : ax . set_ylabel ( \"prediction\" ) for j , lab in enumerate ( nm_landmarks [:: 2 ]): ax = fig . add_subplot ( 2 , Nlandmark , count ) count += 1 ax . imshow ( y_tra [ i ,:,:, j ]) ax . axis ( \"off\" ) if j == 0 : ax . set_ylabel ( \"true\" ) plt . show () Evaluate the performance in (x,y) coordinate Transform heatmap back to the (x,y) coordinate In order to evaluate the model performance in terms of the root mean square error (RMSE) on (x,y) coordiantes, I need to transform heatmap of the landmarks back to the (x,y) coordiantes. The simplest way would be to use the (x,y) coordiantes of the pixcel with the largest estimated density as the estimated coordinate. In this procedure, however, the estimated (x,y) coordinates are always integers while the true (x,y) coordinates are not necessarily integers. Instead, we may use weighted average of the (x,y) coordinates corresponding to the pixcels with the top \"n_points\" largest estimated density. Then question is, how many \"n_points\" should we use to calculate the weighted average of the coordinates. In the following script, I experiment the effects of the change in \"n_points\" on the RMSE using training set For the choice of \"n_points\" I only consider $n&#94;2$ for integer $n$ to allow that selected coordinates to form symetric geometry. RMSE is calculated in three ways: RMSE1: (x,y) coordinates from estimated heatmap VS (x,y) coordinates from true heatmap RMSE2: (x,y) coordinates from est heatmap VS true (x,y) coordinates of the landmark RMSE3: (x,y) coordinates from true heatmap VS true (x,y) coordinates of the landmark Ideally, we want to find n_points that returns the smallest RMSE1, RMSE2 and RMSE3. To reduce the computation time, I will only use the subset of training images for this experiment. Results The largest \"n_points\" = $96$x$96$ does not return the smallest RMSE1, RMSE2 or RMSE3. This makes sense because the iamge size is finite and some of the landmarks are at the corner. Taking weighted average across the entire image may results will bias in the coordinate values toward the center of images. This would be the reason why RMSE3 is never 0. This observation makes me think that it would be better to make the n_points depends on each (image, landmark) combination separately. For example, if the highest density point is at (0,0), then we should not consider large n_points because the estimated density at (-1,-1) would have been large but density in such coordinates are not estimated. On the other hand, if the highest density point is at around the center of the image, then I should consider large n_points. For the simpliciy, I will not implement such procedure, and this would be my future work. I will use n_points = 25 as it yields the smallest RMSE3. In [17]: def get_ave_xy ( hmi , n_points = 4 , thresh = 0 ): ''' hmi : heatmap np array of size (height,width) n_points : x,y coordinates corresponding to the top densities to calculate average (x,y) coordinates convert heatmap to (x,y) coordinate x,y coordinates corresponding to the top densities are used to calculate weighted average of (x,y) coordinates the weights are used using heatmap if the heatmap does not contain the probability > then we assume there is no predicted landmark, and x = -1 and y = -1 are recorded as predicted landmark. ''' if n_points < 1 : ## Use all hsum , n_points = np . sum ( hmi ), len ( hmi . flatten ()) ind_hmi = np . array ([ range ( input_width )] * input_height ) i1 = np . sum ( ind_hmi * hmi ) / hsum ind_hmi = np . array ([ range ( input_height )] * input_width ) . T i0 = np . sum ( ind_hmi * hmi ) / hsum else : ind = hmi . argsort ( axis = None )[ - n_points :] ## pick the largest n_points topind = np . unravel_index ( ind , hmi . shape ) index = np . unravel_index ( hmi . argmax (), hmi . shape ) i0 , i1 , hsum = 0 , 0 , 0 for ind in zip ( topind [ 0 ], topind [ 1 ]): h = hmi [ ind [ 0 ], ind [ 1 ]] hsum += h i0 += ind [ 0 ] * h i1 += ind [ 1 ] * h i0 /= hsum i1 /= hsum if hsum / n_points <= thresh : i0 , i1 = - 1 , - 1 return ([ i1 , i0 ]) def transfer_xy_coord ( hm , n_points = 64 , thresh = 0.2 ): ''' hm : np.array of shape (height,width, n-heatmap) transfer heatmap to (x,y) coordinates the output contains np.array (Nlandmark * 2,) * 2 for x and y coordinates, containing the landmark location. ''' assert len ( hm . shape ) == 3 Nlandmark = hm . shape [ - 1 ] #est_xy = -1*np.ones(shape = (Nlandmark, 2)) est_xy = [] for i in range ( Nlandmark ): hmi = hm [:,:, i ] est_xy . extend ( get_ave_xy ( hmi , n_points , thresh )) return ( est_xy ) ## (Nlandmark * 2,) def transfer_target ( y_pred , thresh = 0 , n_points = 64 ): ''' y_pred : np.array of the shape (N, height, width, Nlandmark) output : (N, Nlandmark * 2) ''' y_pred_xy = [] for i in range ( y_pred . shape [ 0 ]): hm = y_pred [ i ] y_pred_xy . append ( transfer_xy_coord ( hm , n_points , thresh )) return ( np . array ( y_pred_xy )) def getRMSE ( y_pred_xy , y_train_xy , pick_not_NA ): res = y_pred_xy [ pick_not_NA ] - y_train_xy [ pick_not_NA ] RMSE = np . sqrt ( np . mean ( res ** 2 )) return ( RMSE ) nimage = 500 rmelabels = [ \"(x,y) from est heatmap VS (x,y) from true heatmap\" , \"(x,y) from est heatmap VS true (x,y) \" , \"(x,y) from true heatmap VS true (x,y) \" ] n_points_width = range ( 1 , 10 ) res = [] n_points_final , min_rmse = - 1 , np . Inf for nw in n_points_width + [ 0 ]: n_points = nw * nw y_pred_xy = transfer_target ( y_pred [: nimage ], 0 , n_points ) y_train_xy = transfer_target ( y_tra [: nimage ], 0 , n_points ) pick_not_NA = ( y_train_xy != - 1 ) ts = [ getRMSE ( y_pred_xy , y_train_xy , pick_not_NA )] ts . append ( getRMSE ( y_pred_xy , y_train0 . values [: nimage ], pick_not_NA )) ts . append ( getRMSE ( y_train_xy , y_train0 . values [: nimage ], pick_not_NA )) res . append ( ts ) print ( \"n_points to evaluate (x,y) coordinates = {}\" . format ( n_points )) print ( \" RMSE\" ) for r , lab in zip ( ts , rmelabels ): print ( \" {}:{:5.3f}\" . format ( lab , r )) if min_rmse > ts [ 2 ]: min_rmse = ts [ 2 ] n_points_final = n_points res = np . array ( res ) for i , lab in enumerate ( rmelabels ): plt . plot ( n_points_width + [ input_width ], res [:, i ], label = lab ) plt . legend () plt . ylabel ( \"RMSE\" ) plt . xlabel ( \"n_points\" ) plt . show () n_points to evaluate (x,y) coordinates = 1 RMSE (x,y) from est heatmap VS (x,y) from true heatmap:0.986 (x,y) from est heatmap VS true (x,y) :0.940 (x,y) from true heatmap VS true (x,y) :0.289 n_points to evaluate (x,y) coordinates = 4 RMSE (x,y) from est heatmap VS (x,y) from true heatmap:0.830 (x,y) from est heatmap VS true (x,y) :0.808 (x,y) from true heatmap VS true (x,y) :0.197 n_points to evaluate (x,y) coordinates = 9 RMSE (x,y) from est heatmap VS (x,y) from true heatmap:0.736 (x,y) from est heatmap VS true (x,y) :0.728 (x,y) from true heatmap VS true (x,y) :0.137 n_points to evaluate (x,y) coordinates = 16 RMSE (x,y) from est heatmap VS (x,y) from true heatmap:0.677 (x,y) from est heatmap VS true (x,y) :0.675 (x,y) from true heatmap VS true (x,y) :0.119 n_points to evaluate (x,y) coordinates = 25 RMSE (x,y) from est heatmap VS (x,y) from true heatmap:0.645 (x,y) from est heatmap VS true (x,y) :0.652 (x,y) from true heatmap VS true (x,y) :0.083 n_points to evaluate (x,y) coordinates = 36 RMSE (x,y) from est heatmap VS (x,y) from true heatmap:0.629 (x,y) from est heatmap VS true (x,y) :0.642 (x,y) from true heatmap VS true (x,y) :0.085 n_points to evaluate (x,y) coordinates = 49 RMSE (x,y) from est heatmap VS (x,y) from true heatmap:0.628 (x,y) from est heatmap VS true (x,y) :0.645 (x,y) from true heatmap VS true (x,y) :0.090 n_points to evaluate (x,y) coordinates = 64 RMSE (x,y) from est heatmap VS (x,y) from true heatmap:0.619 (x,y) from est heatmap VS true (x,y) :0.642 (x,y) from true heatmap VS true (x,y) :0.097 n_points to evaluate (x,y) coordinates = 81 RMSE (x,y) from est heatmap VS (x,y) from true heatmap:0.615 (x,y) from est heatmap VS true (x,y) :0.639 (x,y) from true heatmap VS true (x,y) :0.106 n_points to evaluate (x,y) coordinates = 0 RMSE (x,y) from est heatmap VS (x,y) from true heatmap:0.854 (x,y) from est heatmap VS true (x,y) :0.909 (x,y) from true heatmap VS true (x,y) :0.163 Prepare submission Evaluate the model performance on testing images for submission. In [18]: y_pred_test = model . predict ( X_test ) ## estimated heatmap y_pred_test = y_pred_test . reshape ( - 1 , output_height , output_width , nClasses ) y_pred_test_xy = transfer_target ( y_pred_test , thresh = 0 , n_points = n_points_final ) ## estimated xy coord y_pred_test_xy = pd . DataFrame ( y_pred_test_xy , columns = nm_landmarks ) IdLookup = pd . read_csv ( os . path . expanduser ( FIdLookup )) def prepare_submission ( y_pred4 , loc ): ''' loc : the path to the submission file save a .csv file that can be submitted to kaggle ''' ImageId = IdLookup [ \"ImageId\" ] FeatureName = IdLookup [ \"FeatureName\" ] RowId = IdLookup [ \"RowId\" ] submit = [] for rowId , irow , landmark in zip ( RowId , ImageId , FeatureName ): submit . append ([ rowId , y_pred4 [ landmark ] . iloc [ irow - 1 ]]) submit = pd . DataFrame ( submit , columns = [ \"RowId\" , \"Location\" ]) ## adjust the scale submit [ \"Location\" ] = submit [ \"Location\" ] submit . to_csv ( loc , index = False ) print ( \"File is saved at:\" + loc ) filename = \"result/FCNish_point{:03.0f}.csv\" . format ( n_points_final ) prepare_submission ( y_pred_test_xy , filename ) File is saved at:result/FCNish_point025.csv if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Achieving top 5 in Kaggle's facial keypoints detection using FCN"},{"url":"Learn-about-Fully-Convolutional-Networks-for-semantic-segmentation.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In this blog post, I will learn a semantic segmentation problem and review fully convolutional networks. In an image for the semantic segmentation, each pixcel is usually labeled with the class of its enclosing object or region. For example, a pixcel might belongs to a road, car, building or a person. The semantic segmentation problem requires to make a classification at every pixel. I will use Fully Convolutional Networks (FCN) to classify every pixcel. To understand the semantic segmentation problem, let's look at an example data prepared by divamgupta . Note: I will use this example data rather than famous segmentation data e.g., pascal VOC2012 because it requires pre-processing. Reference Fully Convolutional Networks for Semantic Segmentation Upsamling Up-sampling with Transposed Convolution FCN8 implmentation in Keras by divamgupta VGG16 defenition in Keras ## Data Sample semantic segmentation data by divamgupta pascal VOC2012 pascal VOC2012 mirror First, I download data from: https://drive.google.com/file/d/0B0d9ZiqAgFkiOHR1NTJhWVJMNEU/view and save the downloaded data1 folder in the current directory. In [1]: dir_data = \"dataset1/\" dir_seg = dir_data + \"/annotations_prepped_train/\" dir_img = dir_data + \"/images_prepped_train/\" Visualize a single segmentation image In this data, there are 12 segmentation classes and the image is from a driving car. In [2]: import cv2 , os import numpy as np import matplotlib.pyplot as plt import seaborn as sns ## seaborn has white grid by default so I will get rid of this. sns . set_style ( \"whitegrid\" , { 'axes.grid' : False }) ldseg = np . array ( os . listdir ( dir_seg )) ## pick the first image file fnm = ldseg [ 0 ] print ( fnm ) ## read in the original image and segmentation labels seg = cv2 . imread ( dir_seg + fnm ) # (360, 480, 3) img_is = cv2 . imread ( dir_img + fnm ) print ( \"seg.shape={}, img_is.shape={}\" . format ( seg . shape , img_is . shape )) ## Check the number of labels mi , ma = np . min ( seg ), np . max ( seg ) n_classes = ma - mi + 1 print ( \"minimum seg = {}, maximum seg = {}, Total number of segmentation classes = {}\" . format ( mi , ma , n_classes )) fig = plt . figure ( figsize = ( 5 , 5 )) ax = fig . add_subplot ( 1 , 1 , 1 ) ax . imshow ( img_is ) ax . set_title ( \"original image\" ) plt . show () fig = plt . figure ( figsize = ( 15 , 10 )) for k in range ( mi , ma + 1 ): ax = fig . add_subplot ( 3 , n_classes / 3 , k + 1 ) ax . imshow (( seg == k ) * 1.0 ) ax . set_title ( \"label = {}\" . format ( k )) plt . show () 0016E5_01620.png seg.shape=(360, 480, 3), img_is.shape=(360, 480, 3) minimum seg = 0, maximum seg = 11, Total number of segmentation classes = 12 Data preprocessing: Resize image To simplify the problem, I will reshape all the images to the same size: (224,224). Why (224,224)? This is the iamge shape used in VGG and FCN model in this blog uses a network that takes advantage of VGG structure. The FCN model becomes easier to explain when the image shape is (224,224). However, FCN does not requires the image shape to be (224,224). Let's visualize how the resizing make the images look like. The images look fine. In [3]: import random def give_color_to_seg_img ( seg , n_classes ): ''' seg : (input_width,input_height,3) ''' if len ( seg . shape ) == 3 : seg = seg [:,:, 0 ] seg_img = np . zeros ( ( seg . shape [ 0 ], seg . shape [ 1 ], 3 ) ) . astype ( 'float' ) colors = sns . color_palette ( \"hls\" , n_classes ) for c in range ( n_classes ): segc = ( seg == c ) seg_img [:,:, 0 ] += ( segc * ( colors [ c ][ 0 ] )) seg_img [:,:, 1 ] += ( segc * ( colors [ c ][ 1 ] )) seg_img [:,:, 2 ] += ( segc * ( colors [ c ][ 2 ] )) return ( seg_img ) input_height , input_width = 224 , 224 output_height , output_width = 224 , 224 ldseg = np . array ( os . listdir ( dir_seg )) for fnm in ldseg [ np . random . choice ( len ( ldseg ), 3 , replace = False )]: fnm = fnm . split ( \".\" )[ 0 ] seg = cv2 . imread ( dir_seg + fnm + \".png\" ) # (360, 480, 3) img_is = cv2 . imread ( dir_img + fnm + \".png\" ) seg_img = give_color_to_seg_img ( seg , n_classes ) fig = plt . figure ( figsize = ( 20 , 40 )) ax = fig . add_subplot ( 1 , 4 , 1 ) ax . imshow ( seg_img ) ax = fig . add_subplot ( 1 , 4 , 2 ) ax . imshow ( img_is / 255.0 ) ax . set_title ( \"original image {}\" . format ( img_is . shape [: 2 ])) ax = fig . add_subplot ( 1 , 4 , 3 ) ax . imshow ( cv2 . resize ( seg_img ,( input_height , input_width ))) ax = fig . add_subplot ( 1 , 4 , 4 ) ax . imshow ( cv2 . resize ( img_is ,( output_height , output_width )) / 255.0 ) ax . set_title ( \"resized to {}\" . format (( output_height , output_width ))) plt . show () Resize all the images. We have 367 images in this dataset. In [4]: def getImageArr ( path , width , height ): img = cv2 . imread ( path , 1 ) img = np . float32 ( cv2 . resize ( img , ( width , height ))) / 127.5 - 1 return img def getSegmentationArr ( path , nClasses , width , height ): seg_labels = np . zeros (( height , width , nClasses )) img = cv2 . imread ( path , 1 ) img = cv2 . resize ( img , ( width , height )) img = img [:, : , 0 ] for c in range ( nClasses ): seg_labels [: , : , c ] = ( img == c ) . astype ( int ) ##seg_labels = np.reshape(seg_labels, ( width*height,nClasses )) return seg_labels images = os . listdir ( dir_img ) images . sort () segmentations = os . listdir ( dir_seg ) segmentations . sort () X = [] Y = [] for im , seg in zip ( images , segmentations ) : X . append ( getImageArr ( dir_img + im , input_width , input_height ) ) Y . append ( getSegmentationArr ( dir_seg + seg , n_classes , output_width , output_height ) ) X , Y = np . array ( X ) , np . array ( Y ) print ( X . shape , Y . shape ) ((367, 224, 224, 3), (367, 224, 224, 12)) Import Keras and Tensorflow to develop deep learning FCN models In [5]: ## Import usual libraries import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras , sys , time , warnings from keras.models import * from keras.layers import * import pandas as pd warnings . filterwarnings ( \"ignore\" ) os . environ [ \"CUDA_DEVICE_ORDER\" ] = \"PCI_BUS_ID\" config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"2\" set_session ( tf . Session ( config = config )) print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )); del keras print ( \"tensorflow version {}\" . format ( tf . __version__ )) Using TensorFlow backend. python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.0.6 tensorflow version 1.2.1 From classifier to dense FCN The recent successful deep learning models such as VGG are originally designed for classification task. The network stacks convolution layers together with down-sampling layers, such as max-pooling, and then finally stacks fully connected layers. Appending a fully connected layer enables the network to learn something using global information where the spatial arrangement of the input falls away. Fully convosutional network For the segmentation task, however, spatial infomation should be stored to make a pixcel-wise classification. FCN allows this by making all the layers of VGG to convolusional layers. Fully convolutional indicates that the neural network is composed of convolutional layers without any fully-connected layers usually found at the end of the network. Fully Convolutional Networks for Semantic Segmentation motivates the use of fully convolutional networks by \"convolutionalizing\" popular CNN architectures e.g. VGG can also be viewed as FCN. ... fully connected layers can also be viewed as convolutions with kernels that cover their entire input regions. Doing so casts them into fully convolutional networks that take input of any size and output classification maps. (Section 3.1) The model I used in this blog post is FCN8 from Fully Convolutional Networks for Semantic Segmentation . It deplicates VGG16 net by discarding the final classifier layer and convert all fully connected layers to convolutions. Fully Convolutional Networks for Semantic Segmentation appends a 1 x 1 convolution with channel dimension the same as the number of segmentation classes (in our case, this is 12) to predict scores at each of the coarse output locations, followed by upsampling deconvolution layers which brings back low resolution image to the output image size. In our example, output image size is (output_height, output_width) = (224,224). Upsampling The upsampling layer brings low resolution image to high resolution. There are various upsamping methods. This presentation gives a good overview. For example, one may double the image resolution by duplicating each pixcel twice. This is so-called nearest neighbor approach and implemented in Keras's UpSampling2D . Another method may be bilinear upsampling, which linearly interpolates the nearest four inputs. These upsampling layers do not have weights/parameters so the model is not flexible. Instead, FCN8 uses upsampling procedure called backwards convolusion (sometimes called deconvolution ) with some output stride. This method simply reverses the forward and backward passes of convolution and implemented in Keras's Conv2DTranspose . This deconvolusion upsampling layer is well explained in this blog post: Up-sampling with Transposed Convolution . In FCN8, the upsampling layer is followed by several skip connections. See details at Fully Convolutional Networks for Semantic Segmentation . I downloaded VGG16 weights from fchollet's Github This is a massive .h5 file (57MB). In [6]: ## location of VGG weights VGG_Weights_path = \"../FacialKeypoint/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\" In [7]: def FCN8 ( nClasses , input_height = 224 , input_width = 224 ): ## input_height and width must be devisible by 32 because maxpooling with filter size = (2,2) is operated 5 times, ## which makes the input_height and width 2&#94;5 = 32 times smaller assert input_height % 32 == 0 assert input_width % 32 == 0 IMAGE_ORDERING = \"channels_last\" img_input = Input ( shape = ( input_height , input_width , 3 )) ## Assume 224,224,3 ## Block 1 x = Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block1_conv1' , data_format = IMAGE_ORDERING )( img_input ) x = Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block1_conv2' , data_format = IMAGE_ORDERING )( x ) x = MaxPooling2D (( 2 , 2 ), strides = ( 2 , 2 ), name = 'block1_pool' , data_format = IMAGE_ORDERING )( x ) f1 = x # Block 2 x = Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block2_conv1' , data_format = IMAGE_ORDERING )( x ) x = Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block2_conv2' , data_format = IMAGE_ORDERING )( x ) x = MaxPooling2D (( 2 , 2 ), strides = ( 2 , 2 ), name = 'block2_pool' , data_format = IMAGE_ORDERING )( x ) f2 = x # Block 3 x = Conv2D ( 256 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block3_conv1' , data_format = IMAGE_ORDERING )( x ) x = Conv2D ( 256 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block3_conv2' , data_format = IMAGE_ORDERING )( x ) x = Conv2D ( 256 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block3_conv3' , data_format = IMAGE_ORDERING )( x ) x = MaxPooling2D (( 2 , 2 ), strides = ( 2 , 2 ), name = 'block3_pool' , data_format = IMAGE_ORDERING )( x ) pool3 = x # Block 4 x = Conv2D ( 512 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block4_conv1' , data_format = IMAGE_ORDERING )( x ) x = Conv2D ( 512 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block4_conv2' , data_format = IMAGE_ORDERING )( x ) x = Conv2D ( 512 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block4_conv3' , data_format = IMAGE_ORDERING )( x ) pool4 = MaxPooling2D (( 2 , 2 ), strides = ( 2 , 2 ), name = 'block4_pool' , data_format = IMAGE_ORDERING )( x ) ## (None, 14, 14, 512) # Block 5 x = Conv2D ( 512 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block5_conv1' , data_format = IMAGE_ORDERING )( pool4 ) x = Conv2D ( 512 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block5_conv2' , data_format = IMAGE_ORDERING )( x ) x = Conv2D ( 512 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , name = 'block5_conv3' , data_format = IMAGE_ORDERING )( x ) pool5 = MaxPooling2D (( 2 , 2 ), strides = ( 2 , 2 ), name = 'block5_pool' , data_format = IMAGE_ORDERING )( x ) ## (None, 7, 7, 512) #x = Flatten(name='flatten')(x) #x = Dense(4096, activation='relu', name='fc1')(x) # <--> o = ( Conv2D( 4096 , ( 7 , 7 ) , activation='relu' , padding='same', data_format=IMAGE_ORDERING))(o) # assuming that the input_height = input_width = 224 as in VGG data #x = Dense(4096, activation='relu', name='fc2')(x) # <--> o = ( Conv2D( 4096 , ( 1 , 1 ) , activation='relu' , padding='same', data_format=IMAGE_ORDERING))(o) # assuming that the input_height = input_width = 224 as in VGG data #x = Dense(1000 , activation='softmax', name='predictions')(x) # <--> o = ( Conv2D( nClasses , ( 1 , 1 ) ,kernel_initializer='he_normal' , data_format=IMAGE_ORDERING))(o) # assuming that the input_height = input_width = 224 as in VGG data vgg = Model ( img_input , pool5 ) vgg . load_weights ( VGG_Weights_path ) ## loading VGG weights for the encoder parts of FCN8 n = 4096 o = ( Conv2D ( n , ( 7 , 7 ) , activation = 'relu' , padding = 'same' , name = \"conv6\" , data_format = IMAGE_ORDERING ))( pool5 ) conv7 = ( Conv2D ( n , ( 1 , 1 ) , activation = 'relu' , padding = 'same' , name = \"conv7\" , data_format = IMAGE_ORDERING ))( o ) ## 4 times upsamping for pool4 layer conv7_4 = Conv2DTranspose ( nClasses , kernel_size = ( 4 , 4 ) , strides = ( 4 , 4 ) , use_bias = False , data_format = IMAGE_ORDERING )( conv7 ) ## (None, 224, 224, 10) ## 2 times upsampling for pool411 pool411 = ( Conv2D ( nClasses , ( 1 , 1 ) , activation = 'relu' , padding = 'same' , name = \"pool4_11\" , data_format = IMAGE_ORDERING ))( pool4 ) pool411_2 = ( Conv2DTranspose ( nClasses , kernel_size = ( 2 , 2 ) , strides = ( 2 , 2 ) , use_bias = False , data_format = IMAGE_ORDERING ))( pool411 ) pool311 = ( Conv2D ( nClasses , ( 1 , 1 ) , activation = 'relu' , padding = 'same' , name = \"pool3_11\" , data_format = IMAGE_ORDERING ))( pool3 ) o = Add ( name = \"add\" )([ pool411_2 , pool311 , conv7_4 ]) o = Conv2DTranspose ( nClasses , kernel_size = ( 8 , 8 ) , strides = ( 8 , 8 ) , use_bias = False , data_format = IMAGE_ORDERING )( o ) o = ( Activation ( 'softmax' ))( o ) model = Model ( img_input , o ) return model model = FCN8 ( nClasses = n_classes , input_height = 224 , input_width = 224 ) model . summary () ____________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ==================================================================================================== input_1 (InputLayer) (None, 224, 224, 3) 0 ____________________________________________________________________________________________________ block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 input_1[0][0] ____________________________________________________________________________________________________ block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 block1_conv1[0][0] ____________________________________________________________________________________________________ block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 block1_conv2[0][0] ____________________________________________________________________________________________________ block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 block1_pool[0][0] ____________________________________________________________________________________________________ block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 block2_conv1[0][0] ____________________________________________________________________________________________________ block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 block2_conv2[0][0] ____________________________________________________________________________________________________ block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 block2_pool[0][0] ____________________________________________________________________________________________________ block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 block3_conv1[0][0] ____________________________________________________________________________________________________ block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 block3_conv2[0][0] ____________________________________________________________________________________________________ block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 block3_conv3[0][0] ____________________________________________________________________________________________________ block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 block3_pool[0][0] ____________________________________________________________________________________________________ block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 block4_conv1[0][0] ____________________________________________________________________________________________________ block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 block4_conv2[0][0] ____________________________________________________________________________________________________ block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 block4_conv3[0][0] ____________________________________________________________________________________________________ block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 block4_pool[0][0] ____________________________________________________________________________________________________ block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 block5_conv1[0][0] ____________________________________________________________________________________________________ block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 block5_conv2[0][0] ____________________________________________________________________________________________________ block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 block5_conv3[0][0] ____________________________________________________________________________________________________ conv6 (Conv2D) (None, 7, 7, 4096) 102764544 block5_pool[0][0] ____________________________________________________________________________________________________ pool4_11 (Conv2D) (None, 14, 14, 12) 6156 block4_pool[0][0] ____________________________________________________________________________________________________ conv7 (Conv2D) (None, 7, 7, 4096) 16781312 conv6[0][0] ____________________________________________________________________________________________________ conv2d_transpose_2 (Conv2DTransp (None, 28, 28, 12) 576 pool4_11[0][0] ____________________________________________________________________________________________________ pool3_11 (Conv2D) (None, 28, 28, 12) 3084 block3_pool[0][0] ____________________________________________________________________________________________________ conv2d_transpose_1 (Conv2DTransp (None, 28, 28, 12) 786432 conv7[0][0] ____________________________________________________________________________________________________ add (Add) (None, 28, 28, 12) 0 conv2d_transpose_2[0][0] pool3_11[0][0] conv2d_transpose_1[0][0] ____________________________________________________________________________________________________ conv2d_transpose_3 (Conv2DTransp (None, 224, 224, 12) 9216 add[0][0] ____________________________________________________________________________________________________ activation_1 (Activation) (None, 224, 224, 12) 0 conv2d_transpose_3[0][0] ==================================================================================================== Total params: 135,066,008 Trainable params: 135,066,008 Non-trainable params: 0 ____________________________________________________________________________________________________ Split between training and testing data In [8]: from sklearn.utils import shuffle train_rate = 0.85 index_train = np . random . choice ( X . shape [ 0 ], int ( X . shape [ 0 ] * train_rate ), replace = False ) index_test = list ( set ( range ( X . shape [ 0 ])) - set ( index_train )) X , Y = shuffle ( X , Y ) X_train , y_train = X [ index_train ], Y [ index_train ] X_test , y_test = X [ index_test ], Y [ index_test ] print ( X_train . shape , y_train . shape ) print ( X_test . shape , y_test . shape ) ((311, 224, 224, 3), (311, 224, 224, 12)) ((56, 224, 224, 3), (56, 224, 224, 12)) Training starts here In [9]: from keras import optimizers sgd = optimizers . SGD ( lr = 1E-2 , decay = 5 ** ( - 4 ), momentum = 0.9 , nesterov = True ) model . compile ( loss = 'categorical_crossentropy' , optimizer = sgd , metrics = [ 'accuracy' ]) hist1 = model . fit ( X_train , y_train , validation_data = ( X_test , y_test ), batch_size = 32 , epochs = 200 , verbose = 2 ) Train on 311 samples, validate on 56 samples Epoch 1/200 15s - loss: 2.6345 - acc: 0.0885 - val_loss: 2.4823 - val_acc: 0.0958 Epoch 2/200 8s - loss: 2.4792 - acc: 0.1003 - val_loss: 2.4750 - val_acc: 0.1053 Epoch 3/200 8s - loss: 2.4671 - acc: 0.1113 - val_loss: 2.4546 - val_acc: 0.1229 Epoch 4/200 8s - loss: 2.4317 - acc: 0.1392 - val_loss: 2.3907 - val_acc: 0.1665 Epoch 5/200 8s - loss: 2.3087 - acc: 0.2091 - val_loss: 2.2193 - val_acc: 0.2802 Epoch 6/200 8s - loss: 2.3756 - acc: 0.2297 - val_loss: 2.2791 - val_acc: 0.2472 Epoch 7/200 8s - loss: 2.1612 - acc: 0.2812 - val_loss: 2.1125 - val_acc: 0.3080 Epoch 8/200 8s - loss: 2.0254 - acc: 0.3275 - val_loss: 1.8800 - val_acc: 0.3197 Epoch 9/200 8s - loss: 1.7938 - acc: 0.3446 - val_loss: 1.7250 - val_acc: 0.3914 Epoch 10/200 8s - loss: 1.6222 - acc: 0.4469 - val_loss: 1.5345 - val_acc: 0.4825 Epoch 11/200 8s - loss: 1.4211 - acc: 0.5345 - val_loss: 1.3592 - val_acc: 0.5589 Epoch 12/200 8s - loss: 1.2757 - acc: 0.6089 - val_loss: 1.2417 - val_acc: 0.6301 Epoch 13/200 8s - loss: 1.1592 - acc: 0.6525 - val_loss: 1.1354 - val_acc: 0.6541 Epoch 14/200 8s - loss: 1.1307 - acc: 0.6640 - val_loss: 1.0794 - val_acc: 0.6612 Epoch 15/200 8s - loss: 1.0512 - acc: 0.6718 - val_loss: 1.0449 - val_acc: 0.6650 Epoch 16/200 8s - loss: 1.0254 - acc: 0.6750 - val_loss: 1.0158 - val_acc: 0.6710 Epoch 17/200 8s - loss: 0.9942 - acc: 0.6804 - val_loss: 0.9911 - val_acc: 0.6725 Epoch 18/200 8s - loss: 0.9881 - acc: 0.6805 - val_loss: 0.9929 - val_acc: 0.6754 Epoch 19/200 8s - loss: 0.9522 - acc: 0.6880 - val_loss: 0.9654 - val_acc: 0.6873 Epoch 20/200 8s - loss: 0.9418 - acc: 0.6931 - val_loss: 0.9677 - val_acc: 0.6752 Epoch 21/200 9s - loss: 0.9383 - acc: 0.6941 - val_loss: 0.9180 - val_acc: 0.6993 Epoch 22/200 8s - loss: 0.9033 - acc: 0.7078 - val_loss: 0.8939 - val_acc: 0.7059 Epoch 23/200 8s - loss: 0.9017 - acc: 0.7161 - val_loss: 0.8951 - val_acc: 0.7040 Epoch 24/200 8s - loss: 0.9023 - acc: 0.7166 - val_loss: 0.8691 - val_acc: 0.7270 Epoch 25/200 8s - loss: 0.8655 - acc: 0.7333 - val_loss: 0.8652 - val_acc: 0.7400 Epoch 26/200 8s - loss: 0.8823 - acc: 0.7300 - val_loss: 0.8429 - val_acc: 0.7408 Epoch 27/200 8s - loss: 0.8444 - acc: 0.7441 - val_loss: 0.8817 - val_acc: 0.7391 Epoch 28/200 8s - loss: 0.8403 - acc: 0.7477 - val_loss: 1.0048 - val_acc: 0.6672 Epoch 29/200 8s - loss: 0.8483 - acc: 0.7415 - val_loss: 0.8370 - val_acc: 0.7534 Epoch 30/200 9s - loss: 0.8088 - acc: 0.7586 - val_loss: 0.7951 - val_acc: 0.7606 Epoch 31/200 8s - loss: 0.7974 - acc: 0.7621 - val_loss: 0.8393 - val_acc: 0.7411 Epoch 32/200 8s - loss: 0.8218 - acc: 0.7510 - val_loss: 0.7869 - val_acc: 0.7621 Epoch 33/200 8s - loss: 0.7785 - acc: 0.7685 - val_loss: 0.7917 - val_acc: 0.7659 Epoch 34/200 8s - loss: 0.7657 - acc: 0.7732 - val_loss: 0.7743 - val_acc: 0.7665 Epoch 35/200 8s - loss: 0.7687 - acc: 0.7710 - val_loss: 0.7638 - val_acc: 0.7735 Epoch 36/200 8s - loss: 0.8037 - acc: 0.7571 - val_loss: 0.7540 - val_acc: 0.7759 Epoch 37/200 9s - loss: 0.7501 - acc: 0.7776 - val_loss: 0.7492 - val_acc: 0.7795 Epoch 38/200 9s - loss: 0.7475 - acc: 0.7782 - val_loss: 0.7511 - val_acc: 0.7778 Epoch 39/200 8s - loss: 0.7543 - acc: 0.7749 - val_loss: 0.7368 - val_acc: 0.7852 Epoch 40/200 8s - loss: 0.7326 - acc: 0.7834 - val_loss: 0.7496 - val_acc: 0.7782 Epoch 41/200 8s - loss: 0.7495 - acc: 0.7766 - val_loss: 0.7210 - val_acc: 0.7873 Epoch 42/200 8s - loss: 0.7289 - acc: 0.7830 - val_loss: 0.7379 - val_acc: 0.7840 Epoch 43/200 8s - loss: 0.7182 - acc: 0.7874 - val_loss: 0.7095 - val_acc: 0.7922 Epoch 44/200 8s - loss: 0.7171 - acc: 0.7876 - val_loss: 0.7171 - val_acc: 0.7911 Epoch 45/200 8s - loss: 0.7031 - acc: 0.7930 - val_loss: 0.7119 - val_acc: 0.7880 Epoch 46/200 9s - loss: 0.7188 - acc: 0.7866 - val_loss: 0.7189 - val_acc: 0.7826 Epoch 47/200 9s - loss: 0.6940 - acc: 0.7949 - val_loss: 0.6950 - val_acc: 0.7961 Epoch 48/200 9s - loss: 0.6936 - acc: 0.7952 - val_loss: 0.7239 - val_acc: 0.7902 Epoch 49/200 8s - loss: 0.6873 - acc: 0.7970 - val_loss: 0.6993 - val_acc: 0.7970 Epoch 50/200 8s - loss: 0.6846 - acc: 0.7980 - val_loss: 0.6805 - val_acc: 0.8023 Epoch 51/200 8s - loss: 0.6744 - acc: 0.8015 - val_loss: 0.6895 - val_acc: 0.7996 Epoch 52/200 9s - loss: 0.6755 - acc: 0.8016 - val_loss: 0.6692 - val_acc: 0.8064 Epoch 53/200 8s - loss: 0.6861 - acc: 0.7974 - val_loss: 0.6680 - val_acc: 0.8047 Epoch 54/200 8s - loss: 0.6654 - acc: 0.8041 - val_loss: 0.6657 - val_acc: 0.8046 Epoch 55/200 8s - loss: 0.6617 - acc: 0.8052 - val_loss: 0.6636 - val_acc: 0.8078 Epoch 56/200 8s - loss: 0.6483 - acc: 0.8098 - val_loss: 0.6645 - val_acc: 0.8079 Epoch 57/200 8s - loss: 0.6594 - acc: 0.8064 - val_loss: 0.6597 - val_acc: 0.8080 Epoch 58/200 8s - loss: 0.6406 - acc: 0.8127 - val_loss: 0.6548 - val_acc: 0.8131 Epoch 59/200 8s - loss: 0.6406 - acc: 0.8125 - val_loss: 0.6512 - val_acc: 0.8124 Epoch 60/200 8s - loss: 0.6574 - acc: 0.8074 - val_loss: 0.6667 - val_acc: 0.8055 Epoch 61/200 8s - loss: 0.6369 - acc: 0.8139 - val_loss: 0.6523 - val_acc: 0.8072 Epoch 62/200 8s - loss: 0.6305 - acc: 0.8154 - val_loss: 0.6471 - val_acc: 0.8136 Epoch 63/200 8s - loss: 0.6239 - acc: 0.8180 - val_loss: 0.6321 - val_acc: 0.8162 Epoch 64/200 8s - loss: 0.6331 - acc: 0.8147 - val_loss: 0.6675 - val_acc: 0.8008 Epoch 65/200 8s - loss: 0.6255 - acc: 0.8175 - val_loss: 0.6303 - val_acc: 0.8174 Epoch 66/200 9s - loss: 0.6105 - acc: 0.8226 - val_loss: 0.6577 - val_acc: 0.8082 Epoch 67/200 8s - loss: 0.6204 - acc: 0.8191 - val_loss: 0.6384 - val_acc: 0.8173 Epoch 68/200 8s - loss: 0.6063 - acc: 0.8238 - val_loss: 0.6641 - val_acc: 0.8031 Epoch 69/200 9s - loss: 0.6169 - acc: 0.8200 - val_loss: 0.6190 - val_acc: 0.8229 Epoch 70/200 8s - loss: 0.5995 - acc: 0.8258 - val_loss: 0.6584 - val_acc: 0.8073 Epoch 71/200 9s - loss: 0.6114 - acc: 0.8218 - val_loss: 0.6136 - val_acc: 0.8259 Epoch 72/200 9s - loss: 0.5907 - acc: 0.8294 - val_loss: 0.6093 - val_acc: 0.8273 Epoch 73/200 9s - loss: 0.5867 - acc: 0.8302 - val_loss: 0.6287 - val_acc: 0.8201 Epoch 74/200 8s - loss: 0.5837 - acc: 0.8319 - val_loss: 0.6119 - val_acc: 0.8234 Epoch 75/200 8s - loss: 0.5882 - acc: 0.8300 - val_loss: 0.6042 - val_acc: 0.8270 Epoch 76/200 9s - loss: 0.5782 - acc: 0.8326 - val_loss: 0.6350 - val_acc: 0.8145 Epoch 77/200 8s - loss: 0.6022 - acc: 0.8251 - val_loss: 0.6051 - val_acc: 0.8257 Epoch 78/200 8s - loss: 0.5695 - acc: 0.8358 - val_loss: 0.5962 - val_acc: 0.8315 Epoch 79/200 8s - loss: 0.5739 - acc: 0.8347 - val_loss: 0.6215 - val_acc: 0.8208 Epoch 80/200 9s - loss: 0.5800 - acc: 0.8328 - val_loss: 0.5933 - val_acc: 0.8322 Epoch 81/200 8s - loss: 0.5616 - acc: 0.8387 - val_loss: 0.5864 - val_acc: 0.8325 Epoch 82/200 8s - loss: 0.5552 - acc: 0.8405 - val_loss: 0.6287 - val_acc: 0.8190 Epoch 83/200 8s - loss: 0.5966 - acc: 0.8275 - val_loss: 0.5844 - val_acc: 0.8338 Epoch 84/200 8s - loss: 0.5517 - acc: 0.8419 - val_loss: 0.5838 - val_acc: 0.8343 Epoch 85/200 9s - loss: 0.5468 - acc: 0.8435 - val_loss: 0.5854 - val_acc: 0.8335 Epoch 86/200 8s - loss: 0.5727 - acc: 0.8341 - val_loss: 0.5845 - val_acc: 0.8349 Epoch 87/200 8s - loss: 0.5448 - acc: 0.8445 - val_loss: 0.5862 - val_acc: 0.8345 Epoch 88/200 8s - loss: 0.5627 - acc: 0.8383 - val_loss: 0.5756 - val_acc: 0.8370 Epoch 89/200 8s - loss: 0.5380 - acc: 0.8466 - val_loss: 0.5701 - val_acc: 0.8387 Epoch 90/200 9s - loss: 0.5374 - acc: 0.8465 - val_loss: 0.5705 - val_acc: 0.8384 Epoch 91/200 9s - loss: 0.5294 - acc: 0.8488 - val_loss: 0.5668 - val_acc: 0.8395 Epoch 92/200 9s - loss: 0.5707 - acc: 0.8349 - val_loss: 0.6709 - val_acc: 0.7998 Epoch 93/200 8s - loss: 0.5632 - acc: 0.8368 - val_loss: 0.5680 - val_acc: 0.8394 Epoch 94/200 8s - loss: 0.5289 - acc: 0.8487 - val_loss: 0.5656 - val_acc: 0.8393 Epoch 95/200 8s - loss: 0.5234 - acc: 0.8503 - val_loss: 0.5604 - val_acc: 0.8418 Epoch 96/200 9s - loss: 0.5292 - acc: 0.8485 - val_loss: 0.5594 - val_acc: 0.8418 Epoch 97/200 8s - loss: 0.5144 - acc: 0.8534 - val_loss: 0.5702 - val_acc: 0.8396 Epoch 98/200 8s - loss: 0.5140 - acc: 0.8533 - val_loss: 0.5863 - val_acc: 0.8350 Epoch 99/200 8s - loss: 0.5246 - acc: 0.8499 - val_loss: 0.5567 - val_acc: 0.8421 Epoch 100/200 8s - loss: 0.5066 - acc: 0.8558 - val_loss: 0.5548 - val_acc: 0.8433 Epoch 101/200 8s - loss: 0.5209 - acc: 0.8512 - val_loss: 0.5506 - val_acc: 0.8440 Epoch 102/200 8s - loss: 0.5029 - acc: 0.8571 - val_loss: 0.5689 - val_acc: 0.8388 Epoch 103/200 8s - loss: 0.5168 - acc: 0.8516 - val_loss: 0.5542 - val_acc: 0.8422 Epoch 104/200 8s - loss: 0.5036 - acc: 0.8565 - val_loss: 0.5650 - val_acc: 0.8380 Epoch 105/200 8s - loss: 0.5074 - acc: 0.8544 - val_loss: 0.5675 - val_acc: 0.8383 Epoch 106/200 8s - loss: 0.5117 - acc: 0.8540 - val_loss: 0.5447 - val_acc: 0.8457 Epoch 107/200 8s - loss: 0.4990 - acc: 0.8579 - val_loss: 0.5515 - val_acc: 0.8438 Epoch 108/200 8s - loss: 0.4947 - acc: 0.8594 - val_loss: 0.5631 - val_acc: 0.8369 Epoch 109/200 8s - loss: 0.4960 - acc: 0.8579 - val_loss: 0.5633 - val_acc: 0.8401 Epoch 110/200 8s - loss: 0.4961 - acc: 0.8579 - val_loss: 0.5500 - val_acc: 0.8432 Epoch 111/200 8s - loss: 0.4937 - acc: 0.8588 - val_loss: 0.5465 - val_acc: 0.8448 Epoch 112/200 8s - loss: 0.4907 - acc: 0.8603 - val_loss: 0.5764 - val_acc: 0.8357 Epoch 113/200 9s - loss: 0.4873 - acc: 0.8613 - val_loss: 0.5452 - val_acc: 0.8454 Epoch 114/200 8s - loss: 0.4769 - acc: 0.8646 - val_loss: 0.5358 - val_acc: 0.8477 Epoch 115/200 9s - loss: 0.5442 - acc: 0.8441 - val_loss: 0.5497 - val_acc: 0.8426 Epoch 116/200 8s - loss: 0.4905 - acc: 0.8603 - val_loss: 0.5376 - val_acc: 0.8477 Epoch 117/200 8s - loss: 0.4764 - acc: 0.8648 - val_loss: 0.5345 - val_acc: 0.8485 Epoch 118/200 8s - loss: 0.4947 - acc: 0.8581 - val_loss: 0.5379 - val_acc: 0.8469 Epoch 119/200 9s - loss: 0.4701 - acc: 0.8672 - val_loss: 0.5381 - val_acc: 0.8480 Epoch 120/200 8s - loss: 0.4740 - acc: 0.8652 - val_loss: 0.5315 - val_acc: 0.8489 Epoch 121/200 9s - loss: 0.4642 - acc: 0.8685 - val_loss: 0.5343 - val_acc: 0.8487 Epoch 122/200 8s - loss: 0.4665 - acc: 0.8674 - val_loss: 0.5459 - val_acc: 0.8451 Epoch 123/200 8s - loss: 0.4630 - acc: 0.8688 - val_loss: 0.5298 - val_acc: 0.8500 Epoch 124/200 9s - loss: 0.4556 - acc: 0.8714 - val_loss: 0.5318 - val_acc: 0.8486 Epoch 125/200 8s - loss: 0.4677 - acc: 0.8671 - val_loss: 0.5622 - val_acc: 0.8413 Epoch 126/200 8s - loss: 0.4634 - acc: 0.8683 - val_loss: 0.5258 - val_acc: 0.8508 Epoch 127/200 8s - loss: 0.4568 - acc: 0.8708 - val_loss: 0.5472 - val_acc: 0.8445 Epoch 128/200 9s - loss: 0.4681 - acc: 0.8669 - val_loss: 0.5265 - val_acc: 0.8507 Epoch 129/200 9s - loss: 0.4569 - acc: 0.8702 - val_loss: 0.5280 - val_acc: 0.8505 Epoch 130/200 9s - loss: 0.4532 - acc: 0.8714 - val_loss: 0.5234 - val_acc: 0.8517 Epoch 131/200 8s - loss: 0.4531 - acc: 0.8714 - val_loss: 0.5580 - val_acc: 0.8403 Epoch 132/200 8s - loss: 0.4724 - acc: 0.8654 - val_loss: 0.5249 - val_acc: 0.8520 Epoch 133/200 8s - loss: 0.4497 - acc: 0.8728 - val_loss: 0.5377 - val_acc: 0.8453 Epoch 134/200 9s - loss: 0.4525 - acc: 0.8708 - val_loss: 0.5580 - val_acc: 0.8396 Epoch 135/200 8s - loss: 0.4450 - acc: 0.8744 - val_loss: 0.5307 - val_acc: 0.8494 Epoch 136/200 9s - loss: 0.4604 - acc: 0.8680 - val_loss: 0.5272 - val_acc: 0.8502 Epoch 137/200 8s - loss: 0.4410 - acc: 0.8753 - val_loss: 0.5218 - val_acc: 0.8521 Epoch 138/200 9s - loss: 0.4524 - acc: 0.8719 - val_loss: 0.5386 - val_acc: 0.8466 Epoch 139/200 8s - loss: 0.4381 - acc: 0.8767 - val_loss: 0.5209 - val_acc: 0.8518 Epoch 140/200 8s - loss: 0.4336 - acc: 0.8779 - val_loss: 0.5237 - val_acc: 0.8519 Epoch 141/200 8s - loss: 0.4395 - acc: 0.8754 - val_loss: 0.5429 - val_acc: 0.8450 Epoch 142/200 8s - loss: 0.4376 - acc: 0.8760 - val_loss: 0.5281 - val_acc: 0.8500 Epoch 143/200 8s - loss: 0.4379 - acc: 0.8757 - val_loss: 0.5400 - val_acc: 0.8448 Epoch 144/200 9s - loss: 0.4380 - acc: 0.8758 - val_loss: 0.5325 - val_acc: 0.8481 Epoch 145/200 8s - loss: 0.4335 - acc: 0.8777 - val_loss: 0.5372 - val_acc: 0.8464 Epoch 146/200 9s - loss: 0.4421 - acc: 0.8745 - val_loss: 0.5167 - val_acc: 0.8524 Epoch 147/200 8s - loss: 0.4237 - acc: 0.8808 - val_loss: 0.5236 - val_acc: 0.8504 Epoch 148/200 9s - loss: 0.4263 - acc: 0.8797 - val_loss: 0.5258 - val_acc: 0.8496 Epoch 149/200 9s - loss: 0.4366 - acc: 0.8767 - val_loss: 0.5343 - val_acc: 0.8469 Epoch 150/200 8s - loss: 0.4365 - acc: 0.8767 - val_loss: 0.5168 - val_acc: 0.8528 Epoch 151/200 9s - loss: 0.4244 - acc: 0.8801 - val_loss: 0.5549 - val_acc: 0.8411 Epoch 152/200 9s - loss: 0.4284 - acc: 0.8787 - val_loss: 0.5144 - val_acc: 0.8535 Epoch 153/200 8s - loss: 0.4207 - acc: 0.8818 - val_loss: 0.5240 - val_acc: 0.8505 Epoch 154/200 8s - loss: 0.4340 - acc: 0.8767 - val_loss: 0.5102 - val_acc: 0.8543 Epoch 155/200 9s - loss: 0.4172 - acc: 0.8827 - val_loss: 0.5235 - val_acc: 0.8511 Epoch 156/200 8s - loss: 0.4235 - acc: 0.8805 - val_loss: 0.5109 - val_acc: 0.8546 Epoch 157/200 9s - loss: 0.4127 - acc: 0.8841 - val_loss: 0.5160 - val_acc: 0.8527 Epoch 158/200 8s - loss: 0.4219 - acc: 0.8803 - val_loss: 0.5179 - val_acc: 0.8531 Epoch 159/200 8s - loss: 0.4110 - acc: 0.8844 - val_loss: 0.5185 - val_acc: 0.8527 Epoch 160/200 8s - loss: 0.4288 - acc: 0.8776 - val_loss: 0.5277 - val_acc: 0.8486 Epoch 161/200 8s - loss: 0.4074 - acc: 0.8856 - val_loss: 0.5185 - val_acc: 0.8527 Epoch 162/200 9s - loss: 0.4089 - acc: 0.8852 - val_loss: 0.5154 - val_acc: 0.8536 Epoch 163/200 9s - loss: 0.4194 - acc: 0.8816 - val_loss: 0.5074 - val_acc: 0.8555 Epoch 164/200 8s - loss: 0.4028 - acc: 0.8872 - val_loss: 0.5131 - val_acc: 0.8540 Epoch 165/200 9s - loss: 0.4149 - acc: 0.8829 - val_loss: 0.5130 - val_acc: 0.8534 Epoch 166/200 9s - loss: 0.4024 - acc: 0.8870 - val_loss: 0.5213 - val_acc: 0.8528 Epoch 167/200 8s - loss: 0.4022 - acc: 0.8870 - val_loss: 0.5251 - val_acc: 0.8500 Epoch 168/200 9s - loss: 0.4081 - acc: 0.8850 - val_loss: 0.5180 - val_acc: 0.8533 Epoch 169/200 9s - loss: 0.4013 - acc: 0.8874 - val_loss: 0.5397 - val_acc: 0.8472 Epoch 170/200 8s - loss: 0.4096 - acc: 0.8842 - val_loss: 0.5206 - val_acc: 0.8523 Epoch 171/200 8s - loss: 0.3963 - acc: 0.8890 - val_loss: 0.5155 - val_acc: 0.8535 Epoch 172/200 8s - loss: 0.4020 - acc: 0.8866 - val_loss: 0.5463 - val_acc: 0.8437 Epoch 173/200 9s - loss: 0.4096 - acc: 0.8845 - val_loss: 0.5082 - val_acc: 0.8554 Epoch 174/200 8s - loss: 0.3943 - acc: 0.8897 - val_loss: 0.5112 - val_acc: 0.8550 Epoch 175/200 8s - loss: 0.3940 - acc: 0.8895 - val_loss: 0.5319 - val_acc: 0.8510 Epoch 176/200 8s - loss: 0.3995 - acc: 0.8873 - val_loss: 0.5217 - val_acc: 0.8527 Epoch 177/200 8s - loss: 0.3987 - acc: 0.8877 - val_loss: 0.5183 - val_acc: 0.8515 Epoch 178/200 8s - loss: 0.3957 - acc: 0.8883 - val_loss: 0.5058 - val_acc: 0.8560 Epoch 179/200 8s - loss: 0.3901 - acc: 0.8906 - val_loss: 0.5072 - val_acc: 0.8556 Epoch 180/200 8s - loss: 0.3956 - acc: 0.8891 - val_loss: 0.5070 - val_acc: 0.8561 Epoch 181/200 8s - loss: 0.3882 - acc: 0.8911 - val_loss: 0.5173 - val_acc: 0.8525 Epoch 182/200 8s - loss: 0.3977 - acc: 0.8871 - val_loss: 0.5101 - val_acc: 0.8549 Epoch 183/200 8s - loss: 0.3849 - acc: 0.8925 - val_loss: 0.5121 - val_acc: 0.8536 Epoch 184/200 8s - loss: 0.3819 - acc: 0.8936 - val_loss: 0.5203 - val_acc: 0.8521 Epoch 185/200 8s - loss: 0.3919 - acc: 0.8899 - val_loss: 0.5063 - val_acc: 0.8558 Epoch 186/200 8s - loss: 0.3897 - acc: 0.8908 - val_loss: 0.5236 - val_acc: 0.8498 Epoch 187/200 8s - loss: 0.3873 - acc: 0.8915 - val_loss: 0.5080 - val_acc: 0.8568 Epoch 188/200 8s - loss: 0.3859 - acc: 0.8915 - val_loss: 0.5192 - val_acc: 0.8529 Epoch 189/200 8s - loss: 0.3792 - acc: 0.8940 - val_loss: 0.5178 - val_acc: 0.8533 Epoch 190/200 8s - loss: 0.3963 - acc: 0.8874 - val_loss: 0.5385 - val_acc: 0.8456 Epoch 191/200 8s - loss: 0.3862 - acc: 0.8910 - val_loss: 0.5071 - val_acc: 0.8562 Epoch 192/200 9s - loss: 0.3838 - acc: 0.8927 - val_loss: 0.5186 - val_acc: 0.8518 Epoch 193/200 9s - loss: 0.3828 - acc: 0.8927 - val_loss: 0.5325 - val_acc: 0.8474 Epoch 194/200 9s - loss: 0.3877 - acc: 0.8913 - val_loss: 0.5066 - val_acc: 0.8558 Epoch 195/200 8s - loss: 0.3775 - acc: 0.8946 - val_loss: 0.5084 - val_acc: 0.8563 Epoch 196/200 8s - loss: 0.3758 - acc: 0.8951 - val_loss: 0.5235 - val_acc: 0.8532 Epoch 197/200 8s - loss: 0.3846 - acc: 0.8916 - val_loss: 0.5046 - val_acc: 0.8566 Epoch 198/200 8s - loss: 0.3737 - acc: 0.8957 - val_loss: 0.5085 - val_acc: 0.8562 Epoch 199/200 8s - loss: 0.3785 - acc: 0.8937 - val_loss: 0.5080 - val_acc: 0.8565 Epoch 200/200 9s - loss: 0.3731 - acc: 0.8956 - val_loss: 0.5104 - val_acc: 0.8552 Plot the change in loss over epochs In [10]: for key in [ 'loss' , 'val_loss' ]: plt . plot ( hist1 . history [ key ], label = key ) plt . legend () plt . show () Calculate intersection over union for each segmentation class In [11]: y_pred = model . predict ( X_test ) y_predi = np . argmax ( y_pred , axis = 3 ) y_testi = np . argmax ( y_test , axis = 3 ) print ( y_testi . shape , y_predi . shape ) ((56, 224, 224), (56, 224, 224)) In [12]: def IoU ( Yi , y_predi ): ## mean Intersection over Union ## Mean IoU = TP/(FN + TP + FP) IoUs = [] Nclass = int ( np . max ( Yi )) + 1 for c in range ( Nclass ): TP = np . sum ( ( Yi == c ) & ( y_predi == c ) ) FP = np . sum ( ( Yi != c ) & ( y_predi == c ) ) FN = np . sum ( ( Yi == c ) & ( y_predi != c )) IoU = TP / float ( TP + FP + FN ) print ( \"class {:02.0f}: #TP={:6.0f}, #FP={:6.0f}, #FN={:5.0f}, IoU={:4.3f}\" . format ( c , TP , FP , FN , IoU )) IoUs . append ( IoU ) mIoU = np . mean ( IoUs ) print ( \"_________________\" ) print ( \"Mean IoU: {:4.3f}\" . format ( mIoU )) IoU ( y_testi , y_predi ) class 00: #TP=396127, #FP= 44118, #FN=25057, IoU=0.851 class 01: #TP=636671, #FP=136114, #FN=50651, IoU=0.773 class 02: #TP= 11, #FP= 217, #FN=35459, IoU=0.000 class 03: #TP=840769, #FP= 33361, #FN=42747, IoU=0.917 class 04: #TP= 87924, #FP= 30018, #FN=46374, IoU=0.535 class 05: #TP=224159, #FP= 58411, #FN=48666, IoU=0.677 class 06: #TP= 1621, #FP= 3406, #FN=38029, IoU=0.038 class 07: #TP= 12823, #FP= 12332, #FN=15184, IoU=0.318 class 08: #TP=137585, #FP= 34734, #FN=40379, IoU=0.647 class 09: #TP= 1043, #FP= 3328, #FN=19466, IoU=0.044 class 10: #TP= 362, #FP= 1100, #FN= 9388, IoU=0.033 class 11: #TP= 63859, #FP= 49763, #FN=35502, IoU=0.428 _________________ Mean IoU: 0.438 Visualize the model performance Looks reasonable! In [13]: shape = ( 224 , 224 ) n_classes = 10 for i in range ( 10 ): img_is = ( X_test [ i ] + 1 ) * ( 255.0 / 2 ) seg = y_predi [ i ] segtest = y_testi [ i ] fig = plt . figure ( figsize = ( 10 , 30 )) ax = fig . add_subplot ( 1 , 3 , 1 ) ax . imshow ( img_is / 255.0 ) ax . set_title ( \"original\" ) ax = fig . add_subplot ( 1 , 3 , 2 ) ax . imshow ( give_color_to_seg_img ( seg , n_classes )) ax . set_title ( \"predicted class\" ) ax = fig . add_subplot ( 1 , 3 , 3 ) ax . imshow ( give_color_to_seg_img ( segtest , n_classes )) ax . set_title ( \"true class\" ) plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Learn about Fully Convolutional Networks for semantic segmentation"},{"url":"Extract-URL-for-the-pictures in-Flickrs-public-album-via-python.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } When you make your own website using third-party clouds, e.g. Heroku, there is a restriction for the uploaded data size. For example, Heroku only allows 500MB memory space. This might become a too tight constraint if you want to add some pictures to your websites as high resolution pictures nowadays could easily be about 10MB. Instead of uploading photos to these clouds together with your .html and .css files, it may be wise to use other image hosting service such as Flickr or Instagram: you may upload images to these image hosting services, make them public, and then simply add link to the photos in the image hosting service in your webpage. In this blog post, I will explore this approach and present how to extract pictures from Flickr's public album using python. I assume that you know the user_id of the owner of Flickr's public album. user_id is most likely of the form: 123456789@N12. For example my user id is 157237655@N08 See here to find user_id of Fliker users. References Flickr example: Retrieve a Flickr gallery Flickr's API method, the flickr.photosets.getLists endpoint Flickr's API method, the flickr.photosets.getPhotos endpoint Step 0: Get an API key to make requests Before you can make a request with the Flickr API, you'll need an API key (free). Follow the instruction here . When you register an app, you're given a key and secret. Step 1: Get photoset id via flickr.photosets.getList endpoint To extract photos in Flikr's public albums, I need to know a photoset id. This is an identifier for the album. In [1]: import requests import json , sys sys . path . append ( '../' ) from personal import flikr_api_key as api_key def get_requestURL ( user_id , endpoint = \"getList\" ): user_id = user_id . replace ( \"@\" , \"%40\" ) url_upto_apikey = ( \"https://api.flickr.com/services/rest/?method=flickr.photosets.\" + endpoint + \"&api;_key=\" + api_key + \"&user;_id=\" + user_id + \"&format;=json&nojsoncallback;=1\" ) return ( url_upto_apikey ) user_id = \"157237655@N08\" url = get_requestURL ( user_id , endpoint = \"getList\" ) strlist = requests . get ( url ) . content json_data = json . loads ( strlist ) albums = json_data [ \"photosets\" ][ \"photoset\" ] print ( \" {} albums found for user_id= {} \" . format ( len ( albums ), user_id )) 14 albums found for user_id=157237655@N08 Let's look at some of the album titles In [2]: photosetids , titles = [], [] for album in albums : print ( \"___\" ) print ( \"album title= {} photoset_id= {} \" . format ( album [ 'title' ][ '_content' ], album [ \"id\" ])) photosetids . append ( album [ \"id\" ]) titles . append ( album [ 'title' ][ '_content' ]) ___ album title= 5/5/2018 Day12 photoset_id=72157666947668397 ___ album title=5/7/2018 Day14 photoset_id=72157695104270601 ___ album title=5/6/2018 Day13 photoset_id=72157668988962728 ___ album title=5/4/2018 Day11 photoset_id=72157695104114951 ___ album title=5/2/2018 Day9 photoset_id=72157696204993104 ___ album title=5/3/2018 Day10 photoset_id=72157696204977184 ___ album title=4/28/2018 Day5 photoset_id=72157693696318772 ___ album title=4/29/2018 Day6 photoset_id=72157695104025101 ___ album title=4/30/2018 Day7 photoset_id=72157666947100257 ___ album title=5/1/2018 Day8 photoset_id=72157695104003731 ___ album title=4/27/2018 Day4 photoset_id=72157693696247802 ___ album title=4/26/2018 Day3 photoset_id=72157695103914511 ___ album title=4/25/2018 Day2 photoset_id=72157666947001577 ___ album title=4/24/2018 Day1 photoset_id=72157668988636988 Step 2: For each album, extract infomation of all the photos. In order to find the unique URL to each photo, I need to know: farm ID server ID ID secret Such infomation is extracted using flickr.photosets.getPhotos API. In [3]: def get_photo_url ( farmId , serverId , Id , secret ): return (( \"https://farm\" + str ( farmId ) + \".staticflickr.com/\" + serverId + \"/\" + Id + '_' + secret + \".jpg\" )) URLs = {} for photoset_id , title in zip ( photosetids , titles ): ## for each album url = get_requestURL ( user_id , endpoint = \"getPhotos\" ) + \"&photoset;_id=\" + photoset_id strlist = requests . get ( url ) . content json1_data = json . loads ( strlist ) urls = [] for pic in json1_data [ \"photoset\" ][ \"photo\" ]: ## for each picture in an album urls . append ( get_photo_url ( pic [ \"farm\" ], pic [ 'server' ], pic [ \"id\" ], pic [ \"secret\" ])) URLs [ photoset_id ] = urls Finally let's plot the extracted photos from the first 4 albums The codes seem to be working! In [4]: from IPython.display import Image , display count = 1 for i , ( photoset_id , urls ) in enumerate ( URLs . items ()): print ( \"______________________\" ) print ( \" {} , photoset_id= {} \" . format ( titles [ i ], photoset_id )) for url in urls : print ( url ) display ( Image ( url = url , width = 200 , height = 200 )) count += 1 if count > 4 : break ______________________ 5/5/2018 Day12, photoset_id=72157666947668397 https://farm1.staticflickr.com/830/27250089027_6daf9c74e2.jpg https://farm1.staticflickr.com/979/42120644961_d7332f4076.jpg ______________________ 5/7/2018 Day14, photoset_id=72157695104270601 https://farm1.staticflickr.com/830/42074479842_914d7fc46c.jpg https://farm1.staticflickr.com/912/42074477492_bb637a15b7.jpg https://farm1.staticflickr.com/908/40313692800_0aece89fb9.jpg ______________________ 5/6/2018 Day13, photoset_id=72157668988962728 https://farm1.staticflickr.com/909/27250105877_83c32cf63d.jpg https://farm1.staticflickr.com/945/40313708090_323d71ce91.jpg https://farm1.staticflickr.com/830/27250089027_6daf9c74e2.jpg https://farm1.staticflickr.com/979/42120644961_d7332f4076.jpg ______________________ 5/4/2018 Day11, photoset_id=72157695104114951 https://farm1.staticflickr.com/979/41219229825_0c17b21d8b.jpg https://farm1.staticflickr.com/981/41219233685_72f336ee61.jpg https://farm1.staticflickr.com/908/41219241425_c6d6609c1f.jpg https://farm1.staticflickr.com/980/41219254165_cedb9220b9.jpg https://farm1.staticflickr.com/909/41219255495_7606541450.jpg if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Extract URL for the pictures in Flickr's public album via python"},{"url":"Extract-GPS-data-from-Suuntos-MovesCount.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } I have been using Suunto Ambit 2 as the device for recording any sports activities e.g. running, trekking, or swimming. Recorded activities can be updated in the cloud Suunto Movescount , and see the summary of activities, e.g., speed, pace, elevation gain or elevation loss. Suunto Movescount is a growing sports community where you can create your own sports diary to collect and share your activities as well as customize your compatible Suunto watch. Sometimes, when I do not have a watch with me, I use my iphone's MovesCount app to record the activities as alternative. Unfortunatelly, With the iphone and watch the recording performance varies substantially: When there is no signal, the recording performances of iphone's MovesCount app, especially the net elevation gain and loss, are just awefully unreliable . In this post, I attempt to clean the GPS data from the iphone's MovesCount app, and make the performance conparable to the watch. First, let's look at the GPS data from the two devices. I recorded my hiking activity in Nepal using the two devices: Mountaineering move with Suunto Ambit 2 watch and Trekking move with Phone. I downloaded the GPS data from my movescount page. Click Tools -> Exprot as KML. In [1]: from IPython.display import Image Image ( \"kml_photo.png\" , width = '100%' , height = 100 ) Out[1]: The exported kml files are saved in the two locations. In [2]: dir_move = \"./data/\" path_phone_move_kml = dir_move + '/phone/Move_2018_05_01_06_52_33_Trekking.kml' path_watch_move_kml = dir_move + '/Move_2018_05_01_06_51_39_Mountaineering_Lobche_Gorekshep_EBC.kml' Extract the GPS coordinate infomation and the altitude infomation from the kml file, and save it into a panda dataframe. In [3]: import os import numpy as np import fastkml import pandas as pd import matplotlib.pyplot as plt In [4]: def get_coordinates ( path_kml ): ''' extract lat, lon, altitude info from the kml data ''' doc = open ( path_kml , 'rb+' ) . read () k = fastkml . KML () k . from_string ( doc ) kmlstr = k . to_string () coordstr = kmlstr . split ( \" \" )[ 1 ] . split ( \" \" )[ 0 ] coordlst = coordstr . split ( \" \" ) l = [] for point in coordlst : pointlst = point . split ( \",\" ) l . append ( pointlst ) df = pd . DataFrame ( l , columns = [ \"lon\" , \"lat\" , \"altitude\" ]) for col in df . columns : df [ col ] = pd . to_numeric ( df [ col ], errors = 'coerce' ) return ( df ) df_phone = get_coordinates ( path_phone_move_kml ) df_watch = get_coordinates ( path_watch_move_kml ) ## In this activity, the first 230 samples from the watch and the 14500 samples from the phone roughtly agree df_watch = df_watch . iloc [ 1 : 230 ,:] df_phone = df_phone . iloc [: 14500 ,:] print ( df_phone . shape , df_watch . shape ) df_phone [: 5 ] No geometries found Object does not have a geometry No geometries found Object does not have a geometry (14500, 3) (229, 3) Out[4]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } lon lat altitude 0 86.810962 27.948544 4923.200195 1 86.810972 27.948542 4923.291324 2 86.810975 27.948541 4925.231034 3 86.810979 27.948542 4927.935645 4 86.810985 27.948542 4927.390356 Let's plot the GPS coordinate infos from the two devices using gmplot The latitude and longitude pairs seem to agree between the records from the two devies. In [5]: from gmplot import gmplot filenm = \"my_map.html\" # Place map gmap = gmplot . GoogleMapPlotter ( 27.971815 , 86.828963 , 14 ) gmap . plot ( df_phone [ \"lat\" ], df_phone [ \"lon\" ], 'cornflowerblue' , edge_width = 2 ) gmap . plot ( df_watch [ \"lat\" ], df_watch [ \"lon\" ], '#3B0B39' , edge_width = 2 ) # Draw gmap . draw ( filenm ) In [6]: import sys sys . path . append ( \"../../trip/\" ) ## my google api key from personal import MY_API_KEY def jupyter_display ( gmplot_filename , google_api_key ): \"\"\" Hack to display a gmplot map in Jupyter reference https://github.com/vgm64/gmplot/issues/16 \"\"\" with open ( gmplot_filename , \"r+\" ) as f : f_string = f . read () url_pattern = \"https://maps.googleapis.com/maps/api/js?libraries=visualization&sensor;=true_or_false\" f_string = f_string . replace ( url_pattern , url_pattern + \"&key;= %s \" % google_api_key ) f . write ( f_string ) jupyter_display ( filenm , MY_API_KEY ) Image ( \"my_map.png\" , width = '100%' , height = 100 ) ## from IPython.display import IFrame ## IFrame(filenm, width=990, height=800) Out[6]: We see that the elevation gain and loss infomation are substantially different! I know that the elvation gain of this trail around 5200m elevation was never 1535m (I know I cannot finish the trail otherwise!), and the recording from the watch seems more right! In [7]: from math import sin , cos , sqrt , atan2 , radians def get_distance_btw2pts ( lat1 , lon1 , lat2 , lon2 ): ''' approximate radius of earth in km https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude ''' lat1 = radians ( lat1 ) lon1 = radians ( lon1 ) lat2 = radians ( lat2 ) lon2 = radians ( lon2 ) R = 6373.0 dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin ( dlat / 2 ) ** 2 + cos ( lat1 ) * cos ( lat2 ) * sin ( dlon / 2 ) ** 2 c = 2 * atan2 ( sqrt ( a ), sqrt ( 1 - a )) distance = R * c return ( distance ) def get_distance ( df_phone ): count = 1 df_phone [ \"dist\" ] = np . NaN for lat1 , lat2 , lon1 , lon2 in zip ( df_phone [ \"lat\" ][: - 1 ], df_phone [ \"lat\" ][ 1 :], df_phone [ \"lon\" ][: - 1 ], df_phone [ \"lon\" ][ 1 :]): dis = get_distance_btw2pts ( lat1 , lon1 , lat2 , lon2 ) df_phone [ \"dist\" ] . iloc [ count ] = dis count += 1 print ( \" distance = {:3.1f} km\" . format ( np . sum ( df_phone [ \"dist\" ]))) return ( df_phone ) def get_alt_change ( alt ): diff_alt = alt [ 1 :] - alt [: - 1 ] gain = np . sum ( diff_alt [ diff_alt > 0 ]) loss = np . sum ( diff_alt [ diff_alt < 0 ]) print ( \" Elevation gain {:3.0f} m, Elevation loss {:3.0f} m\" . format ( gain , loss )) print ( \" N samples = {:3d} \" . format ( len ( alt ))) return gain , loss print ( \"watch\" ) get_distance ( df_watch ) get_alt_change ( df_watch [ \"altitude\" ] . values ) print ( \"phone\" ) get_distance ( df_phone ) get_alt_change ( df_phone [ \"altitude\" ] . values ) print ( \"-------------\" ) watch distance = 7.6km Elevation gain 471m, Elevation loss -162m N samples = 229 phone distance = 8.7km Elevation gain 1535m, Elevation loss -1197m N samples = 14500 ------------- Cleaning the GPS records from iphone To clean the GPS records from iphone, let's learn about the recording frequency of the difference devices: Recording frequency Devise Move Type Recording frequency Phone Trekking Every 1 second Suunto Ambit 2 watch Trekking Every 10 seconds Suunto Ambit 2 watch Mountaineering Every 10 seconds Suunto Ambit 2 watch Running Every 1 second Despite that the iphone's recording performance is not so great, the recording frequency of the Trekking activity using iphone (every 1 second) is higher than Trekking or the Mountaineering of Suunto Ambit 2 watch (every 10 seconds). This suggests that we can take the rolling average of every 10 samples to clean the latitudes, longitudes and altitudes infomation from the iphone record. GPS data cleaning By taking the rolling average, the discripancies between the watch and phone records become somewhat more similar (yet they still differ). In [8]: def get_rolling_average ( vec , k = 5 ): rvec = np . zeros ( len ( vec )) for t in range ( len ( vec )): start = np . max ([ t - k , 0 ]) end = np . min ([ t + k , len ( vec )]) rvec [ t ] = np . mean ( vec [ start : end ]) return ( rvec ) for col in df_phone . columns : df_phone [ col ] = get_rolling_average ( df_phone [ col ] . values , k = 5 ) print ( \"watch\" ) get_distance ( df_watch ) get_alt_change ( df_watch [ \"altitude\" ] . values ) print ( \"phone\" ) get_distance ( df_phone ) get_alt_change ( df_phone [ \"altitude\" ] . values ) print ( \"-------------\" ) watch distance = 7.6km Elevation gain 471m, Elevation loss -162m N samples = 229 phone distance = 8.0km Elevation gain 643m, Elevation loss -308m N samples = 14500 ------------- if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Extract GPS data from Suunto's MovesCount"},{"url":"Driver-facial-keypoint-detection-with-data-augmentation.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } I will revisit Driver's facial keypoint detection . In this blog, I will improve the landmark detection model performance with data augmentation . ImageDataGenerator for the purpose of landmark detection is implemented at my github account and discussed in my previous blog - Data augmentation for facial keypoint detection- . In [1]: ## Import usual libraries import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras , sys , time , os , warnings , cv2 import numpy as np import pandas as pd warnings . filterwarnings ( \"ignore\" ) os . environ [ \"CUDA_DEVICE_ORDER\" ] = \"PCI_BUS_ID\" config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"4\" set_session ( tf . Session ( config = config )) print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )); del keras print ( \"tensorflow version {}\" . format ( tf . __version__ )) Using TensorFlow backend. python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.1.3 tensorflow version 1.5.0 In [2]: dir_data = \"DrivFace/\" Read in the annotated data This step is the same as previous analysis . I will read in the drivePoints.txt into a panda dataframe that contains the facial keypoints and driver's infomation for each image. The data contain not only the facial keypoints but also the bounding box. In [3]: labels = open ( dir_data + \"drivPoints.txt\" ) . read () labels = labels . split ( \" \\r\\n \" ) lines = [ line . split ( \",\" ) for line in labels ] df_label = pd . DataFrame ( lines [ 1 :], columns = lines [ 0 ]) cols = list ( set ( df_label . columns ) - set ([ \"fileName\" ])) df_label [ cols ] = df_label [ cols ] . apply ( pd . to_numeric , errors = 'coerce' ) ## remove the rows with NA print ( df_label . shape ) df_label = df_label . dropna () print ( df_label . shape ) df_label . head ( 3 ) (607, 19) (606, 19) Out[3]: fileName subject imgNum label ang xF yF wF hF xRE yRE xLE yLE xN yN xRM yRM xLM yLM 0 20130529_01_Driv_001_f 1.0 1.0 2.0 0.0 292.0 209.0 100.0 112.0 323.0 232.0 367.0 231.0 353.0 254.0 332.0 278.0 361.0 278.0 1 20130529_01_Driv_002_f 1.0 2.0 2.0 0.0 286.0 200.0 109.0 128.0 324.0 235.0 366.0 235.0 353.0 258.0 333.0 281.0 361.0 281.0 2 20130529_01_Driv_003_f 1.0 3.0 2.0 0.0 290.0 204.0 105.0 121.0 325.0 240.0 367.0 239.0 351.0 260.0 334.0 282.0 362.0 282.0 The annotated landmarks are: Right Eye (RE) Left Eye (LE) Nose (N) Right Mouth (RM) Left Mouth (LM) In the panda dataframe above, the (x,y) coordinates of these landmarks are recorded in the columns named as x\"name of the landmark\" and y\"name of the landmark\". In [4]: landmarks = [ \"RE\" , \"LE\" , \"N\" , \"RM\" , \"LM\" ] Extract image data The image data is extracted in the same order as the row of df_label. In the following code, I create a list object imgs such that: imgs[i] contains a numpy array of image corresponding to the df_label.iloc[i,:]. In [5]: from keras.preprocessing.image import img_to_array , load_img imgs = [] count = 0 for jpg in df_label [ \"fileName\" ]: if count % 100 == 0 : print ( count ) try : img = img_to_array ( load_img ( dir_data + \"/DrivImages/\" + jpg + \".jpg\" )) except : img = [] pass imgs . append ( img ) count += 1 assert len ( imgs ) == df_label . shape [ 0 ] 0 100 200 300 400 500 600 Bounding box of varying size is available Our data luckily provides bounging box. But the width and height of the box vary across images but they are always more than 90. The plots below shows the histogram of the width and heights of the bounding box. In our analysis, we assume that the bounding box is given as in previous analysis . Then the model performance was assessed on the landmark detection accuracy within the \"down-sized\" bounding box. Here, \"down-sized\" bounding box means that the original image is trimmed to have bounding box and then the bounded image is rescaled to have reduced size; width = 90 and height = 90. So, during the training, we may trim the original image into bounding box, resize the bounded image to have size (90, 90), and then translate the resized bounded image into various augmented images using the ImageDataGenerator for landmark detection . However,resizing the image before the image translation will reduce the number of potential augmented images that image translation can make, in comparisons to doing the image translation after resizing. Therefore, for training, I will first trim the image to have the same bounding box size (without resizing). As the bounding box provided from the data has varying width and height, I adjusted the width and height to have 150 by extending the box size (while keeping the center of the box to be the same as the original). Then the bounded (150,150) image is translated to various augmented images using ImageDataGenerator for landmark detection . The augmented images are finally translated to have size (90,90). For testing image, we will use the \"down-sized\" bounding box having the size (90,90) so that the model performance is comparable to previous analysis . In [6]: fig = plt . figure ( figsize = ( 15 , 7 )) for count , label in enumerate ([ \"wF\" , \"hF\" ], 1 ): ax = fig . add_subplot ( 2 , 1 , count ) ml = int ( np . max ( df_label [ label ])) ax . hist ( df_label [ label ]) ax . set_title ( \"bounding box in data: {}, Max={}\" . format ( label , ml )) ax . set_xlim ([ 80 , 160 ]) plt . show () Define the input image size for CNN. In [7]: target_shape = ( 90 , 90 , 3 ) Prepare data in two ways: training data trim using bounding box upsize the image to (150,150) this data will be passed to ImageDataGenerator and used as the original image for the data augmentation (after data augmentation, image will be resized to (90,90) and then the resized image is passed to our deep learning model). evaluation data trim using bounding box downsize the image to (90,90) this data will be used for evaluating data In [8]: def get_bbox ( img , row ): ''' row : df_label.iloc[i,:] use the bounding box defined in dataframe ''' faces = ( int ( row [ \"xF\" ]), int ( row [ \"yF\" ]), int ( row [ \"wF\" ]), int ( row [ \"hF\" ])) return ( faces ) def adjust_loc ( rows , x_topleft = 0 , y_topleft = 0 ): ''' adjust the landmark coordinates with respect to bbox output: [(xRE,yRE), (xLE,yLE), (xN,yN), (xRM,yRM), (xLM,yLM)] with respect to the bounding box ''' out = [] for lm in landmarks : out . append (( int ( rows [ \"x\" + lm ]) - x_topleft , int ( rows [ \"y\" + lm ]) - y_topleft )) return ( out ) def adjust_xy ( y , shape_orig , shape_new ): ''' y : [x1,y1,x2,y2,...] ''' y [ 0 :: 2 ] = y [ 0 :: 2 ] * shape_new [ 1 ] / float ( shape_orig [ 1 ]) y [ 1 :: 2 ] = y [ 1 :: 2 ] * shape_new [ 0 ] / float ( shape_orig [ 0 ]) return y def expand_bbox ( faces , adjw = 150 , adjh = 150 ): ( x , y , w , h ) = faces winc = int ( adjw - w ) hinc = int ( adjh - h ) x -= int ( winc / 2.0 ) y -= int ( hinc / 2.0 ) return ( x , y , adjw , adjh ) ## increase the width and height of the bounding box by prop*100 % bd_shape = ( 150 , 150 ) prop_bd = 0.3 imgs_bd , lms_bd = [], [] imgs_bd_test , lms_bd_test = [], [] count = 0 for i , img in enumerate ( imgs ): row = df_label . iloc [ i ,:] faces = get_bbox ( img , row ) ( x , y , w , h ) = faces ys = np . array ( adjust_loc ( row , x , y )) . flatten () _img = img [ y :( y + h ), x :( x + w )] imgr = cv2 . resize ( _img , target_shape [: 2 ]) ys = adjust_xy ( ys , _img . shape , imgr . shape ) lms_bd_test . append ( ys ) imgs_bd_test . append ( imgr ) if row [ \"subject\" ] != 4 : ( x , y , w , h ) = expand_bbox ( faces , * bd_shape ) lms_bd . append ( adjust_loc ( row , x , y )) imgs_bd . append ( img [ y :( y + h ), x :( x + w )]) assert len ( imgs_bd_test ) == df_label . shape [ 0 ] print ( \" {} training images\" . format ( len ( lms_bd ))) print ( \" {} evaluation images\" . format ( len ( lms_bd_test ))) 516 training images 606 evaluation images Training data Let's look at the training images before the image translation. In [9]: for i in [ 100 , 200 , 400 ]: img , img_bd , lm_bd = imgs [ i ], imgs_bd [ i ], lms_bd [ i ] row = df_label . iloc [ i ,:] fig = plt . figure ( figsize = ( 15 , 4 )) fig . subplots_adjust ( hspace = 0 , wspace = 0 ) ## ------------------- ## ## Original image ## ------------------- ## ax = fig . add_subplot ( 1 , 3 , 1 ) ax . imshow ( img / 255.0 ) ax . set_title ( \"original image\" ) ax . axis ( \"off\" ) for ( x , y ) in adjust_loc ( row ): ax . scatter ( x , y ) ## ------------------- ## ## Original bbox ## ------------------- ## ax = fig . add_subplot ( 1 , 3 , 2 ) ( x , y , w , h ) = get_bbox ( img , row ) ax . imshow ( img [ y :( y + h ), x :( x + w )] / 255.0 ) ax . set_title ( \"original bounding box\" ) for ( x , y ) in adjust_loc ( row , x , y ): ax . scatter ( x , y ) ## ------------------- ## ## Expanded bbox ## ------------------- ## ax = fig . add_subplot ( 1 , 3 , 3 ) ax . imshow ( img_bd / 255.0 ) for ( x , y ) in lm_bd : ax . scatter ( x , y ) ax . set_title ( \"Resized bounding box with shape = {}\" . format ( bd_shape )) plt . show () Instantiate the ImageDataGenerator_landmarks object developed in Data augmentation for facial keypoint detection . The class defenition is available at Github . Just download the ImageDataGenerator_landmarks.py file into the current directory and import the module. I will consider wide zooming range. In [10]: from keras.preprocessing.image import ImageDataGenerator , img_to_array , load_img import ImageDataGenerator_landmarks as idg reload ( idg ) y_scale = np . min ( target_shape [: 2 ]) print ( \"y_scale={}\" . format ( y_scale )) ## pre-processing function is applied AFTER image translation and rescaling to target_shape def scaley ( y ): my = float ( y_scale ) / 2.0 y = ( y - my ) / float ( y_scale ) return ( y ) def preprocessing ( x , y ): x = x / 255.0 y = scaley ( y ) return ( x , y ) datagen = ImageDataGenerator ( rotation_range = 0 , width_shift_range = 0 , height_shift_range = 0 , ## Float. Shear Intensity (Shear angle in counter-clockwise direction in degrees) shear_range = 0 , ## zoom_range: Float or [lower, upper]. ## Range for random zoom. If a float, ## [lower, upper] = [1-zoom_range, 1+zoom_range] zoom_range = [ 0.7 , 1.001 ], fill_mode = 'nearest' , #cval=-2, horizontal_flip = True , vertical_flip = False ) generator = idg . ImageDataGenerator_landmarks ( datagen , preprocessing_function = preprocessing , ignore_horizontal_flip = False , loc_xRE = 0 , loc_xLE = 2 , target_shape = target_shape , flip_indicies = (( 0 , 2 ), # xRE <-> xLE ( 1 , 3 ), # yRE <-> yLE ( 6 , 8 ), # xRM <-> xLM ( 7 , 9 )) # yRM <-> yLM ) y_scale=90 For training images, create y_mask from the landmark's (x,y) coordinates In [11]: xy = [] for img_bd , lm_bd in zip ( imgs_bd , lms_bd ): xy . append ( generator . get_ymask ( img_bd , lm_bd )) xy_train = np . array ( xy ) print ( \"xy_train.shape={}\" . format ( xy_train . shape )) assert xy_train . shape [ 0 ] == len ( imgs_bd ) xy_train.shape=(516, 150, 150, 4) Let's see some example translated images In [12]: def sp ( ax , ys ): my = y_scale / 2.0 for x , y in zip ( ys [ 0 :: 2 ], ys [ 1 :: 2 ]): ax . scatter ( x * y_scale + my , y * y_scale + my ) for xs , ys in generator . flow ( xy_train , batch_size = 600 ): break print ( \"**translated images**\" ) print ( \"x.shape={}\" . format ( xs . shape )) print ( \"x: min={:4.3f}, max={:4.3f}\" . format ( np . min ( xs ), np . max ( xs ))) print ( \"y.shape={}\" . format ( ys . shape )) print ( \"y: min={:4.3f}, max={:4.3f}\" . format ( np . min ( ys ), np . max ( ys ))) Nrow , Ncol , count = 5 , 7 , 1 fig = plt . figure ( figsize = ( 15 , 10 )) for irow in range ( 0 , 500 , 10 ): ax = fig . add_subplot ( Nrow , Ncol , count ) ax . imshow ( xs [ irow ]) sp ( ax , ys [ irow ]) count += 1 if count > Nrow * Ncol : break plt . show () **translated images** x.shape=(1032, 90, 90, 3) x: min=0.000, max=1.000 y.shape=(1032, 10) y: min=-0.437, max=0.360 For testing images, we do not need make y_mask. In [13]: xx = np . array ( imgs_bd_test ) yy = np . array ( lms_bd_test ) print ( \"xx.shape={}\" . format ( xx . shape )) print ( \"yy.shape={}\" . format ( yy . shape )) xx.shape=(606, 90, 90, 3) yy.shape=(606, 10) In [14]: xs , ys = xx / 255.0 , scaley ( yy ) print ( \"**translated images**\" ) print ( \"x.shape={}\" . format ( xs . shape )) print ( \"x: min={:4.3f}, max={:4.3f}\" . format ( np . min ( xs ), np . max ( xs ))) print ( \"y.shape={}\" . format ( ys . shape )) print ( \"y: min={:4.3f}, max={:4.3f}\" . format ( np . min ( ys ), np . max ( ys ))) Nrow , Ncol , count = 3 , 7 , 1 fig = plt . figure ( figsize = ( 15 , 6 )) for irow in range ( 0 , 600 , 20 ): ax = fig . add_subplot ( Nrow , Ncol , count ) ax . imshow ( xs [ irow ]) sp ( ax , ys [ irow ]) count += 1 if count > Nrow * Ncol : break plt . show () **translated images** x.shape=(606, 90, 90, 3) x: min=0.000, max=1.000 y.shape=(606, 10) y: min=-0.367, max=0.356 Model Here, I will use a vanilla CNN loosely based on a state of art model used in Facial Landmark Detection with Tweaked Convolutional Neural Networks . The difference to the paper is that our input dimension is (90,90,3) rather than (40,40,3). In [15]: from keras.layers import Conv2D , MaxPooling2D , Flatten , Dropout , Activation , Dense from keras.models import Sequential batch_size = 64 num_channels = 3 def StandardCNN ( input_shape = ( 150 , 150 , 3 )): ''' WithDropout: If True, then dropout regularlization is added. This feature is experimented later. ''' model = Sequential () # uses theano ordering. Note that we leave the image size as None to allow multiple image sizes model . add ( Conv2D ( 16 , kernel_size = ( 5 , 5 ), name = \"CL1\" , input_shape = input_shape )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 ))) model . add ( Conv2D ( 48 , kernel_size = ( 3 , 3 ), name = \"CL2\" )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 ))) model . add ( Conv2D ( 64 , kernel_size = ( 3 , 3 ), name = \"CL3\" )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ), strides = ( 2 , 2 ))) model . add ( Conv2D ( 64 , kernel_size = ( 2 , 2 ), name = \"CL4\" )) model . add ( Activation ( 'relu' )) model . add ( Flatten ()) model . add ( Dense ( 100 , name = \"FC5\" )) model . add ( Activation ( 'relu' )) model . add ( Dense ( 10 , name = \"FC6\" )) model . compile ( loss = 'mse' , optimizer = 'adam' ) return ( model ) model = StandardCNN ( input_shape = target_shape ) model . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= CL1 (Conv2D) (None, 86, 86, 16) 1216 _________________________________________________________________ activation_1 (Activation) (None, 86, 86, 16) 0 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 43, 43, 16) 0 _________________________________________________________________ CL2 (Conv2D) (None, 41, 41, 48) 6960 _________________________________________________________________ activation_2 (Activation) (None, 41, 41, 48) 0 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 20, 20, 48) 0 _________________________________________________________________ CL3 (Conv2D) (None, 18, 18, 64) 27712 _________________________________________________________________ activation_3 (Activation) (None, 18, 18, 64) 0 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 9, 9, 64) 0 _________________________________________________________________ CL4 (Conv2D) (None, 8, 8, 64) 16448 _________________________________________________________________ activation_4 (Activation) (None, 8, 8, 64) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 4096) 0 _________________________________________________________________ FC5 (Dense) (None, 100) 409700 _________________________________________________________________ activation_5 (Activation) (None, 100) 0 _________________________________________________________________ FC6 (Dense) (None, 10) 1010 ================================================================= Total params: 463,046 Trainable params: 463,046 Non-trainable params: 0 _________________________________________________________________ Training starts here In [16]: batch_size = xy_train . shape [ 0 ] Nepochs = 200 iepoch = 1 hists = [] for xs , ys in generator . flow ( xy_train , batch_size = batch_size ): hist = model . fit ( xs , ys , epochs = 1 , verbose = False ) h = hist . history [ \"loss\" ][ 0 ] hists . append ( h ) if iepoch % 10 == 0 : print ( \"Epoch {:03.0f} - {:8.7f}\" . format ( iepoch , h )) if iepoch > Nepochs : break iepoch += 1 Epoch 010 - 0.0006441 Epoch 020 - 0.0003549 Epoch 030 - 0.0002512 Epoch 040 - 0.0002695 Epoch 050 - 0.0002240 Epoch 060 - 0.0002051 Epoch 070 - 0.0001912 Epoch 080 - 0.0001768 Epoch 090 - 0.0001759 Epoch 100 - 0.0001440 Epoch 110 - 0.0001457 Epoch 120 - 0.0001474 Epoch 130 - 0.0001468 Epoch 140 - 0.0001414 Epoch 150 - 0.0001460 Epoch 160 - 0.0001453 Epoch 170 - 0.0001319 Epoch 180 - 0.0001264 Epoch 190 - 0.0001247 Epoch 200 - 0.0001325 Plot of loss over epochs In [17]: plt . plot ( hists ) plt . xlabel ( \"loss\" ) plt . show () Model performance on testing data In [18]: pick = ( df_label [ \"subject\" ] == 4 ) . values ## ============= ## ## training data ## ============= ## x_tr , y_tr = xx [ ~ pick ], yy [ ~ pick ] y_pred0 = model . predict ( x_tr / 255.0 ) print ( \"Training MSE={:7.6f}\" . format ( np . mean (( y_pred0 - scaley ( y_tr )) ** 2 ))) ## ============= ## ## testing data ## ============= ## x_test , y_test = xx [ pick ], yy [ pick ] y_pred0 = model . predict ( x_test / 255.0 ) print ( \"Testing MSE={:7.6f}\" . format ( np . mean (( y_pred0 - scaley ( y_test )) ** 2 ))) y_pred = y_pred0 * y_scale + ( y_scale / 2.0 ) assert np . all ( y_pred <= np . max ( target_shape )) assert np . all ( y_pred >= 0 ) Training MSE=0.000241 Testing MSE=0.001665 In the previous post, Driver's facial keypoint detection , the model performance was assessed on the 4th driver using the normalized mean Euclidean distances between the true facial keypoint and the estimated one within bounding box. In [19]: df_label [ \"IPD\" ] = np . sqrt (( df_label [ \"xRE\" ] - df_label [ \"xLE\" ]) ** 2 + ( df_label [ \"yRE\" ] - df_label [ \"yLE\" ]) ** 2 ) for ii , facialkp in enumerate ( landmarks ): i = ii * 2 ## Model prediction df_label [ \"Model - data augmentation_\" + facialkp ] = np . NaN xterm = ( y_pred [:, i ] - y_test [:, i ]) ** 2 yterm = ( y_pred [:, i + 1 ] - y_test [:, i + 1 ]) ** 2 ## save it for the test subjects df_label [ \"Model - data augmentation_\" + facialkp ] . loc [ df_label [ \"subject\" ] == 4 ] = np . sqrt ( xterm + yterm ) Model performance on testing data summary Remind you that without data augmentation, the model performance in previous analysis was: Landmark Median normalized ED % (normalized ED < 10 percent) LE 4.184403 95.555556 RE 4.128279 97.777778 N 5.725722 87.777778 RM 3.794306 95.555556 LM 3.349447 100.000000 Clearly, the model performance improved by using data augmentation! In [ ]: def proplessthan ( vec ): values = [ np . median ( vec [ \"value\" ]), np . mean ( vec [ \"value\" ] < 10 ) * 100 ] return ( pd . Series ( values , index = [ \"Median normalized ED\" , \"% (normalized ED < 10%)\" ] )) collabels = [] for nm in landmarks : collabels . extend ([ \"Model - data augmentation_\" + nm ]) df_eval = df_label [ collabels ] df_eval = df_eval . dropna () ## NA is recorded for the training image ## un-pivot so that there is a a single column containing box plot values ## this un-pivot is necessary for seaborn boxplot df_boxplot = pd . melt ( df_eval , value_vars = collabels ) v = np . array ([ term . split ( \"_\" ) for term in df_boxplot [ \"variable\" ]]) df_boxplot [ \"procedure\" ] = v [:, 0 ] df_boxplot [ \"keypoints\" ] = v [:, 1 ] df_boxplot_summary = df_boxplot . groupby ([ \"keypoints\" , \"procedure\" ]) . apply ( proplessthan ) . reset_index () df_boxplot_summary Out[ ]: keypoints procedure Median normalized ED % (normalized ED < 10%) 0 LE Model - data augmentation 5.751284 95.555556 1 LM Model - data augmentation 2.634319 100.000000 2 N Model - data augmentation 5.030684 98.888889 3 RE Model - data augmentation 5.402357 98.888889 4 RM Model - data augmentation 4.277941 100.000000 Visualization of the model performance In [ ]: dir_image = 'driver_data_augmentation/' try : os . mkdir ( dir_image ) except : pass def create_gif ( gifname , dir_image , duration = 1 ): import imageio filenames = np . sort ( os . listdir ( dir_image )) filenames = [ fnm for fnm in filenames if \".png\" in fnm ] with imageio . get_writer ( dir_image + '/' + gifname + '.gif' , mode = 'I' , duration = duration ) as writer : for filename in filenames : image = imageio . imread ( dir_image + filename ) writer . append_data ( image ) for irow in range ( x_test . shape [ 0 ]): img = x_test [ irow ] ys = y_pred [ irow ] fig = plt . figure ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( 1 , 1 , 1 ) ax . axis ( \"off\" ) ax . imshow ( img / 255.0 ) for x , y in zip ( ys [ 0 :: 2 ], ys [ 1 :: 2 ]): ax . scatter ( x , y , c = \"green\" , s = 500 , marker = \"X\" ) plt . savefig ( dir_image + \"/fig{:04.0f}.png\" . format ( irow ), bbox_inches = 'tight' , pad_inches = 0 ) create_gif ( \"driver_data_augmentation\" , dir_image , duration = 0.5 ) plt . close ( 'all' ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Driver's facial keypoint detection with data augmentation"},{"url":"Data-augmentation-for-facial-keypoint-detection.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } The python class ImageDataGenerator_landmarks is available at my github account . This blog explains about his class. Why data augmentation? Deep learning model is data greedy and the performance of the model may be surprisingly bad when testing images vary from training images a lot. Data augmentation is an essential technique to utilize limited amount of training images. In my previous blog post, I have seen poor performance of a deep learning model when testing images contain the translation of the training images. However, the model performance improves when training data also contains translated images. See Assess the robustness of CapsNet . This experiment shows that it is essential to increase the data size using data augmentation to develop a robust deep learning model. Keras's ImageDataGenerator and its limit Data augmentation could increase the number of training images substantially which could raise a storage problem. Keras has a powerful API called ImageDataGenerator that resolve this problem. The generator can generate augmented images from the training images on the fly. This generator has been used in many of my previous blog posts, for example: CNN modeling with image translations using MNIST data Learn about ImageDataGenerator Despite that it is a powerful and popular API, this API is limited to the image classification problem where the target does not depend on the translation of images. For example, the image of a dog is still an image of a dog even if the image is shifted by 3 pixels. So the target label \"dog\" does not need to be translated. In landmark detection or facial keypoint detections, the target values also needs to change when an image is translated. That means that if the image of a face is shifted by 3 pixels, the (x,y) coordinates of the eye location also needs to be shifted. I was looking for some existing API that can translate both images and coordinates. However, I couldn't. In my previous blog post Achieving Top 23% in Kaggle's Facial Keypoints Detection with Keras + Tensorflow , I implemented a python class that can flip the image horizontally and shift the image both along horizontal and vertical axes while adjusting the landmark coordinates. But there are so many other translations that I want to do; e.g., shearing, zooming, or all of them at once! And I do not want to code rotation matrix by myself! Keras's ImageDataGenerator for facial keypoint detection problem. I came up with a rather simple approach that takes full advantage of Keras's ImageDataGenerator. Although this is probably not the most optimized approach, it is very simple and the method allows us to use all Keras's ImageDataGenerator functionalities for landmark detection problem. Simple idea The idea is simple: I will create a mask having the same size as the image. The pixels of the mask corresponding to a landmark is indexed. The original image is augmented with this mask as the 4th channel (assuming that the image has 3 channels). Then we will pass this 4-channel image to Keras's ImagedataGenerator and find where the indexed landmark will be after image translation. The code below shows how I implemented this approach. In [1]: ## Import usual libraries import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras , sys , time , os , warnings import numpy as np import pandas as pd import cv2 warnings . filterwarnings ( \"ignore\" ) os . environ [ \"CUDA_DEVICE_ORDER\" ] = \"PCI_BUS_ID\" config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.025 config . gpu_options . visible_device_list = \"4\" set_session ( tf . Session ( config = config )) print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )); del keras print ( \"tensorflow version {}\" . format ( tf . __version__ )) Using TensorFlow backend. python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.1.3 tensorflow version 1.5.0 Extract a single image with a landmark and bounding box This image is extracted from CVC11 , and I previously analyzed images from this data. See Driver's facial keypoint detection . I will only extract a single image from this data to demonstrate my data augmentation routine. In [2]: from keras.preprocessing.image import img_to_array , load_img dir_data = \"DrivFace/\" ## For this data, we have annotation right eye, left eye, nose, right mouth and left mouth landmarks = [ \"RE\" , \"LE\" , \"N\" , \"RM\" , \"LM\" ] img = img_to_array ( load_img ( dir_data + \"/DrivImages/20130529_01_Driv_001_f .jpg\" )) row_name = [ \"xF\" , \"yF\" , \"wF\" , \"hF\" , \"xRE\" , \"yRE\" , \"xLE\" , \"yLE\" , \"xN\" , \"yN\" , \"xRM\" , \"yRM\" , \"xLM\" , \"yLM\" ] row = [ 272 , # xF: (xF, yF) : top left corner of the bounding box 292 189 , # yF: 209 140 , # wF: width of the bounding box 100 152 , # hF: height of the bounding box 112 323 , # xRE: (xRE,yRE) the (x,y) coordinate of right eye 232 , # yRE: 367 , # xLE: (xLE, yLE) the (x,y) coordinate of left eye 231 , # yLE: 353 , # xN : (xN, yN) the (x,y) coordinate of the nose 254 , # yN : 332 , # xRM: (xRM, yRM) the (x,y) coordinate of the right mouth tip 278 , # yRM: 361 , # xLM: (xLM, yLM) the (x,y) coordinate of the left mouth tip 278 ] # yLM: row = pd . DataFrame ( row ) . T row . columns = row_name row Out[2]: xF yF wF hF xRE yRE xLE yLE xN yN xRM yRM xLM yLM 0 272 189 140 152 323 232 367 231 353 254 332 278 361 278 Let's look at the original image Driver image! In [3]: fig = plt . figure ( figsize = ( 5 , 5 )) ## original image ax = fig . add_subplot ( 1 , 1 , 1 ) ax . imshow ( img / 255.0 ) for landmark in landmarks : ax . scatter ( row [ \"x\" + landmark ], row [ \"y\" + landmark ]) plt . show () Create a function to extract a bounding box In practice, deep learning model often does not take an original wide-view image as an input of landmark detection model. Instead, an image is usually trimmed to reduced the space that focuses to the face by using face detection algorithm. This data already provide a bounding box so we will use this bounding box to reduce the image size. I will demonstrate my data augmentation routine using the trimmed image within bounding box. In [4]: def get_bbox ( row ): ''' extract bounding box from the dataframe ''' faces = ( int ( row [ \"xF\" ]), int ( row [ \"yF\" ]), int ( row [ \"wF\" ]), int ( row [ \"hF\" ])) return ( faces ) ( x , y , w , h ) = get_bbox ( row ) As the recorded (x,y) coordinates of landmarks are with respect to original image, I adjust the landmark coordinates to the bounding box. In [5]: def adjust_loc ( rows , x_topleft = 0 , y_topleft = 0 ): ''' adjust the landmark coordinates with respect to bbox output: ''' out = [] for lm in landmarks : out . append (( int ( rows [ \"x\" + lm ]) - x_topleft , int ( rows [ \"y\" + lm ]) - y_topleft )) return ( out ) landmark_bd = adjust_loc ( row , x_topleft = x , y_topleft = y ) landmark_bd Out[5]: [(51, 43), (95, 42), (81, 65), (60, 89), (89, 89)] The image in bounding box. I will use this single image to demonstrate my data augmentation routine. In [6]: ## image in bounding box img_bd = img [ y :( y + w ), x :( x + w )] fig = plt . figure ( figsize = ( 5 , 5 )) ax = fig . add_subplot ( 1 , 1 , 1 ) ax . imshow ( img_bd / 255.0 ) for ( x , y ) in landmark_bd : ax . scatter ( x , y ) plt . show () Data augmentation Step 1: create a mask that record the location of landmark An image is augmented with a mask as the 4th channel. In [7]: def get_ymask ( img , xys ): ''' img : (N width, N height, N channel) array of image xys : A list containint tuple of (x,y) coordinate od landmark. For example: xys = [(x1,y1), (x2,y2), (x3,y3), (x4,y4), ...] ''' yimg = np . zeros (( img . shape [ 0 ], img . shape [ 1 ], 1 )) yimg [:] = - 1 for iland , ( ix , iy ) in enumerate ( xys ): yimg [ iy , ix ] = iland return ( np . dstack ([ img , yimg ])) yimg = get_ymask ( img_bd , landmark_bd ) print ( \"The dimension of the original image {} -> masked image {}\" . format ( img_bd . shape , yimg . shape )) plt . figure ( figsize = ( 6 , 6 )) plt . imshow ( yimg [:,:, 3 ]) plt . title ( \"The mask receives non-negative values at landmarks\" ) plt . show () The dimension of the original image (140, 140, 3) -> masked image (140, 140, 4) Step 2: define Keras's ImageDataGenerator with the parameter of your choice. In [8]: from keras.preprocessing.image import ImageDataGenerator , img_to_array , load_img datagen = ImageDataGenerator ( rotation_range = 20 , width_shift_range = 10.0 , height_shift_range = 10.0 , ## Float. Shear Intensity (Shear angle in counter-clockwise direction in degrees) shear_range = 5.0 , ## zoom_range: Float or [lower, upper]. ## Range for random zoom. If a float, ## [lower, upper] = [1-zoom_range, 1+zoom_range] zoom_range = [ 0.6 , 1.2 ], fill_mode = 'nearest' , #cval=-2, horizontal_flip = True , vertical_flip = False ) Step 3: Define a class ImageDataGenerator_landmarks The class assume that get_ymask is used before the flow method. After translation of image, you can resize the image via target_shape parameter. Translation with original resolution and then down size resolution gives more sample than translating the down sized images. The class defenition of ImageDataGenerator_landmarks is available from my Github account In [9]: class ImageDataGenerator_landmarks ( object ): def __init__ ( self , datagen , preprocessing_function = lambda x , y : ( x , y ), loc_xRE = None , loc_xLE = None , flip_indicies = None , target_shape = None , ignore_horizontal_flip = True ): ''' datagen : Keras's ImageDataGenerator preprocessing_function : The function that will be implied on each input. The function will run after the image is resized and augmented. The function should take one argument: one image (Numpy tensor with rank 3), and should output a Numpy ignore_horizontal_flip : if False, whether the horizontal flip happend is checked using and if the flipping happens, each pair of the are flipped. if True, then , and do not need to be specified. target_shape : If target_shape is not None, A translated image is resized to target_shape. Why? Translation with original resolution and then down size resolution gives wider range of modified images than translating the down sized images. For example, Suppose the landmarks are - right eye (RE) - left eye (LE) - mouth (M) - right mouth edge (RM) - left mouth edge (LM) then there are 5 x 2 coordinates to predict: xRE, yRE, xLE, yLE, xN, yN, xRM, yRM, xLM, yLM When the horizontal flip happens, RE becomes LE and RM becomes LM. So we need to change the target variables accordingly. If the horizontal flip happenes xRE > xLE so loc_xRE = 0 , loc_yRE = 2 In this case, our filp indicies are: self.flip_indicies = ((0,2), # xRE <-> xLE (1,3), # yRE <-> yLE (6,8), # xRM <-> xLM (7,9)) # yRM <-> yLM ''' self . datagen = datagen self . ignore_horizontal_flip = ignore_horizontal_flip self . target_shape = target_shape # check if x-cord of landmark1 is less than x-cord of landmark2 self . loc_xRE , self . loc_xLE = loc_xRE , loc_xLE self . flip_indicies = flip_indicies ## the chanel that records the mask self . loc_mask = 3 self . preprocessing_function = preprocessing_function def flow ( self , imgs , batch_size = 20 ): ''' imgs: the numpy image array : (batch, height, width, image channels + 1) the channel (self.loc_mask)th channel must contain mask ''' generator = self . datagen . flow ( imgs , batch_size = batch_size ) while 1 : ## N = 0 x_bs , y_bs = [], [] while N < batch_size : yimgs = generator . next () ## yimgs.shape = (bsize,width,height,channels + 1) ## where bsize = np.min(batch_size,x.shape[0]) x_batch , y_batch = self . _keep_only_valid_image ( yimgs ) if len ( x_batch ) == 0 : continue x_batch , y_batch = self . preprocessing_function ( x_batch , y_batch ) x_bs . append ( x_batch ) y_bs . append ( y_batch ) N += x_batch . shape [ 0 ] x_batch , y_batch = np . vstack ( x_bs ), np . vstack ( y_bs ) yield ([ x_batch , y_batch ]) def _keep_only_valid_image ( self , yimg ): ''' Transform the mask to (x,y)-coordiantes. Depending on the translation, landmark may \"dissapeear\". For example, if the image is escessively zoomed in, the mask may lose the index of landmark. Such image translation is discarded. x_train and y_train could be an empty array if landmarks of all the translated images are lost i.e. np.array([]) ''' x_train , y_train = [], [] for irow in range ( yimg . shape [ 0 ]): x = yimg [ irow ,:,:,: self . loc_mask ] ymask = yimg [ irow ,:,:, self . loc_mask ] y = self . _findindex_from_mask ( ymask ) # if some landmarks dissapears, do not use the translated image if y is None : continue x , y = self . _resize_image ( x , y ) x_train . append ( x ) y_train . append ( y ) x_train = np . array ( x_train ) y_train = np . array ( y_train ) return ( x_train , y_train ) def _resize_image ( self , x , y ): ''' this function is useful for down scaling the resolution ''' if self . target_shape is not None : shape_orig = x . shape x = cv2 . resize ( x , self . target_shape [: 2 ]) y = self . adjust_xy ( y , shape_orig , self . target_shape ) return x , y def adjust_xy ( self , y , shape_orig , shape_new ): ''' y : [x1,y1,x2,y2,...] ''' y [ 0 :: 2 ] = y [ 0 :: 2 ] * shape_new [ 1 ] / float ( shape_orig [ 1 ]) y [ 1 :: 2 ] = y [ 1 :: 2 ] * shape_new [ 0 ] / float ( shape_orig [ 0 ]) return y def _findindex_from_mask ( self , ymask ): ''' ymask : a mask of shape (height, width, 1) ''' ys = [] for i in range ( self . Nlandmarks ): ix , iy = np . where ( ymask == i ) if len ( ix ) == 0 : return ( None ) ys . extend ([ np . mean ( iy ), np . mean ( ix )]) ys = np . array ( ys ) ys = self . _adjustLR_horizontal_flip ( ys ) return ( ys ) def _adjustLR_horizontal_flip ( self , ys ): ''' if a horizontal flip happens, right eye becomes left eye and right mouth edge becomes left mouth edge So we need to flip the target cordinates accordingly ''' if self . ignore_horizontal_flip : return ( ys ) if ys [ self . loc_xRE ] > ys [ self . loc_xLE ]: ## True if flip happens # x-cord of RE is less than x-coord of left eye # horizontal flip happened! for a , b in self . flip_indicies : ys [ a ], ys [ b ] = ( ys [ b ], ys [ a ]) return ( ys ) def get_ymask ( self , img , xys ): ''' img : (height, width, channels) array of image xys : A list containint tuple of (x,y) coordinates of landmark. For example: xys = [(x0,y0), (x1,y1), (x2,y2), (x3,y3), (x4,y4), ...] output: mask : A numpy array of size (height, width, channels). All locations without the landmarks are recorded -1 A coordinate with (x0, y0) is recorded as 0 A coordinate with (x1, y1) is recorded as 1 ... ''' yimg = np . zeros (( img . shape [ 0 ], img . shape [ 1 ], 1 )) yimg [:] = - 1 for iland , ( ix , iy ) in enumerate ( xys ): yimg [ iy , ix ] = iland self . Nlandmarks = len ( xys ) self . loc_mask = img . shape [ 2 ] return ( np . dstack ([ img , yimg ])) Instantiate the class To instantiate the class you need to provide datagen : Keras's ImageDataGenerator ignore_horizontal_flip : if False, whether the horizontal flip happened is checked, using and if the horizontal flipping happens, each pair of the are flipped. if True, then , and do not need to be specified loc_xRE: the position where x coordinate of right eye is stored in target variable. loc_xLE: the position where y coordinate of right eye is stored in target variable. flip_indicies: the positions where x, y coordinates aer What are they? Consider our scenario: Our landmarks are: - right eye (RE) - left eye (LE) - mouth (M) - right mouth edge (RM) - left mouth edge (LM) then there are 5 x 2 coordinates to predict: xRE, yRE, xLE, yLE, xN, yN, xRM, yRM, xLM, yLM When the horizontal flip happens, RE becomes LE and RM becomes LM. So we need to change the target variables accordingly. If the horizontal flip happenes, we see xRE > xLE (right eye is on the right of left eye!) In this case, we must flip the role of RE and LE as well as RM and LM. So our filp indicies are: self.flip_indicies = ((0,2), # xRE <-> xLE (1,3), # yRE <-> yLE (6,8), # xRM <-> xLM (7,9)) # yRM <-> yLM In [10]: generator = ImageDataGenerator_landmarks ( datagen , ignore_horizontal_flip = False , target_shape = ( 90 , 90 , 3 ), loc_xRE = 0 , loc_xLE = 2 , flip_indicies = (( 0 , 2 ), # xRE <-> xLE ( 1 , 3 ), # yRE <-> yLE ( 6 , 8 ), # xRM <-> xLM ( 7 , 9 )) # yRM <-> yLM ) xy = np . array ([ generator . get_ymask ( img_bd , landmark_bd )]) Let's visualize! Notice that RE is always at the left of LE. In [11]: plt . close ( 'all' ) def singleplot ( ax , x , y ): ax . imshow ( x / 255.0 ) colors = [ 'b' , 'g' , 'r' , 'c' , 'm' ] for i , marker , c in zip ( range ( 0 , len ( y ), 2 ), landmarks , colors ): ax . annotate ( marker , ( y [ i ], y [ i + 1 ]), color = c ) def pannelplot ( figID = 0 , dir_image = None , nrow_plot = 6 , ncol_plot = 6 , fignm = \"fig\" , save = True ): fig = plt . figure ( figsize = ( 15 , 15 )) #fig.subplots_adjust(hspace=0,wspace=0) xs , ys = [], [] count = 1 for x_train , y_train in generator . flow ( xy , batch_size = 1 ): if len ( x_train ) == 1 : ax = fig . add_subplot ( nrow_plot , ncol_plot , count ) #ax.axis(\"off\") singleplot ( ax , x_train [ 0 ], y_train [ 0 ]) if count == nrow_plot * ncol_plot : break count += 1 if save : plt . savefig ( dir_image + \"/fig{:04.0f}.png\" . format ( figID ), bbox_inches = 'tight' , pad_inches = 0 ) else : plt . show () pannelplot ( save = False ) Make a gif In [12]: def create_gif ( gifname , dir_image , duration = 1 ): import imageio filenames = np . sort ( os . listdir ( dir_image )) filenames = [ fnm for fnm in filenames if \".png\" in fnm ] with imageio . get_writer ( dir_image + '/' + gifname + '.gif' , mode = 'I' , duration = duration ) as writer : for filename in filenames : image = imageio . imread ( dir_image + filename ) writer . append_data ( image ) dir_image = 'data_augmentation/' plt . close ( 'all' ) for count in range ( 100 ): pannelplot ( count , dir_image , nrow_plot = 6 , ncol_plot = 6 , fignm = \"fig\" , save = True ) create_gif ( \"example\" , dir_image , duration = 0.5 ) plt . close ( 'all' ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Data augmentation for facial keypoint detection"},{"url":"Review-on-Gaussian-process.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In this blog post, I would like to review the traditional Gaussian process modeling. This blog was motivated by the blog post Fitting Gaussian Process Models in Python by Christ at Domino which explains the basic of Gaussian process modeling. When I was reading his blog post, I felt that some mathemtatical details are missing. Therefore, I am writing this blog to digest his blog post. In [1]: import numpy as np import matplotlib.pylab as plt import scipy , os from scipy.stats import norm import warnings warnings . filterwarnings ( \"ignore\" ) print \"scipy= {} v, numpy= {} v, matplotlib= {} v\" . format ( scipy . __version__ , np . __version__ , plt . __version__ ) scipy=1.0.0v, numpy=1.14.0v, matplotlib=1.14.0v Gaussian model assumption Say you have a pair of data $(x_i, y_i),i=1,\\cdots,n$ and you want to estimate the future value of $y_m$ for given features $x_m$, Gaussian process may be useful just like any other regression model e.g. linear regression, support vector machine or neural network model etc. One of the unique characteristics of Gaussian model is that it can give you the closed form expression for the distribution of the future target variable $y_m$ given observed data, AKA the posterior predictive distribution. I will get to this point later, but let's formulate the Gaussian process. Let's get notation straight. I use bold symbol to express a vector. $\\boldsymbol{x}_n = [x_1,\\cdots,x_n]$ $\\boldsymbol{y}_n = [y_1,\\cdots,y_n]$ Just like linear regression model, the Gaussian process assumes that your target variable $y_i$ can be modelled to have a mean $\\mu_i$ and some unknown variance $\\sigma&#94;2$. $$ y_i \\overset{ind.}{\\sim} N(\\mu_i, \\sigma&#94;2) $$ We can express the same distribution in vector form using multivariate normal distribution. $$ \\boldsymbol{y}_n \\sim N(\\boldsymbol{\\mu}_n, \\sigma&#94;2 \\boldsymbol{I}) $$ The mean $\\mu_i$ is modeled with a $x_i$. If this were simple linear regression model, we would assume that $\\mu_i$ was modeled as a linear function of $x_i$ and it has unknown weights that linearly link $x_i$ and $\\mu_i$ as $\\mu_i = x_i&#94;T w + b_0$. The unknown weights can be estimated by minimizing mean square error, or in other words, maximizing the likelihood function. In Gaussian process, the mean $\\mu_i$ is modeled with a $x_i$ but functional form of $\\mu_i = f(x_i)$ is not known. It could be a linear function of $x_i$ or cubic function of $x_i$. Gaussian process allows us to keep it general. To model our uncertainty in the form of $\\mu_i=f(x_i)$, Gaussian process assume that $\\mu_i$ is random, and from a multivariate normal with mean $\\boldsymbol{0}$ and covariance matrix $\\boldsymbol{K}(\\boldsymbol{x})$: $$ \\boldsymbol{\\mu}_n | \\boldsymbol{x}_n \\sim N(\\boldsymbol{0}, \\boldsymbol{K}(\\boldsymbol{x}_n)) $$ Kernel for covariance matrix The covariance matrix is modeled with the feature vector $\\boldsymbol{x}$: $$ \\boldsymbol{K}(\\boldsymbol{x}_n) = \\begin{bmatrix} k(x_1,x_1)&\\cdots&\\cdots\\\\ k(x_1,x_2)&k;(x_2,x_2)&\\cdots\\\\ k(x_1,x_3)&k;(x_2,x_3)&k;(x_3,x_3)\\\\ \\vdots\\\\ \\end{bmatrix} $$ Here, dependence between $\\mu_i$ and $\\mu_j$ are modelled with features $x_i$ and $x_j$ through kernel $k$, and I need to give functional form to kernel. For example, I can use the squared exponential covariance function. This kernel is also called as the Radial Basis Function kernel or the Gaussian kernel. $$ k(x,x') = \\theta_1 \\exp \\left( -\\frac{ \\theta_2 }{ 2 } (x-x')&#94;2 \\right) $$ Here, $0 < k(x,x') <\\theta_1$, and this kernel is infinitely differentiable with respect to $\\theta_1$ and $\\theta_2$. In this kernel, $\\theta_1>0$ and $\\theta_2>0$ are parameters. What are their intepretations? When $x = x'$, $k(x,x') = \\theta_1$. So $\\theta_1=Var(y)$, so $\\theta_1$ determines the marginal variability of $y$. If you are standardizing the training data before fitting, it may be reasonable to assume that $\\theta_1=1$. if $\\theta_2$ is large then $k(x,x')$ are very small regardless of the distance between x and x' indicating that the correlation depends only on the distance between x and x'. There are a lot of good discussion of the choice of kernel by David Kristjanson Duvenaud from CS department of University of Tronto . The presentation slides from Tutorial: Gaussian process models for machine learning was very comprehensive. Marginal likelihood That's all assumptions for the Gaussian model. Gaussian model is really just a hiearchical model with two layers of ditribution assumption: $\\boldsymbol{y}_n | \\boldsymbol{\\mu}_n$ and $\\boldsymbol{\\mu}_n|\\boldsymbol{x}_n$. The distribution $\\boldsymbol{y}_n|\\boldsymbol{x}_n$ is called marginal likelihood because it can be obtained by integrating out $\\boldsymbol{\\mu}_n$. In the derivation below, I drop the subscript $n$ to avoid cluttered notations. $$ p(\\boldsymbol{y}|\\boldsymbol{x}) \\\\ %% = \\int p(\\boldsymbol{y},\\boldsymbol{\\mu}|\\boldsymbol{x}) d \\boldsymbol{\\mu} \\textrm{ by total low of probability}\\\\ %% = \\int p(\\boldsymbol{y} | \\boldsymbol{\\mu})p(\\boldsymbol{\\mu}|\\boldsymbol{x})d \\boldsymbol{\\mu} \\textrm{ because $\\boldsymbol{y}$ is independent of $\\boldsymbol{x}$ given $\\boldsymbol{\\mu}$}\\\\ %% \\propto \\int exp \\left( -\\frac{1}{2\\sigma&#94;2}( \\boldsymbol{y} - \\boldsymbol{\\mu})&#94;T ( \\boldsymbol{y} - \\boldsymbol{\\mu}) \\right) exp \\left( -\\frac{1}{2}\\boldsymbol{\\mu}&#94;T \\boldsymbol{K}&#94;{-1} \\boldsymbol{\\mu} \\right) d\\boldsymbol{\\mu}\\\\ %% \\propto exp \\left( -\\frac{1}{2}\\boldsymbol{\\mu}&#94;T \\left(\\boldsymbol{K} + \\sigma&#94;2\\boldsymbol{I})&#94;{-1} \\boldsymbol{\\mu} \\right) \\right) $$ Since the final equation is proportional to the multivariate normal density with mean 0 and variance $\\boldsymbol{K}(\\boldsymbol{x}) + \\sigma&#94;2 \\boldsymbol{I}$, after standardizing the final line to have area under the density = 1, we have: $$ \\boldsymbol{y}_n | \\boldsymbol{x}_n \\sim N (\\boldsymbol{0},\\boldsymbol{K}(\\boldsymbol{x}_n) + \\sigma&#94;2 \\boldsymbol{I}) $$ The parameterers, $\\theta_1$, $\\theta_2$ and $\\sigma&#94;2$ are some unknown parameters that we need to estimate using available data. I will discuss the maximum likelihood procedure later. In [2]: def nlog_pdf_multivariate_gauss ( x , mu , cov ): ''' Caculate the multivariate normal density (pdf) x : n by 1 mu : n by 1 cov: n by n ''' mu = np . array ( mu ) . reshape ( - 1 , 1 ) # [n by 1] x = np . array ( x ) . reshape ( - 1 , 1 ) # [n by 1] term1 = - np . log ( (( 2.0 * np . pi ) ** ( len ( mu ) / 2.0 )) * ( np . linalg . det ( cov ) ** ( 1 / 2.0 )) ) term2 = ( - 1 / 2.0 ) * (( x - mu ) . T . dot ( np . linalg . inv ( cov ))) . dot (( x - mu )) term = float ( term1 + term2 ) return - np . sum ( term ) Let's check the calculation is right for 1 dimentional normal scenario The assert statement return error message if the hand-calculated density does not agree to the negative of the log density from stats.norm. In [3]: cov = np . array ([[ 1.2 ]]) x = np . array ([ 3 ]) mu = np . array ([ 0 ]) nldf = nlog_pdf_multivariate_gauss ( x = x , mu = mu , cov = cov ) print ( \" 1/( {sigma2:} * np.sqrt(2 * np.pi * {sigma2:} )) * np.exp( - ( {x:} - {mu:} )**2 / (2.0 * ( {sigma2:} )) ) = {ldens:} \" . format ( x = x [ 0 ] , mu = mu [ 0 ] , sigma2 = cov [ 0 ][ 0 ], ldens = nldf )) assert np . abs ( nldf + scipy . stats . norm . logpdf ( x = x [ 0 ], scale = np . sqrt ( cov [ 0 ][ 0 ]))) < 1E-10 1/(1.2 * np.sqrt(2 * np.pi * 1.2 )) * np.exp( - (3 - 0)**2 / (2.0 * (1.2)) ) = 4.7600993116 Define the squared exponential covariance function In [4]: from scipy.spatial.distance import pdist from scipy.spatial.distance import squareform def exponential_cov ( eucl_dist , para ): ''' the core of the exponential covariance defenition ''' assert len ( para ) == 2 , len ( para ) return para [ 0 ] * np . exp ( - 0.5 * para [ 1 ] * eucl_dist ** 2 ) def exponential_cov_mat ( x , para ): ''' return matrix ''' eucl_dist = squareform ( pdist ( x )) return ( exponential_cov ( eucl_dist , para )) def exponential_cov_pair ( x1 , x2 , para ): ''' return scalar ''' x1 , x2 = np . array ( x1 ) . flatten (), np . array ( x2 ) . flatten () eucl_dist = np . subtract . outer ( x1 , x2 ) return ( exponential_cov ( eucl_dist , para )) Let's see example values of the squared exponential covariance function Looks fine! In [5]: para = [ 1 , 0.2 ] x1s = [ 1 , 2 , 3 , 5 ] x2s = [ 13 , 10 , 92 , 45 ] for x1 , x2 in zip ( x1s , x2s ): ex = exponential_cov_pair ( x1 , x2 , para ) print ( \" {theta1:} *exp(-0.5* {theta2:} *( {x1:} - {x2:} )**2)= {value:} \" . format ( theta1 = para [ 0 ], theta2 = para [ 1 ], x1 = x1 , x2 = x2 , value = ex )) 1*exp(-0.5*0.2*(1-13)**2)=[[5.57390369e-07]] 1*exp(-0.5*0.2*(2-10)**2)=[[0.00166156]] 1*exp(-0.5*0.2*(3-92)**2)=[[0.]] 1*exp(-0.5*0.2*(5-45)**2)=[[3.25748853e-70]] Posterior predictive distribution of $y_m | \\boldsymbol{y}_n$ So what do we get by assuming Gaussian distribution over $Y_1,Y_2,...$? We can get the distribution for the future $y_m$ given the available data $y_1,...,y_n$. The predictive distribution is the distribution of the target variable given the observed data $y_m |data$ and it is used for inference in Bayesian statistics. Let $n$ be the observed data size (training data size) and consider the scenario where I have $data = \\boldsymbol{y}_{n} \\in R&#94;n$ observed data. Under the model assumption, we have a closed form expression for the posterior distribution of $y_m \\in R&#94;1$ is: $$ \\left. y_m \\right| \\boldsymbol{y}_n \\sim N_1 \\left( \\Sigma_{m,n}\\Sigma_{n,n}&#94;{-1} \\boldsymbol{y}_n, k(x_m,x_m) + \\sigma&#94;2- \\Sigma_{m,n}\\Sigma_{n,n}&#94;{-1} \\Sigma_{m,n}&#94;T \\right) $$ $$ \\Sigma_{m,n} = \\begin{bmatrix} k(x_1,x_m)\\\\ k(x_2,x_m)\\\\ k(x_3,x_m)\\\\ \\vdots\\\\ k(x_n,x_m)\\\\ \\end{bmatrix}&#94;T $$ $$ \\Sigma_{n,n} = \\begin{bmatrix} k(x_1,x_1)+\\sigma&#94;2&\\cdots&\\cdots&\\cdots&k;(x_n,x_1)\\\\ k(x_2,x_1)&k;(x_2,x_2)+\\sigma&#94;2&\\cdots&\\cdots&k;(x_n,x_2)\\\\ k(x_3,x_1)&k;(x_3,x_2)&k;(x_3,x_3)+\\sigma&#94;2&\\cdots&k;(x_3,x_n)\\\\ \\vdots\\\\ k(x_n,x_1)&k;(x_n,x_2)&k;(x_n,x_3)&\\cdots&k;(x_n,x_n)+\\sigma&#94;2\\\\ \\end{bmatrix} $$ Some observations The Gaussian process becomes computationally very expensive to evaluate $E(y_m | \\boldsymbol{y}_n)$, because we need to take an inverse of $\\Sigma_{n,n}$ and this dimension depends on the observed data size. Imagine if $n =$ 1 million, how can we take an inverse of such function ?! Does this mean that the Gaussian process is not very scalable? Posterior predictive mean In Baysian statistics, the conditional mean of $y_m$, $E(y_m | \\boldsymbol{y}_n)$, is called posterior predictive mean. Let's study the property of this posterior predictive mean. Consider the scenario when the training data only have a single $(y_1,x_1)$ so $n=1$, and $(y_m, x_m)$ happened to have $x_1 = x_m$. Then you might expect that the posterior predicitve mean of $y_m$ becomes $y_1$. This is not the case! $$ E(y_m|\\boldsymbol{y}_n) = k(x_1,x_m') (k(x_1,x_1)+\\sigma&#94;2)&#94;{-1} y_1 = \\frac{\\theta_1}{ \\theta_1+\\sigma&#94;2} y_1\\\\ $$ Notice that $E(y_m|\\boldsymbol{y}_n)=y_1$ only if $\\sigma&#94;2=0$. Remind you that $\\sigma&#94;2 = Var(y_i|\\mu_i)$. Nonzero $\\sigma&#94;2$ means that knowledge of $\\mu_i$ is not quite enough to know the value of $y_i$ and this is because $\\mu_i$ is also a rondom quantity. Since this random quantity $\\mu_i$ has a prior mean at 0, your posterior predictive mean of $y_i$ is pulled toward 0 when $\\sigma&#94;2$ is large relative to $\\theta_1$. If you feel that $(y_m,x_m)$ should be the same as $(y_1,x_1)$, when $x_1= x_m$, you are essntially assuming that $\\sigma&#94;2 = 0$ no noise in the training data and $x_1$ can completely explain $y_1$. Your model is reduced to: $ \\boldsymbol{y} | \\boldsymbol{x} \\sim N(\\boldsymbol{0},\\boldsymbol{K})$. Generate toy data We have enough theories and let's look at examples to understand the theories! I will first genrate data $y_1,.....,y_{15}$ from Gaussian distribution. In [6]: import numpy as np np . random . seed ( 1 ) para_true = [ 1 , 0.01 , 0.25 ] ## theta1, theta2, sigma&#94;2 N = 15 means = np . array ([ 0 ] * N ) xmin , xmax = - 100 , 100 observed_x = np . random . uniform ( xmin , xmax , N ) . reshape ( - 1 , 1 ) cov = exponential_cov_mat ( observed_x , para_true [: 2 ]) cov [ range ( cov . shape [ 0 ]), range ( cov . shape [ 0 ])] += para_true [ 2 ] observed_y = np . random . multivariate_normal ( means , cov , 1 ) . reshape ( - 1 , 1 ) observed_data = ( observed_x , observed_y ) I will use scipy.optimizer.minimize for finding the maximum likelihood estimates. In pratice, we do not need to define Gaussian likelihood by hand to find the MLE for the model. For example, one can use However, for the sake of understanding I will use the optimizer routine and directly minimize the negative of log-likelihood to obtain MLE. In [7]: from scipy.stats import multivariate_normal as mn from scipy.optimize import minimize def nllk_gaussian ( para , y , x , verbose = False , kernel_mat = exponential_cov_mat , gauss_mean = 0 ): ''' calculate negative log likelihood of Gaussian model para = [log(theta2),log(sigma&#94;2)] ''' if ~ hasattr ( gauss_mean , \"__len__\" ): ## is this scalar ? gauss_mean = [ gauss_mean ] * len ( y ) para = np . array ([ 0 ] + list ( para )) para = np . exp ( para ) assert len ( para ) == 3 assert len ( y ) == len ( x ) assert len ( y ) == len ( gauss_mean ) cov = kernel_mat ( x . reshape ( - 1 , 1 ), para [: 2 ]) cov [ range ( cov . shape [ 0 ]), range ( cov . shape [ 1 ])] += para [ 2 ] ## add sigma&#94;2 for nllk = nlog_pdf_multivariate_gauss ( y , gauss_mean , cov ) if verbose : print ( \"theta1 = {:5.3f} , theta2 = {:5.3f} , sigma&#94;2= {:5.3f} -llk = {:6.5f} \" . format ( para [ 0 ], para [ 1 ], para [ 2 ], nllk )) if ~ np . isfinite ( nllk ): nllk = np . inf return ( nllk ) para0 = [ np . log ( 1.0 ), np . log ( 1 )] ## theta2 and sigma&#94;2 res = minimize ( fun = nllk_gaussian , x0 = para0 , args = ( observed_y , observed_x , True ), #method='BFGS', options = { 'disp' : True , 'maxiter' : 100 } ) para = [ 1 ] + list ( np . exp ( res [ \"x\" ])) print ( \"The maximume likelihood estimates of theta1 = {:5.4f} , theta2 = {:5.3f} , sigma&#94;2 = {:5.3f} \" . format ( para [ 0 ], para [ 1 ], para [ 2 ])) theta1 = 1.000, theta2 = 1.000, sigma&#94;2=1.000 -llk = 23.64354 theta1 = 1.000, theta2 = 1.000, sigma&#94;2=1.000 -llk = 23.64354 theta1 = 1.000, theta2 = 1.000, sigma&#94;2=1.000 -llk = 23.64354 theta1 = 1.000, theta2 = 1.000, sigma&#94;2=1.000 -llk = 23.64354 theta1 = 1.000, theta2 = 0.982, sigma&#94;2=0.364 -llk = 22.91802 theta1 = 1.000, theta2 = 0.982, sigma&#94;2=0.364 -llk = 22.91802 theta1 = 1.000, theta2 = 0.982, sigma&#94;2=0.364 -llk = 22.91802 theta1 = 1.000, theta2 = 0.982, sigma&#94;2=0.364 -llk = 22.91802 theta1 = 1.000, theta2 = 0.935, sigma&#94;2=0.317 -llk = 22.89801 theta1 = 1.000, theta2 = 0.935, sigma&#94;2=0.317 -llk = 22.89801 theta1 = 1.000, theta2 = 0.935, sigma&#94;2=0.317 -llk = 22.89801 theta1 = 1.000, theta2 = 0.935, sigma&#94;2=0.317 -llk = 22.89801 theta1 = 1.000, theta2 = 0.836, sigma&#94;2=0.272 -llk = 22.88609 theta1 = 1.000, theta2 = 0.836, sigma&#94;2=0.272 -llk = 22.88609 theta1 = 1.000, theta2 = 0.836, sigma&#94;2=0.272 -llk = 22.88609 theta1 = 1.000, theta2 = 0.836, sigma&#94;2=0.272 -llk = 22.88609 theta1 = 1.000, theta2 = 0.702, sigma&#94;2=0.244 -llk = 22.87374 theta1 = 1.000, theta2 = 0.702, sigma&#94;2=0.244 -llk = 22.87374 theta1 = 1.000, theta2 = 0.702, sigma&#94;2=0.244 -llk = 22.87374 theta1 = 1.000, theta2 = 0.702, sigma&#94;2=0.244 -llk = 22.87374 theta1 = 1.000, theta2 = 0.349, sigma&#94;2=0.158 -llk = 22.66653 theta1 = 1.000, theta2 = 0.349, sigma&#94;2=0.158 -llk = 22.66653 theta1 = 1.000, theta2 = 0.349, sigma&#94;2=0.158 -llk = 22.66653 theta1 = 1.000, theta2 = 0.349, sigma&#94;2=0.158 -llk = 22.66653 theta1 = 1.000, theta2 = 0.021, sigma&#94;2=0.028 -llk = 22.93146 theta1 = 1.000, theta2 = 0.021, sigma&#94;2=0.028 -llk = 22.93146 theta1 = 1.000, theta2 = 0.021, sigma&#94;2=0.028 -llk = 22.93146 theta1 = 1.000, theta2 = 0.021, sigma&#94;2=0.028 -llk = 22.93146 theta1 = 1.000, theta2 = 0.081, sigma&#94;2=0.064 -llk = 21.69681 theta1 = 1.000, theta2 = 0.081, sigma&#94;2=0.064 -llk = 21.69681 theta1 = 1.000, theta2 = 0.081, sigma&#94;2=0.064 -llk = 21.69681 theta1 = 1.000, theta2 = 0.081, sigma&#94;2=0.064 -llk = 21.69681 theta1 = 1.000, theta2 = 0.068, sigma&#94;2=0.057 -llk = 21.67129 theta1 = 1.000, theta2 = 0.068, sigma&#94;2=0.057 -llk = 21.67129 theta1 = 1.000, theta2 = 0.068, sigma&#94;2=0.057 -llk = 21.67129 theta1 = 1.000, theta2 = 0.068, sigma&#94;2=0.057 -llk = 21.67129 theta1 = 1.000, theta2 = 0.000, sigma&#94;2=0.000 -llk = 4070932991425.09766 theta1 = 1.000, theta2 = 0.000, sigma&#94;2=0.000 -llk = 4070932991425.09766 theta1 = 1.000, theta2 = 0.000, sigma&#94;2=0.000 -llk = 4070932991425.09766 theta1 = 1.000, theta2 = 0.000, sigma&#94;2=0.000 -llk = 4070932991425.09766 theta1 = 1.000, theta2 = 0.068, sigma&#94;2=0.057 -llk = 21.67129 theta1 = 1.000, theta2 = 0.068, sigma&#94;2=0.057 -llk = 21.67129 theta1 = 1.000, theta2 = 0.068, sigma&#94;2=0.057 -llk = 21.67129 theta1 = 1.000, theta2 = 0.068, sigma&#94;2=0.057 -llk = 21.67129 theta1 = 1.000, theta2 = 0.068, sigma&#94;2=0.057 -llk = 21.67129 theta1 = 1.000, theta2 = 0.068, sigma&#94;2=0.057 -llk = 21.67129 theta1 = 1.000, theta2 = 0.068, sigma&#94;2=0.057 -llk = 21.67129 theta1 = 1.000, theta2 = 0.068, sigma&#94;2=0.057 -llk = 21.67129 theta1 = 1.000, theta2 = 0.000, sigma&#94;2=0.000 -llk = 25540347.18033 theta1 = 1.000, theta2 = 0.000, sigma&#94;2=0.000 -llk = 25540347.18033 theta1 = 1.000, theta2 = 0.000, sigma&#94;2=0.000 -llk = 25540347.18440 theta1 = 1.000, theta2 = 0.000, sigma&#94;2=0.000 -llk = 25540346.80619 theta1 = 1.000, theta2 = 0.000, sigma&#94;2=0.002 -llk = 2788.20045 theta1 = 1.000, theta2 = 0.000, sigma&#94;2=0.002 -llk = 2788.20045 theta1 = 1.000, theta2 = 0.000, sigma&#94;2=0.002 -llk = 2788.20043 theta1 = 1.000, theta2 = 0.000, sigma&#94;2=0.002 -llk = 2788.20042 theta1 = 1.000, theta2 = 0.014, sigma&#94;2=0.022 -llk = 24.86537 theta1 = 1.000, theta2 = 0.014, sigma&#94;2=0.022 -llk = 24.86537 theta1 = 1.000, theta2 = 0.014, sigma&#94;2=0.022 -llk = 24.86537 theta1 = 1.000, theta2 = 0.014, sigma&#94;2=0.022 -llk = 24.86537 theta1 = 1.000, theta2 = 0.054, sigma&#94;2=0.050 -llk = 21.69128 theta1 = 1.000, theta2 = 0.054, sigma&#94;2=0.050 -llk = 21.69128 theta1 = 1.000, theta2 = 0.054, sigma&#94;2=0.050 -llk = 21.69128 theta1 = 1.000, theta2 = 0.054, sigma&#94;2=0.050 -llk = 21.69128 theta1 = 1.000, theta2 = 0.065, sigma&#94;2=0.056 -llk = 21.66965 theta1 = 1.000, theta2 = 0.065, sigma&#94;2=0.056 -llk = 21.66965 theta1 = 1.000, theta2 = 0.065, sigma&#94;2=0.056 -llk = 21.66965 theta1 = 1.000, theta2 = 0.065, sigma&#94;2=0.056 -llk = 21.66965 theta1 = 1.000, theta2 = 0.065, sigma&#94;2=0.056 -llk = 21.66634 theta1 = 1.000, theta2 = 0.065, sigma&#94;2=0.056 -llk = 21.66634 theta1 = 1.000, theta2 = 0.065, sigma&#94;2=0.056 -llk = 21.66634 theta1 = 1.000, theta2 = 0.065, sigma&#94;2=0.056 -llk = 21.66634 theta1 = 1.000, theta2 = 0.064, sigma&#94;2=0.056 -llk = 21.65312 theta1 = 1.000, theta2 = 0.064, sigma&#94;2=0.056 -llk = 21.65312 theta1 = 1.000, theta2 = 0.064, sigma&#94;2=0.056 -llk = 21.65312 theta1 = 1.000, theta2 = 0.064, sigma&#94;2=0.056 -llk = 21.65312 theta1 = 1.000, theta2 = 0.061, sigma&#94;2=0.057 -llk = 21.60055 theta1 = 1.000, theta2 = 0.061, sigma&#94;2=0.057 -llk = 21.60055 theta1 = 1.000, theta2 = 0.061, sigma&#94;2=0.057 -llk = 21.60055 theta1 = 1.000, theta2 = 0.061, sigma&#94;2=0.057 -llk = 21.60055 theta1 = 1.000, theta2 = 0.050, sigma&#94;2=0.061 -llk = 21.39528 theta1 = 1.000, theta2 = 0.050, sigma&#94;2=0.061 -llk = 21.39528 theta1 = 1.000, theta2 = 0.050, sigma&#94;2=0.061 -llk = 21.39528 theta1 = 1.000, theta2 = 0.050, sigma&#94;2=0.061 -llk = 21.39528 theta1 = 1.000, theta2 = 0.022, sigma&#94;2=0.082 -llk = 20.68440 theta1 = 1.000, theta2 = 0.022, sigma&#94;2=0.082 -llk = 20.68440 theta1 = 1.000, theta2 = 0.022, sigma&#94;2=0.082 -llk = 20.68440 theta1 = 1.000, theta2 = 0.022, sigma&#94;2=0.082 -llk = 20.68440 theta1 = 1.000, theta2 = 0.004, sigma&#94;2=0.333 -llk = 19.45677 theta1 = 1.000, theta2 = 0.004, sigma&#94;2=0.333 -llk = 19.45677 theta1 = 1.000, theta2 = 0.004, sigma&#94;2=0.333 -llk = 19.45677 theta1 = 1.000, theta2 = 0.004, sigma&#94;2=0.333 -llk = 19.45677 theta1 = 1.000, theta2 = 0.008, sigma&#94;2=0.203 -llk = 19.59351 theta1 = 1.000, theta2 = 0.008, sigma&#94;2=0.203 -llk = 19.59351 theta1 = 1.000, theta2 = 0.008, sigma&#94;2=0.203 -llk = 19.59351 theta1 = 1.000, theta2 = 0.008, sigma&#94;2=0.203 -llk = 19.59351 theta1 = 1.000, theta2 = 0.005, sigma&#94;2=0.283 -llk = 19.38598 theta1 = 1.000, theta2 = 0.005, sigma&#94;2=0.283 -llk = 19.38598 theta1 = 1.000, theta2 = 0.005, sigma&#94;2=0.283 -llk = 19.38598 theta1 = 1.000, theta2 = 0.005, sigma&#94;2=0.283 -llk = 19.38598 theta1 = 1.000, theta2 = 0.004, sigma&#94;2=0.256 -llk = 19.38758 theta1 = 1.000, theta2 = 0.004, sigma&#94;2=0.256 -llk = 19.38758 theta1 = 1.000, theta2 = 0.004, sigma&#94;2=0.256 -llk = 19.38758 theta1 = 1.000, theta2 = 0.004, sigma&#94;2=0.256 -llk = 19.38758 theta1 = 1.000, theta2 = 0.005, sigma&#94;2=0.270 -llk = 19.37982 theta1 = 1.000, theta2 = 0.005, sigma&#94;2=0.270 -llk = 19.37982 theta1 = 1.000, theta2 = 0.005, sigma&#94;2=0.270 -llk = 19.37982 theta1 = 1.000, theta2 = 0.005, sigma&#94;2=0.270 -llk = 19.37982 theta1 = 1.000, theta2 = 0.005, sigma&#94;2=0.270 -llk = 19.37981 theta1 = 1.000, theta2 = 0.005, sigma&#94;2=0.270 -llk = 19.37981 theta1 = 1.000, theta2 = 0.005, sigma&#94;2=0.270 -llk = 19.37981 theta1 = 1.000, theta2 = 0.005, sigma&#94;2=0.270 -llk = 19.37981 theta1 = 1.000, theta2 = 0.005, sigma&#94;2=0.270 -llk = 19.37981 theta1 = 1.000, theta2 = 0.005, sigma&#94;2=0.270 -llk = 19.37981 theta1 = 1.000, theta2 = 0.005, sigma&#94;2=0.270 -llk = 19.37981 theta1 = 1.000, theta2 = 0.005, sigma&#94;2=0.270 -llk = 19.37981 Optimization terminated successfully. Current function value: 19.379811 Iterations: 11 Function evaluations: 116 Gradient evaluations: 29 The maximume likelihood estimates of theta1 = 1.0000, theta2 = 0.005, sigma&#94;2 = 0.270 The negative log likelihood of the multivariate normal distribution evaluated at the true parameters Because of the sampling error, the maximum likelihood estimates (MLE) are not completely the same as the true parameters. In fact, the lowest negative likelihood values evaluated at the true parameters are not the smallest. In theory, the sampling error will average out and when the sample size goes to infinity, the MLE should converge to the true parameter values. In [8]: nllk_gaussian ( np . log ( para_true [ 1 :]), y = observed_y , x = observed_x , verbose = True ) theta1 = 1.000, theta2 = 0.010, sigma&#94;2=0.250 -llk = 19.67072 Out[8]: 19.67071970911342 Using the MLE, let's illustrate the model's performance. In [9]: def predict ( x , kernel_pair , para , observed_sigma , observed_data ): ''' calcualte the expected value of y | x, data m = 1 as this function predict E(y|x,data) for a scalar y x must be scalar para = [theta1, theta2, sigma&#94;2] ''' if hasattr ( x , \"__len__\" ): x = x [ 0 ] ( observed_x , observed_y ) = observed_data observed_y = np . array ( observed_y ) . reshape ( - 1 , 1 ) ## [n by 1] Smn = kernel_pair ( x , observed_x , para [: 2 ]) ## [m by n] Sinv = np . linalg . inv ( observed_sigma ) ## [n by n] y_pred = np . dot ( Smn , Sinv ) . dot ( observed_y ) ## [m by n] [n by n] by [n by 1] = [m by 1] ## variance sigma_new = kernel_pair ( x , x , para [: 2 ]) + para [ 2 ] - np . dot ( Smn , Sinv ) . dot ( Smn . T ) return y_pred , sigma_new def get_predictive_y ( newxs , kernel_pair , para , observed_data ): ''' evaluate the estimated E(y_i|x_i,data) and SD(y_i|x_i,data) at every i = 1, 2, 3,... in newx ''' ( observed_x , observed_y ) = observed_data Snn = exponential_cov_mat ( observed_x , para [: 2 ]) Snn [ range ( Snn . shape [ 0 ]), range ( Snn . shape [ 1 ])] += para [ 2 ] y_preds , y_preds_sigma = [], [] for newx in newxs : pr = predict ( newx , kernel_pair , para , Snn , observed_data ) y_preds . append ( pr [ 0 ]) y_preds_sigma . append ( pr [ 1 ]) return ( y_preds , y_preds_sigma ) Visualization of Gaussian process In [10]: ## create a function for plotting def plotGauss ( ax , newxs , y_preds , y_preds_sigma , observed_data , title = \"noise model\" ): ( ox , oy ) = observed_data ax . errorbar ( newxs , y_preds , yerr = np . sqrt ( y_preds_sigma ), capsize = 0 ) count = 0 for x , y in zip ( ox , oy ): if count == 0 : ax . plot ( x , y , \"ro\" , label = \"observed x,y\" ) count += 1 else : ax . plot ( x , y , \"ro\" ) ax . set_ylim ( - 3 , 3 ) ax . set_xlabel ( \"x\" , fontsize = 30 ) ax . set_ylabel ( \"y\" , fontsize = 30 ) ax . set_title ( title + \" training data size: n= {} \" . format ( len ( ox )), fontsize = 30 ) ax . legend ( fontsize = 30 ) In [11]: dir_image = \"./GP_images/\" from copy import copy kernel_pair = exponential_cov_pair newxs = np . linspace ( xmin * 1.5 , xmax * 1.5 , 500 ) para_no_noise = copy ( para ) para_no_noise [ 2 ] = 0 for n in range ( 2 , len ( observed_x ), 1 ): observed_data = ( observed_x [: n ], observed_y [: n ]) y_preds , y_preds_sigma = get_predictive_y ( newxs , kernel_pair , para , observed_data ) y_preds0 , y_preds_sigma0 = get_predictive_y ( newxs , kernel_pair , para_no_noise , observed_data ) fig = plt . figure ( figsize = ( 20 , 10 )) fig . subplots_adjust ( hspace = 0 , wspace = 0 ) ax = fig . add_subplot ( 2 , 1 , 1 ) plotGauss ( ax , newxs , y_preds , y_preds_sigma , observed_data , title = \"GP regrssion with GP noise\" ) ax = fig . add_subplot ( 2 , 1 , 2 ) plotGauss ( ax , newxs , y_preds0 , y_preds_sigma0 , observed_data , title = \"GP regression without noise (sigma2=0)\" ) plt . savefig ( dir_image + \"/fig {:04.0f} .png\" . format ( n ), bbox_inches = 'tight' , pad_inches = 0 ) if n % 5 == 4 : plt . show () plt . close ( 'all' ) In [12]: def create_gif ( gifname , dir_image , duration = 1 ): import imageio filenames = np . sort ( os . listdir ( dir_image )) filenames = [ fnm for fnm in filenames if \".png\" in fnm ] with imageio . get_writer ( dir_image + '/' + gifname + '.gif' , mode = 'I' , duration = duration ) as writer : for filename in filenames : image = imageio . imread ( dir_image + filename ) writer . append_data ( image ) create_gif ( \"GP\" , dir_image , duration = 0.5 ) Use existing scikit learn routine for Gussian process fitting In practice, people do not write their own likelihood to find parameters. For python user, scikit learn has nice Gaussian regrssion model API ! Let's try to fit the same model using scikit learn Gaussian modeling API. In [13]: from sklearn.gaussian_process import GaussianProcessRegressor This Gaussian process regressor takes one argument, kernel. This kernel defines the covariance structure for the marginal likelihood $\\boldsymbol{y}_n|\\boldsymbol{x}_n$. Remind you that the covariance for our marginal liklihood is: $$ Var(\\boldsymbol{y}_n | \\boldsymbol{x}_n) = \\boldsymbol{K}(\\boldsymbol{x}_n) + \\boldsymbol{I}\\sigma&#94;2 $$ That is, the diagonal entires are: $$ Var(y_i | x_i) = \\theta_1 + \\sigma&#94;2 $$ The off-diagonal entries are: $$ Cov(y_i,y_j | x_i,x_j) = \\theta_1 exp\\left( -\\frac{1}{2} \\theta_2 \\left|\\left| x_i - x_j \\right|\\right|&#94;2 \\right) $$ where $i\\ne j$. In my python script parameters are saved in para object as: $$ \\texttt{para}=[\\theta_1, \\theta_2, \\sigma&#94;2] $$ In order to define this kernel in scikit learn API, we need to use following 3 modules. In [14]: from sklearn.gaussian_process.kernels import RBF , ConstantKernel , WhiteKernel The three kernels are defined as: RBF kernel: $$ k_{\\textrm{RBF}}(x_i, x_j) = exp \\left( -\\frac{1}{2} \\left|\\left| \\frac{x_i}{\\textrm{length_scale}} - \\frac{x_j}{\\textrm{length_scale}}\\right|\\right|&#94;2 \\right)\\\\ = exp \\left( -\\frac{1}{2\\textrm{length_scale}&#94;2} \\left|\\left| x_i -x_j\\right|\\right|&#94;2 \\right) $$ Constant kernel: $$ k_{\\textrm{constant}}(x_i, x_j) = \\theta $$ White-noise kernel: $$ k_{\\textrm{white}}(x_i, x_j) = \\sigma&#94;2 \\textrm{ if $x_i = x_j$ else 0} $$ So by adding and multipling these kernels, I can produce my kernel as: $$ k(x_i, x_j) = k_{\\textrm{constant}}(x_i, x_j) \\textrm{ x } k_{\\textrm{RBF}}(x_i, x_j) = \\theta exp \\left( -\\frac{1}{2\\textrm{length_scale}&#94;2} \\left|\\left| x_i -x_j\\right|\\right|&#94;2 \\right) $$ The parameters in scikit learn API and the parameters in prevoius script are linked as: $\\texttt{para_true[0]} = \\theta $ $\\texttt{para_true[1]} = \\frac{1}{\\textrm{length_scale}&#94;2}$ $\\texttt{para_true[2]} = \\sigma&#94;2$ Let's fit the two Gaussian process model as before: one, assuming noise in data and another without noise. In [15]: ## specify the range of parameters kernel_with_noise = ( ConstantKernel ( constant_value_bounds = ( 1e-2 , 2 )) * RBF ( length_scale_bounds = ( 5 , 100.0 )) + WhiteKernel ( noise_level = 1e-5 , noise_level_bounds = ( 1e-10 , 1 ))) kernel0 = ( ConstantKernel ( constant_value_bounds = ( 1e-2 , 2 )) * RBF ( length_scale_bounds = ( 5 , 100.0 ))) gp_noise = GaussianProcessRegressor ( kernel = kernel_with_noise , n_restarts_optimizer = 10 ) gp0 = GaussianProcessRegressor ( kernel = kernel0 , n_restarts_optimizer = 10 ) gps = [ gp_noise , gp0 ] Fit the 2 Gaussian models to same observed_data and plot the results Observation: the results of predicted distribution plots are more or less similar to the plots with MLEs previously obtaiend. In [16]: dir_image = \"./GP_images_scikit/\" from copy import copy kernel_pair = exponential_cov_pair newxs = np . linspace ( xmin * 1.5 , xmax * 1.5 , 500 ) para_no_noise = copy ( para ) para_no_noise [ 2 ] = 0 for n in range ( 2 , len ( observed_x ), 1 ): fig = plt . figure ( figsize = ( 20 , 10 )) fig . subplots_adjust ( hspace = 0 , wspace = 0 ) observed_data = ( observed_x [: n ], observed_y [: n ]) for count , gp in enumerate ( gps , 1 ): ax = fig . add_subplot ( len ( gps ), 1 , count ) gp . fit ( * observed_data ) y_preds , y_preds_sigma = gp . predict ( newxs . reshape ( - 1 , 1 ), return_std = True ) plotGauss ( ax , newxs , y_preds , y_preds_sigma , observed_data , title = \"GP regrssion with GP noise\" ) if n % 5 == 1 : if count == 1 : print ( \"---- \\n Estimated parameters of Gaussian model:\" ) print ( gp . kernel_ ) plt . savefig ( dir_image + \"/fig {:04.0f} .png\" . format ( n ), bbox_inches = 'tight' , pad_inches = 0 ) if n % 5 == 4 : plt . show () plt . close ( 'all' ) create_gif ( \"GP\" , dir_image , duration = 0.5 ) ---- Estimated parameters of Gaussian model: 1.09**2 * RBF(length_scale=16.5) + WhiteKernel(noise_level=9.94e-09) 1.09**2 * RBF(length_scale=16.5) ---- Estimated parameters of Gaussian model: 0.995**2 * RBF(length_scale=16.4) + WhiteKernel(noise_level=0.169) 1.41**2 * RBF(length_scale=5) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Review on Gaussian process"},{"url":"Measure-the-uncertainty-in-deep-learning-models-using-dropout.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } Seminal blog post of Yarin Gal from Cambridge machine learning group What my deep model doesn't know... motivated me to learn how Dropout can be used to describe the uncertainty in my deep learning model. This blog post is dedicated to learn how to use Dropout to measure the uncertainty using Keras framework. In [1]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import sys , time , os , warnings import numpy as np import pandas as pd from collections import Counter warnings . filterwarnings ( \"ignore\" ) print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )); del keras print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.5 config . gpu_options . visible_device_list = \"4\" set_session ( tf . Session ( config = config )) Using TensorFlow backend. python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.1.3 tensorflow version 1.5.0 Review Dropout First, I will review how Keras defines the dropout by creating very simple NN architecture with dropout. Then, I will make sure that the output of hand-made NN agrees to the output of Keras's NN with dropout. For simplicity, I create two-layer feed-forward NN with dropout by hand (i.e., 1 hidden layer and 1 output layer.). The input and output dimentions are only 1 and the NN contains only one hidden layer with two hidden units. This model only contains 7 parameters (See below). The hidden layer is followed by the dropout with dropout rate 0.7. Dropout in Keras Dropout in Keras with tensorflow backend is defined here and it essentially calls for tf.nn.dropout . The documentation of tf.nn.dropout says: Computes dropout. With probability keep_prob, outputs the input element scaled up by 1 / keep_prob, otherwise outputs 0. The scaling is so that the expected sum is unchanged. By default, each element is kept or dropped independently. Therefore, you can think of the dropout as the scaled random vector from Bernoulli distribution with dropout rate as the success probability. The random vector is scaled by 1/(1 - dropout rate). The length of the random vector is the same as the number of hidden units on the hidden layer prior to the dropout layer. If the number of hidden units is 2 in the prior layer, then the random vector of the dropout is defined as: $$ \\boldsymbol{I}=[I_1,I_2]\\\\ I_k \\overset{i.i.d.}{\\sim} \\textrm{Ber}(p_{\\textrm{dropout}})\\\\ k = 1,2 $$ where $*$ is an element wise multiplication. The 2 layer NN definition Formally I can define my simple NN with dropout in two line as: $$ \\boldsymbol{h}_1 = \\textrm{relu}( x \\boldsymbol{W}_1 + \\boldsymbol{b}_1)\\\\ y = (\\boldsymbol{h}_1 * \\boldsymbol{I})\\boldsymbol{W}_2 + \\boldsymbol{b}_2 $$ where: Input: 1x1 matrix: $x \\in R&#94;{1x1}$ output: 1x1 matrix: $y \\in R&#94;{1x1}$ Parameters: $ \\boldsymbol{W}_1 \\in R&#94;{1 x 2}, \\boldsymbol{b}_1 \\in R&#94;{1 x 2} \\\\ \\boldsymbol{W}_2 \\in R&#94;{2 x 1}, \\boldsymbol{b}_1 \\in R&#94;{1 x 1} $ Notice that bias does not get dropped out. Assign values to parameters We will provide values to the weights and biases as follows: In [2]: import numpy as np w1 , b1 = np . array ([ 3 , 7 ]) . reshape ( 1 , 2 ), np . array ([ 5 , 1 ]) . reshape ( 2 ) w2 , b2 = np . array ([ - 10 , 10 ]) . reshape ( 2 , 1 ), np . array ([ 11 ]) . reshape ( 1 ) print ( \"w1={}, \\n b1={}, \\n w2={}, \\n b2={}\" . format ( w1 , b1 , w2 , b2 )) weights = (( w1 , b1 ), ( w2 , b2 )) w1=[[3 7]], b1=[5 1], w2=[[-10] [ 10]], b2=[11] Define the 2 layer NN with dropout in python. In [3]: def relu ( x ): x [ x < 0 ] = 0 return ( x ) def NN2layer_dropout ( x , weights , p_dropout = 0.1 ): ## rate: float between 0 and 1. Fraction of the input units to drop. ( w1 , b1 ), ( w2 , b2 ) = weights ## dropout keep_prob = 1 - p_dropout Nh1 = w2 . shape [ 0 ] Ikeep = np . random . binomial ( 1 , keep_prob , size = Nh1 ) I = np . zeros (( 1 , Nh1 )) I [:, Ikeep == 1 ] = 1 / keep_prob ## 2-layer NN with Dropout h1 = relu ( np . dot ( x , w1 ) + b1 ) y = np . dot ( h1 * I , w2 ) + b2 return ( y [ 0 , 0 ]) Define 1-d input. Here I only consider a single sample. x is defined as: In [4]: x = np . array ([ 2 ]) . reshape ( 1 , 1 ) Using the defined NN, I will predict the y value corresponding to this x using dropout. AS we define the dropout rate to be 0.7, I expect there are four possible predicted value for this x with the distribution as: $\\boldsymbol{I}$ probability $y$ value [0, 0] 0.3 x 0.3=0.09 144 [1, 0] 0.7 x 0.3=0.21 -355 [0, 1] 0.3 x 0.7=0.21 511 [1, 1] 0.7 x 0.7=0.49 11 The script below generate predicted value of y 1,000 times, and plot its histogram. The distribution of predicted y more or less agree to the table above. In [5]: from collections import Counter , OrderedDict def plot_histogram ( ys , ax , title = \"\" ): counter = Counter ( ys ) counter = OrderedDict ( sorted ( counter . items ())) probs = np . array ( counter . values ()) * 100.0 / np . sum ( counter . values ()) keys = np . array ( counter . keys ()) print ( title ) for k , p in zip ( keys , probs ): print ( \"{:8.2f}: {:5.2f}%\" . format ( k , p )) n , bins , rect = ax . hist ( ys ) ax . set_title ( title ) ax . grid ( True ) Nsim = 1000 p_dropout = 0.7 ys_handmade = [] for _ in range ( Nsim ): ys_handmade . append ( NN2layer_dropout ( x , weights , p_dropout )) fig = plt . figure ( figsize = ( 6 , 2 )) ax = fig . add_subplot ( 1 , 1 , 1 ) plot_histogram ( ys_handmade , ax , \"1000 Dropout outputs from hand-made NN\" ) plt . show () 1000 Dropout outputs from hand-made NN -355.67: 20.80% 11.00: 49.50% 144.33: 9.00% 511.00: 20.70% Keras's dropout defenition I will check whether my understanding is correct by creating the same architecture with Keras. In [6]: import keras.backend as K from keras.models import Model from keras.layers import Input , Dense , Dropout Nfeat = 1 tinput = Input ( shape = ( Nfeat ,), name = \"ts_input\" ) h = Dense ( 2 , activation = 'relu' , name = \"dense1\" )( tinput ) hout = Dropout ( p_dropout )( h ) out = Dense ( 1 , activation = \"linear\" , name = \"dense2\" )( hout ) model = Model ( inputs = [ tinput ], outputs = out ) model . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= ts_input (InputLayer) (None, 1) 0 _________________________________________________________________ dense1 (Dense) (None, 2) 4 _________________________________________________________________ dropout_1 (Dropout) (None, 2) 0 _________________________________________________________________ dense2 (Dense) (None, 1) 3 ================================================================= Total params: 7 Trainable params: 7 Non-trainable params: 0 _________________________________________________________________ Manually set weight values in the Keras's model In [7]: for layer in model . layers : if layer . name == \"dense1\" : layer . set_weights (( w1 , b1 )) if layer . name == \"dense2\" : layer . set_weights (( w2 , b2 )) Define a class that do model prediction with dropoff. Keras's prediction function switches off the dropout during prediciton by default. In order to use dropout during prediction, we need to use Keras's backend function. A very clean code is provided as this stack overflow question's answer by Marcin MoÅ¼ejko . I will wrap his code and create a class. In [8]: class KerasDropoutPrediction ( object ): def __init__ ( self , model ): self . f = K . function ( [ model . layers [ 0 ] . input , K . learning_phase ()], [ model . layers [ - 1 ] . output ]) def predict ( self , x , n_iter = 10 ): result = [] for _ in range ( n_iter ): result . append ( self . f ([ x , 1 ])) result = np . array ( result ) . reshape ( n_iter , len ( x )) . T return result Predict y with keras's dropout 1000 times In [9]: kdp = KerasDropoutPrediction ( model ) result = kdp . predict ( x , Nsim ) ys_keras = result . flatten () Compare the predicted y from hand-made NN and Keras's NN in terms of the distribution of output y. Both seems to be comparable, indicating that I correctly implemented Keras's dropout function. In [10]: fig = plt . figure ( figsize = ( 12 , 2 )) ax = fig . add_subplot ( 1 , 2 , 1 ) plot_histogram ( ys_handmade , ax , \"1000 Dropout outputs from hand-made NN\" ) ax = fig . add_subplot ( 1 , 2 , 2 ) plot_histogram ( ys_keras , ax , \"1000 Dropout outputs from Keras's NN\" ) plt . show () 1000 Dropout outputs from hand-made NN -355.67: 20.80% 11.00: 49.50% 144.33: 9.00% 511.00: 20.70% 1000 Dropout outputs from Keras's NN -355.67: 19.40% 11.00: 51.30% 144.33: 9.60% 511.00: 19.70% Generate synthetic data Now that I understand what actually going on behind Dropout layer of Keras. Let's use it to describe uncertainty of the model following ideas described in Yarin Gal's blog . I will create a toy 1-d time series for both x and y. x : [-2, -1.999, -1.998,...,-1.502,-1.501,-1.500] + [-1.000, -0.999, -0.998,...,0.498,0.499,0.500] y : sin(x) - x Notice that x has a gap between - 1.5 and -1. In [11]: import random , math ## Define x inc = 0.001 x_train = np . concatenate ([ np . arange ( - 2 , - 1.5 , inc ), np . arange ( - 1 , 0.5 , inc )]) x_train = x_train . reshape ( len ( x_train ), 1 ) ## Define y steps_per_cycle = 1 def sinfun ( xs , noise = 0 ): xs = xs . flatten () def randomNoise ( x ): wnoise = random . uniform ( - noise , noise ) return ( math . sin ( x * ( 2 * math . pi / steps_per_cycle ) ) + wnoise ) vec = [ randomNoise ( x ) - x for x in xs ] return ( np . array ( vec ) . flatten ()) y_train0 = sinfun ( x_train ) y_train = y_train0 . reshape ( len ( y_train0 ), 1 ) print ( \" x_train.shape={}\" . format ( x_train . shape )) print ( \" y_train.shape={}\" . format ( y_train . shape )) x_train.shape=(2000, 1) y_train.shape=(2000, 1) Plot the x, y data In [12]: plt . figure ( figsize = ( 20 , 3 )) plt . scatter ( x_train . flatten (), y_train . flatten (), s = 0.5 ) plt . xlabel ( \"x_train\" ) plt . ylabel ( \"y_train\" ) plt . title ( \"The x values of the synthetic data ranges between {:4.3f} and {:4.3f}\" . format ( np . min ( x_train ), np . max ( x_train ))) plt . show () Training We consider 4-layer NN. Each hidden layer has relu activation and it is followed by dropout layer. In [13]: from keras.models import Model from keras.layers import Input , Dense , Dropout from keras import optimizers def define_model ( rate_dropout = 0.5 , activation = \"relu\" , dropout_all_hlayers = True ): tinput = Input ( shape = ( 1 ,), name = \"ts_input\" ) Nhlayer = 4 network = tinput for i in range ( Nhlayer ): network = Dense ( 1024 , activation = activation )( network ) if dropout_all_hlayers or i == ( Nhlayer - 1 ): network = Dropout ( rate_dropout )( network ) out = Dense ( 1 , activation = \"linear\" )( network ) model = Model ( inputs = [ tinput ], outputs = out ) model . compile ( loss = \"mean_squared_error\" , optimizer = optimizers . Adam ( lr = 0.0001 )) #0.0005 return ( model ) model1 = define_model () model1 . summary () hist = model1 . fit ( x_train , y_train , batch_size = len ( x_train ), verbose = False , epochs = 20000 ) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= ts_input (InputLayer) (None, 1) 0 _________________________________________________________________ dense_1 (Dense) (None, 1024) 2048 _________________________________________________________________ dropout_2 (Dropout) (None, 1024) 0 _________________________________________________________________ dense_2 (Dense) (None, 1024) 1049600 _________________________________________________________________ dropout_3 (Dropout) (None, 1024) 0 _________________________________________________________________ dense_3 (Dense) (None, 1024) 1049600 _________________________________________________________________ dropout_4 (Dropout) (None, 1024) 0 _________________________________________________________________ dense_4 (Dense) (None, 1024) 1049600 _________________________________________________________________ dropout_5 (Dropout) (None, 1024) 0 _________________________________________________________________ dense_5 (Dense) (None, 1) 1025 ================================================================= Total params: 3,151,873 Trainable params: 3,151,873 Non-trainable params: 0 _________________________________________________________________ Plot the validaiton loss and training loss In [14]: for key in hist . history . keys (): plt . plot ( hist . history [ key ], label = key ) plt . title ( \"loss={:5.4f}\" . format ( hist . history [ \"loss\" ][ - 1 ])) plt . legend () plt . yscale ( 'log' ) plt . show () Create testing data The first half of the testing x contains the x_train, and the second half contains the values outside of the range of x_train In [15]: maxx = np . max ( x_train ) x_test = np . arange ( - 2 , 3 , inc ) . reshape ( - 1 , 1 ) print ( \"x_train -- Min:{:4.3f} Max:{:4.3f}\" . format ( np . min ( x_train ), maxx )) print ( \"x_test -- Min:{:4.3f} Max:{:4.3f}\" . format ( np . min ( x_test ), np . max ( x_test ))) x_train -- Min:-2.000 Max:0.499 x_test -- Min:-2.000 Max:2.999 Prediction with dropout Repeat the prediction with dropout 100 times. As the dropout randomly drop 50% (dropout rate) of the hidden units at every hidden layers, the predicted values vary across the 100 simulations. In [16]: kdp = KerasDropoutPrediction ( model1 ) y_pred_do = kdp . predict ( x_test , n_iter = 100 ) y_pred_do_mean = y_pred_do . mean ( axis = 1 ) Prediction with no dropout In [17]: y_pred = model1 . predict ( x_test ) Visualize the descripancies between the prediction without dropout and the average prediction with dropout The two predicted values agree more or less. In [18]: plt . figure ( figsize = ( 5 , 5 )) plt . scatter ( y_pred_do_mean , y_pred , alpha = 0.1 ) plt . xlabel ( \"The average of dropout predictions\" ) plt . ylabel ( \"The prediction without dropout from Keras\" ) plt . show () Visualize the prediction with and without dropout on testing data The dropout during the prediction recoginize that some of my test data is far from my training data. i.e., the variance of the dropout prediction increases substantially in the region with no training data. In [19]: def vertical_line_trainin_range ( ax ): minx , maxx = np . min ( x_train ), np . max ( x_train ) ax . axvline ( maxx , c = \"red\" , ls = \"--\" ) ax . axvline ( minx , c = \"red\" , ls = \"--\" , label = \"The range of the x_train\" ) def plot_y_pred_do ( ax , y_pred_do , fontsize = 20 , alpha = 0.05 , title = \"The 100 y values predicted with dropout\" ): for iiter in range ( y_pred_do . shape [ 1 ]): ax . plot ( x_test , y_pred_do [:, iiter ], label = \"y_pred (Dropout)\" , alpha = alpha ) ax . set_ylim ( * ylim ) vertical_line_trainin_range ( ax ) ax . set_title ( title , fontsize = fontsize ) ylim = ( - 6 , 4 ) fig = plt . figure ( figsize = ( 18 , 5 )) fig . subplots_adjust ( hspace = 0.13 , wspace = 0.05 ) ax = fig . add_subplot ( 1 , 2 , 1 ) ax . plot ( x_test , y_pred , color = \"yellow\" , label = \"y_pred (from Keras)\" ) ax . scatter ( x_test , y_pred_do_mean , s = 50 , alpha = 0.05 , color = \"magenta\" , label = \"y_pred (Dropout average)\" ) ax . scatter ( x_train , y_train , s = 2 , label = \"y_train\" ) ax . set_ylim ( * ylim ) ax . set_xlabel ( \"x\" ) vertical_line_trainin_range ( ax ) ax . legend () ax = fig . add_subplot ( 1 , 2 , 2 ) plot_y_pred_do ( ax , y_pred_do ) plt . show () Create gif for the dropout prediciton Create a gif that is shown at the beggining of this blog post. First, save all the images as png. In [20]: ## location where the png files should be saved dir_image = \"../result/dropout_experiment/\" for icol in range ( y_pred_do . shape [ 1 ]): fig = plt . figure ( figsize = ( 10 , 10 )) fig . subplots_adjust ( hspace = 0 , wspace = 0 ) ax = fig . add_subplot ( 1 , 1 , 1 ) plot_y_pred_do ( ax , y_pred_do [:,[ icol ]], fontsize = 20 , alpha = 0.5 ) ax . scatter ( x_train , y_train , s = 4 , color = \"red\" , label = \"x_train and y_train\" ) ax . legend () ax . set_xlabel ( \"x\" , fontsize = 20 ) ax . set_ylabel ( \"y\" , fontsize = 20 ) ax . tick_params ( labelsize = 20 ) plt . savefig ( dir_image + \"/fig{:04.0f}.png\" . format ( icol ), bbox_inches = 'tight' , pad_inches = 0 ) plt . close ( 'all' ) Now we are ready to create a gif from the png files In [21]: def create_gif ( gifname , dir_image , duration = 1 ): import imageio filenames = np . sort ( os . listdir ( dir_image )) filenames = [ fnm for fnm in filenames if \".png\" in fnm ] with imageio . get_writer ( dir_image + '/' + gifname + '.gif' , mode = 'I' , duration = duration ) as writer : for filename in filenames : image = imageio . imread ( dir_image + filename ) writer . append_data ( image ) create_gif ( \"dropout_experiment\" , dir_image , duration = 0.1 ) How much does the uncertainty change by changing hyper parameters? The hyper parameters to consider are: rate_dropout activation function for hidden layers the location of hidden layers Some findings: The variance of the dropout prediction is affected by the hyper parameter values quite a bit for the area with no training data. The larger the dropout rate is, the larger the variance of the dropout prediction. Nevertheless, all the considered hyper-parameter combination shows that the model is more uncertain in the region with no training data. With tanh-activation, the uncertainty seems under estimated in comparisons to the relu-activation. This observation agrees to Yarin Gal's discussion . In [22]: rate_dropouts = [ 0.75 , 0.5 , 0.1 , 0.5 , 0.5 ] activations = [ \"relu\" , \"relu\" , \"relu\" , \"tanh\" , \"relu\" ] dropout_all_hlayers = [ True , True , True , True , False ] N = len ( rate_dropouts ) fig = plt . figure ( figsize = ( 10 , N * 7 )) count = 1 for para in zip ( rate_dropouts , activations , dropout_all_hlayers ): model = define_model ( * para ) model . fit ( x_train , y_train , batch_size = len ( x_train ), verbose = False , epochs = 20000 ) kdp = KerasDropoutPrediction ( model ) y_pred_do = kdp . predict ( x_test , n_iter = 100 ) ax = fig . add_subplot ( N , 1 , count ) plot_y_pred_do ( ax , y_pred_do , title = \"dropout_rate = {}, activation={}, dropout for all layers = {}\" . format ( * para )) count += 1 plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Measure the uncertainty in deep learning models using dropout"},{"url":"Object-detection-using-Haar-feature-based-cascade-classifiers-on-my-face.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } The above gif shows the object detection results from the Haar cascades implemented in OpenCV. In ths previous blog post Driver's facial keypoint detection , I used public dataset CVC11 to train a facial keypoint detection model. The crucial step in the modeling procedure was to trim image using the face's bounding box. In practice, you might not have access to nice bounding box. In such cases, pre-trained facial detection classifier such as Haar cascade can be useful. Import necessary libraries In [1]: import os import cv2 import time import numpy as np from collections import Counter Import images These images are extracted from iphone 6s video. I converted the video into sequence of images with 3FPS frequency. See Extract series of jpgs files from iPhone 6S video to learn how I did it. I will only use the middle 300 images. In [2]: dir_jpgs = \"IMG_7367/\" jpg_nms = np . sort ( os . listdir ( dir_jpgs ))[ 1000 : 1300 ] In [3]: from keras.preprocessing.image import img_to_array , load_img imgs = [] count = 0 for jpg in jpg_nms : if count % 200 == 0 : print ( count ) try : img = img_to_array ( load_img ( dir_jpgs + jpg )) except : img = [] pass imgs . append ( img ) count += 1 /Users/yumikondo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Using TensorFlow backend. 0 200 Haar cascades OpenCV provides us with pre-trained classifiers that are ready to be used for face detection. The Haar Classifier is a machine learning based approach, an algorithm created by Paul Viola and Michael Jones; which (as mentioned before) are trained from many many positive images (with faces) and negatives images (without faces). Some references on Haar cascades FACE DETECTION USING OPENCV AND PYTHON: A BEGINNER'S GUIDE OpenCV: Face Detection using Haar Cascades Youtube tutorial: Haar Cascade Object Detection Face & Eye - OpenCV with Python for Image and Video Analysis 16 To use the pre-trined Haar Classifiers, we need to import the classifiers. I clone the opencv repository from Github to get the pretrained cascades. You can see the cloned xmls in the following cells. You see that there are not only face classifiers but also other classifiers e.g. eye or fullbody. In [4]: ls opencv / data / haarcascades / haarcascade_eye.xml haarcascade_eye_tree_eyeglasses.xml haarcascade_frontalcatface.xml * haarcascade_frontalcatface_extended.xml * haarcascade_frontalface_alt.xml haarcascade_frontalface_alt2.xml haarcascade_frontalface_alt_tree.xml haarcascade_frontalface_default.xml haarcascade_fullbody.xml haarcascade_lefteye_2splits.xml haarcascade_licence_plate_rus_16stages.xml haarcascade_lowerbody.xml haarcascade_profileface.xml haarcascade_righteye_2splits.xml haarcascade_russian_plate_number.xml haarcascade_smile.xml haarcascade_upperbody.xml Create a cascade classifier method that dtermine the bounding box for face. Notice that the cascade classifier requires to parameters: scaleFactor minNeighbors I found that these parameters change the face detection performance a lot. So let's decide which values to use for face classifier by cross validation. The choice of parameters are described in various places. For example: stack overflow In [5]: haar_face_cascade = cv2 . CascadeClassifier ( 'opencv/data/haarcascades/haarcascade_frontalface_alt.xml' ) haar_eye_cascade = cv2 . CascadeClassifier ( 'opencv/data/haarcascades/haarcascade_eye.xml' ) haar_profile_cascade = cv2 . CascadeClassifier ( 'opencv/data/haarcascades/haarcascade_profileface.xml' ) def get_bounding_box ( img , cascade , scaleFactor = None , minNeighbors = None , printing = False ): ## process images in gray scales, ## because we don't need color information to decide if a picture has a face or not gray_img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2GRAY ) gray_img = np . array ( gray_img , dtype = 'uint8' ) faces = cascade . detectMultiScale ( gray_img , scaleFactor = scaleFactor , ## 1.1 minNeighbors = int ( minNeighbors )); if printing : print ( 'Faces found: ' , len ( faces )) return ( faces ) Cross validation to decide scaleFactor and minNeighbors We will use subset of the 100 images to decide which parameter combination to use. I will use slicing factor of 10 and select 0, 10, 20,...,1000th images for cross validation. The slicing factor is > 1 so that relatively desimilar images are selected. As our data does not have true bounding box to assess the cascade classifier's performance, I will evaluate the classifier's performance by proportion of images where the classifier find one face. Ideally, we should get 1 face per frame. In [6]: ## extract 100 images Nsample = 50 myslice = 10 img_subset = imgs [ 0 : Nsample * myslice : myslice ] minNeighbors = [ 2 , 5 , 10 ] scaleFactors = [ 1.05 , 1.1 , 1.5 ] Nallfaces , neigh , scaleF , times = [], [], [], [] for minNeighbor in minNeighbors : for scaleFactor in scaleFactors : start = time . time () faces = [] for img in img_subset : fs = get_bounding_box ( img , haar_face_cascade , scaleFactor , minNeighbor ) faces . append ( len ( fs )) end = time . time () Nallfaces . append ( faces ) neigh . append ( minNeighbor ) scaleF . append ( scaleFactor ) times . append ( end - start ) The cross validation results In [7]: import matplotlib.pyplot as plt count = 1 best_prop1 = 0 fig = plt . figure ( figsize = ( 20 , 10 )) for k in range ( len ( minNeighbors ) * len ( scaleFactors )): allfaces = np . array ( Nallfaces [ k ]) prop1 = 100 * np . mean ( allfaces == 1 ) nei , sF = neigh [ k ], scaleF [ k ] if prop1 >= best_prop1 : best_prop1 = prop1 best_para = ( nei , sF ) fig . subplots_adjust ( hspace = 0.52 ) ax = fig . add_subplot ( len ( minNeighbors ), len ( scaleFactors ), count ) ax . set_ylim ( 0 , 50 ) ax . set_xlim ( 0 , 3 ) ax . set_xticks ([ 0 , 1 , 2 , 3 ]) ax . hist ( allfaces , bins = [ 0 , 1 , 2 , 3 ]) ax . set_title ( \"minNeighbor= {} , scaleFactor= {} \\n %(1face)= {:3.0f} % ( {:5.2f} MIN)\" . format ( nei , sF , prop1 , times [ k ] / 60 )) count += 1 plt . show () The best parameter combination in terms of the proportion of the images detected with one face was: In [8]: #best_para = (10,1.05) print ( \"minNeighbor= {} , scaleFactor= {} \" . format ( * best_para )) minNeighbor=10, scaleFactor=1.05 In [9]: def plot_rectangle ( faces , ax , color = \"red\" ): for face in faces : ax . add_patch ( patches . Rectangle ( face [: 2 ], # (x,y) face [ 2 ], # width face [ 3 ], # height fill = False , color = color ) ) def plot_objects ( img , objecs , object_names , ax ): colors = [ \"red\" , \"blue\" , \"yellow\" ] ax . imshow ( img / 255.0 ) count = 100 for obj , nm , color in zip ( objecs , object_names , colors ): plot_rectangle ( obj , ax , color = color ) ax . text ( 800 , count , nm , color = color , fontsize = 20 ) count += 50 Create directories to save each image as .png In [10]: dir_image = \"drive_image/\" try : os . mkdir ( dir_image ) print ( \"folder is created.\" ) except : pass folder is created. For each frame, detect all the objects of intrest, plot the bounding box around it, and then save it as an png image. In [11]: import matplotlib.patches as patches ## name of all the classifiers obj_nms = [ \"Face\" , \"Eye\" , \"Profile face\" ] ## all the classifiers objects = [ haar_face_cascade , haar_eye_cascade , haar_profile_cascade ] count = 0 y_pred = None data = [] for myid , img in enumerate ( imgs ): objs = [] for haar in objects : objs . append ( get_bounding_box ( img , haar , best_para [ 0 ], best_para [ 1 ])) fig = plt . figure ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( 1 , 1 , 1 ) ax . set_xticks ([]) ax . set_yticks ([]) plot_objects ( img , objs , obj_nms , ax ) plt . savefig ( dir_image + \"/frame {:05.0f} .png\" . format ( myid ), bbox_inches = 'tight' , pad_inches = 0 ) plt . close ( 'all' ) Create gif In [12]: import imageio filenames = np . sort ( os . listdir ( dir_image )) filenames = [ fnm for fnm in filenames if \".png\" in fnm ] with imageio . get_writer ( dir_image + '/driver.gif' , mode = 'I' ) as writer : for filename in filenames : image = imageio . imread ( dir_image + filename ) writer . append_data ( image ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Object detection using Haar feature-based cascade classifiers on my face"},{"url":"Driver-facial-keypoint-detection.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } What you see above is the gif of a driver subject from test data with the estimated facial keypoints. Evaluation of driving performance is very important to reduce road accident rate. There are a lot of public data related to the driver's faces, including some from the Kaggle competition by State Farm Distracted Driver Detection . The techinical tasks associated with these driver data vary from the classification of the driver behaviors to facial keypoint detections. In this blog post, I will use public driver data CVC11 to detect driver's facial keypoints (Right eye, left eye, nose, right mouth edge and left mouth edge). In my previous blog post Achieving Top 23% in Kaggle's Facial Keypoints Detection with Keras + Tensorflow , I also conducted facial keypoint detection using Facial Keypoints Detection and the techinical task is more or less similar. From this blog post, you will: create a gif of the driver together with the estimated facial keypoints. In [37]: ## Import usual libraries import cv2 import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import sys , time , os , warnings import numpy as np import pandas as pd from collections import Counter warnings . filterwarnings ( \"ignore\" ) print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )); del keras print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"4\" set_session ( tf . Session ( config = config )) def set_seed ( sd = 123 ): from numpy.random import seed from tensorflow import set_random_seed import random as rn ## numpy random seed seed ( sd ) ## core python's random number rn . seed ( sd ) ## tensor flow's random number set_random_seed ( sd ) python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.1.3 tensorflow version 1.5.0 I downloaded the DrivFace zip file from CVC11 and saved it at: In [38]: dir_data = \"DrivFace/\" Let's take a look at the read me file. The images are obtained from 4 drivers. cat DriveFace/readme.txt returns the following info: The DrivFace database contains images sequences of subjects while driving in real scenarios. It is composed of 606 samples of 640ï¿½480 pixels each, acquired over different days from 4 drivers (2 women and 2 men) with several facial features like glasses and beard. The ground truth contains the annotation of the face bounding box and the facial key points (eyes, nose and mouth). A set of labels assigning each image into 3 possible gaze direction classes are given. The first class is the ï¿½looking-rightï¿½ class and contains the head angles between -45ï¿½ and -30ï¿½. The second one is the ï¿½frontalï¿½ class and contains the head angles between -15ï¿½ and 15ï¿½. The last one is the ï¿½looking-leftï¿½ class and contains the head angles between 30ï¿½ and 45ï¿½. ATTN: This database is free for academic usage. For other purposes, please contact PhD. Katerine Diaz (kdiaz@cvc.uab.es). Files and scripts ï¿½ DrivImages.zip has the driver images. The imag's name has the format: * YearMonthDay_subject_Driv_imNum_HeadPose.jpg i.e. 20130529_01_Driv_011_f .jpg is a frame of the fisrts driver corresponding to the 11 sequence's image and the head pose is frontal. subject = [1:4], imNum = [001:...], HeadPose = lr (looking-right), f (frontal) and lf (looking-left). ï¿½ drivPoints.txt contains the ground truth in table's format, where the columns have the follow information: * fileName is the imagen's name into DrivImages.zip * subject = [1:4] * imgNum = int * label = [1/2/3] (head pose class that corresponding to [lr/f/lf], respectively) * ang = [-45, -30/ -15 0 15/ 30 15] (head pose angle) * [xF yF wF hF] = face position * [xRE yRE] = rigth eye position * [xLE yL] = left eye position * [xN yN] = Nose position * [xRM yRM] = rigth corner of mouth * [xLM yLM] = left corner of mouth ï¿½ read_drivPoints.m is a Matlab function to read the drivPoints file. You can also use: * Table = readtable('drivPoints.txt'); ï¿½ DrivFace.zip has all the previous files Citations Katerine Diaz-Chito, Aura Hernï¿½ndez-Sabatï¿½, Antonio M. Lï¿½pez, A reduced feature set for driver head pose estimation, Applied Soft Computing, Volume 45, August 2016, Pages 98-107, ISSN 1568-4946, http://dx.doi.org/10.1016/j.asoc.2016.04.027. Contact information Questions? Email Katerine Diaz (kdiaz@cvc.uab.es) Read in the annocation data As the first step, I will read in the drivePoints.txt into a panda dataframe that contains the facial keypoints and driver's infomation for each image. The data contain not only the facial keypoints but also the bounding box. In [39]: labels = open ( dir_data + \"drivPoints.txt\" ) . read () labels = labels . split ( \" \\r\\n \" ) lines = [ line . split ( \",\" ) for line in labels ] df_label = pd . DataFrame ( lines [ 1 :], columns = lines [ 0 ]) cols = list ( set ( df_label . columns ) - set ([ \"fileName\" ])) df_label [ cols ] = df_label [ cols ] . apply ( pd . to_numeric , errors = 'coerce' ) ## remove the rows with NA print ( df_label . shape ) df_label = df_label . dropna () print ( df_label . shape ) df_label . head ( 3 ) (607, 19) (606, 19) Out[39]: fileName subject imgNum label ang xF yF wF hF xRE yRE xLE yLE xN yN xRM yRM xLM yLM 0 20130529_01_Driv_001_f 1.0 1.0 2.0 0.0 292.0 209.0 100.0 112.0 323.0 232.0 367.0 231.0 353.0 254.0 332.0 278.0 361.0 278.0 1 20130529_01_Driv_002_f 1.0 2.0 2.0 0.0 286.0 200.0 109.0 128.0 324.0 235.0 366.0 235.0 353.0 258.0 333.0 281.0 361.0 281.0 2 20130529_01_Driv_003_f 1.0 3.0 2.0 0.0 290.0 204.0 105.0 121.0 325.0 240.0 367.0 239.0 351.0 260.0 334.0 282.0 362.0 282.0 Create a list object imgs. imgs[i] contains a numpy array of image corresponding to the df_label.iloc[i,:]. In [40]: from keras.preprocessing.image import img_to_array , load_img imgs = [] count = 0 for jpg in df_label [ \"fileName\" ]: if count % 100 == 0 : print ( count ) try : img = img_to_array ( load_img ( dir_data + \"/DrivImages/\" + jpg + \".jpg\" )) except : img = [] pass imgs . append ( img ) count += 1 assert len ( imgs ) == df_label . shape [ 0 ] 0 100 200 300 400 500 600 look at some example images For each driver, we plot the first 4 images, together with the bounding box and the keypoints. I will use the first 3 drivers to create a model and use the last driver to test the model performance. In [41]: import matplotlib.pyplot as plt import matplotlib.patches as patches import numpy as np uni_subject = np . unique ( df_label [ \"subject\" ]) Nimage_subj = 4 count = 1 fig = plt . figure ( figsize = ( 25 , 18 )) for subj in uni_subject : pick = df_label [ \"subject\" ] == subj df_label_subj = df_label . loc [ pick ,:] start = np . min ( np . where ( pick )[ 0 ]) imgs_subj = imgs [ start :] for wh , jpg in enumerate ( df_label_subj [ \"fileName\" ] . values [: Nimage_subj ]): img = imgs_subj [ wh ] row = df_label_subj . loc [ df_label_subj [ \"fileName\" ] == jpg ,:] ax = fig . add_subplot ( len ( uni_subject ), Nimage_subj , count ) ax . imshow ( img / 255 ) ## the boundign box ax . add_patch ( patches . Rectangle ( ( row [ \"xF\" ] . values , row [ \"yF\" ] . values ), # (x,y) row [ \"wF\" ] . values , # width row [ \"hF\" ] . values , # height fill = False ) ) ## facial key points ax . scatter ( row [ \"xRE\" ] . values , row [ \"yRE\" ] . values , c = \"red\" ) ax . scatter ( row [ \"xLE\" ] . values , row [ \"yLE\" ] . values , c = \"red\" ) ax . scatter ( row [ \"xN\" ] . values , row [ \"yN\" ] . values , c = \"yellow\" ) ax . scatter ( row [ \"xRM\" ] . values , row [ \"yRM\" ] . values , c = \"blue\" ) ax . scatter ( row [ \"xLM\" ] . values , row [ \"yLM\" ] . values , c = \"blue\" ) count += 1 plt . show () Assume that we have a bounding box As this data contains the bounding box, we could use this bounding box to trim out the face of the driver. However, relying on the bounding box may be problematic in the testing scenario where there is no available bounding box. In such cases, we might need to use some machine learning methods to find bounding box. Such methods are discussed in various blogs e.g.: tutorial blog about opencv face detection with python CascadeClassifier description Nevertheless, for this blog post, I will assume that the bounding box is given. In [42]: def adjust_loc ( xys , x_topleft , y_topleft ): out = [] for xy in xys : out . append (( xy [ 0 ] - x_topleft , xy [ 1 ] - y_topleft )) return ( out ) def get_bounding_box ( img , row = None , printing = False ): ''' if row is provided, then use the bounding box defined in dataframe ''' faces = [( int ( row [ \"xF\" ]), int ( row [ \"yF\" ]), int ( row [ \"wF\" ]), int ( row [ \"hF\" ]))] if printing : print ( 'Faces found: ' , len ( faces )) return ( faces ) def adjust_xy ( x , y , shape_orig , shape_new ): xnew = x * shape_new [ 1 ] / float ( shape_orig [ 1 ]) ynew = y * shape_new [ 0 ] / float ( shape_orig [ 0 ]) return xnew , ynew def adjust_xys ( xys , img_small , img_small_resize ): ''' a vector containing: ['xRE', 'yRE', 'xLE', 'yLE', 'xN', 'yN', 'xRM', 'yRM', 'xLM', 'yLM'] ''' xys_resize = [] for xy in xys : x , y = adjust_xy ( xy [ 0 ], xy [ 1 ], img_small . shape , img_small_resize . shape ) xys_resize . extend ([ x , y ]) return ( xys_resize ) Record the names of columns containing the facial landmarks In [43]: labels_to_keep = [ \"RE\" , \"LE\" , \"N\" , \"RM\" , \"LM\" ] Ycolumns = [] for lab in labels_to_keep : Ycolumns . extend ([ \"x\" + lab , \"y\" + lab ]) Ycolumns Out[43]: ['xRE', 'yRE', 'xLE', 'yLE', 'xN', 'yN', 'xRM', 'yRM', 'xLM', 'yLM'] Modeling strategy The entire original image is pretty large: 640 x 480. We will trim out the images in bounding box and use them as training images. Here, I have a problem. The bounding box sizes differ across images. My CNN model has fully connected layers, meaning that the model assumes that the image sizes are the same across all the training data. Therefore, I will reshape the image in bounded box into the pre-specified target shape, and change the facial key point coordinates accordingly. Set the target size: In [44]: target_shape = ( 90 , 90 ) Here is the example plots to show how original image is converted to target shape using bounding box. In [45]: from copy import copy X , Y = [], [] for i in [ 1 , 200 , 400 , 600 ]: row = df_label . iloc [ i ,:] img = imgs [ i ] xys = [] for label in labels_to_keep : xys . append (( row [ \"x\" + label ], row [ \"y\" + label ])) #go over list of faces and draw them as rectangles on original colored img_copy = copy ( img ) faces = get_bounding_box ( img , row ) ( x , y , w , h ) = faces [ 0 ] cv2 . rectangle ( img_copy , ( x , y ), ( x + w , y + h ), ( 0 , 255 , 0 ), 2 ) xys = adjust_loc ( xys , x , y ) ## 1. original image ## convert image to RGB and show image fig = plt . figure ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( 1 , 3 , 1 ) ax . imshow ( img_copy / 255 ) ax . set_title ( \"original \\n shape={}\" . format ( img_copy . shape )) ax . grid ( False ) ## 2. cascade image ax = fig . add_subplot ( 1 , 3 , 2 ) img_small = img [ y :( y + h ), x :( x + w )] ax . imshow ( img_small / 255 ) for xy in xys : ax . scatter ( xy [ 0 ], xy [ 1 ]) ax . set_title ( \"bounding box \\n shape={}\" . format ( img_small . shape )) ax . grid ( False ) ##3. reshaped cascade image ax = fig . add_subplot ( 1 , 3 , 3 ) img_small_resize = cv2 . resize ( img_small , target_shape ) ax . imshow ( img_small_resize / 255 ) ax . set_title ( \"resized image \\n shape={}\" . format ( img_small_resize . shape )) ax . grid ( False ) xys_resize = adjust_xys ( xys , img_small , img_small_resize ) for i in range ( 0 , len ( xys_resize ), 2 ): ax . scatter ( xys_resize [ i ], xys_resize [ i + 1 ]) plt . show () Now, I will create training and testing images. Each image size is fixed to the target_shape. In df_label, I will record which image is used as a training image and which one is used as testing image. In [46]: df_label [ \"testTF\" ] = np . NaN df_label [ \"testID\" ] = np . NaN In [47]: X , Y , testTF = [], [], [] count = 0 ## I will use the last subject as the testing data test_subjectID = [ 4 ] for i in range ( len ( imgs )): row = df_label . iloc [ i ,:] img = imgs [ i ] if len ( img ) < 1 : print ( \"no image! {}\" . format ( i )) continue faces = get_bounding_box ( img , row ) if len ( faces ) < 1 : ## no face is discovered print ( \"no face discovered {}\" . format ( i )) continue ## shifting and rescaling anotation xys = [] for label in labels_to_keep : xys . append (( row [ \"x\" + label ], row [ \"y\" + label ])) ( x , y , w , h ) = faces [ 0 ] xys = adjust_loc ( xys , x , y ) img_small = img [ y :( y + h ), x :( x + w )] img_small_resize = cv2 . resize ( img_small , target_shape ) xys_resize = adjust_xys ( xys , img_small , img_small_resize ) X . append ( img_small_resize ) Y . append ( xys_resize ) inTest = row [ \"subject\" ] in test_subjectID testTF . extend ([ inTest ]) df_label [ \"testTF\" ] . iloc [ i ] = inTest if inTest : df_label [ \"testID\" ] . iloc [ i ] = count count += 1 X , Y , testTF = np . array ( X ), np . array ( Y ), np . array ( testTF ) X_train0 , y_train0 = X [ ~ testTF ], Y [ ~ testTF ] X_test0 , y_test0 = X [ testTF ], Y [ testTF ] Standardization Standardize the data according to the mean and standard deviations of the facial keypoints in training set. In [48]: mY = np . mean ( y_train0 , axis = 0 ) sdY = np . std ( y_train0 , axis = 0 ) def standy ( y_train0 , printTF = False ): if printTF : print ( \"Range in original scale: [{:5.3f},{:5.3f})\" . format ( np . min ( y_train0 ), np . max ( y_train0 ))) y_train0 = ( y_train0 - mY ) / sdY if printTF : print ( \"Range in standardized scale: [{:5.3f},{:5.3f})\" . format ( np . min ( y_train0 ), np . max ( y_train0 ))) return ( y_train0 ) y_train , y_test = standy ( y_train0 ), standy ( y_test0 ) X_train , X_test = X_train0 / 255.0 , X_test0 / 255.0 print ( X_train . shape , y_train . shape ) print ( X_test . shape , y_test . shape ) ((516, 90, 90, 3), (516, 10)) ((90, 90, 90, 3), (90, 10)) Baseline In this analysis, we assume that the bounding box is provided. Given bounding box and without model, the natural way to estimate the facial keypoints is to use the mean facial keypoints. i.e., no matter what image is, baseline guesses the (x,y) coordinate of right eye at (34.5,20.85). I will compare the model performance with this baseline method. In [49]: y_pred_baseline = y_train0 . mean ( axis = 0 ) print ( \"The baseline estimate for the x,y coordinate of right eye is at ({:4.3f},{:4.3f}) within the bounding box\" . format ( y_pred_baseline [ 0 ], y_pred_baseline [ 1 ])) The baseline estimate for the x,y coordinate of right eye is at (31.029,18.765) within the bounding box Define CNN model I use standard CNN models. Notice that I am using Dense layer at the end. This is the reason why I need to ensure that the training image sizes are the same. In [50]: import numpy as np from keras.models import Sequential from keras.layers import Conv2D , Activation , MaxPooling2D , Dense , Flatten batch_size = 64 num_channels = 3 model = Sequential () # uses theano ordering. Note that we leave the image size as None to allow multiple image sizes model . add ( Conv2D ( 32 , 3 , 3 , border_mode = 'same' , name = \"conv2d\" , input_shape = target_shape + ( num_channels ,))) model . add ( Activation ( 'relu' )) model . add ( Conv2D ( 32 , 3 , 3 )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Conv2D ( 32 , 3 , 3 , border_mode = 'same' )) model . add ( Activation ( 'relu' )) model . add ( Conv2D ( 32 , 3 , 3 )) model . add ( Activation ( 'relu' )) model . add ( Flatten ()) model . add ( Dense ( Y . shape [ 1 ], name = \"Dense_layer\" )) model . compile ( loss = 'mse' , optimizer = 'sgd' ) model . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 90, 90, 32) 896 _________________________________________________________________ activation_5 (Activation) (None, 90, 90, 32) 0 _________________________________________________________________ conv2d_4 (Conv2D) (None, 88, 88, 32) 9248 _________________________________________________________________ activation_6 (Activation) (None, 88, 88, 32) 0 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 44, 44, 32) 0 _________________________________________________________________ conv2d_5 (Conv2D) (None, 44, 44, 32) 9248 _________________________________________________________________ activation_7 (Activation) (None, 44, 44, 32) 0 _________________________________________________________________ conv2d_6 (Conv2D) (None, 42, 42, 32) 9248 _________________________________________________________________ activation_8 (Activation) (None, 42, 42, 32) 0 _________________________________________________________________ flatten_2 (Flatten) (None, 56448) 0 _________________________________________________________________ Dense_layer (Dense) (None, 10) 564490 ================================================================= Total params: 593,130 Trainable params: 593,130 Non-trainable params: 0 _________________________________________________________________ Training starts here Here, I use 20% of randomly selected the training data as the validation data. This is probablly not a very good idea considering the high correlation across adjacent images within subject. Ideally, training and validation data should not share the same subjects so that validation data can assess the model performance for the future subjects. However, since we only have 4 subjects, we will do random split. In [51]: hist = model . fit ( X_train , y_train , batch_size = 256 , epochs = 800 , verbose = False , validation_split = 0.2 ) Save the model arthitecture and the weights In [52]: modelname = \"model_facialkeypoints.h5\" model . save ( modelname ) statinfo = os . path . getsize ( modelname ) print ( \"The model size: {}MB\" . format ( statinfo / 1000.0 / 1000 )) The model size: 4.781528MB plot training and validation loss The backpropagation algorithm seems to converge. In [67]: for label in hist . history . keys (): plt . plot ( hist . history [ label ], label = label ) plt . xlabel ( \"epochs\" ) plt . legend () plt . title ( \"The final val_loss={:4.3f}\" . format ( hist . history [ \"val_loss\" ][ - 1 ])) plt . show () Model performance on testing data Un-standardize the estimated facial keypoints. The estimated x,y coordinates of y_pred corresponds to the target_shape. In [54]: y_pred = model . predict ( X_test ) y_pred = y_pred * sdY + mY assert np . all ( y_pred <= np . max ( target_shape )) assert np . all ( y_pred >= 0 ) Convert y_pred into the original scale To visualize the model performance on the original image, we need to convert the y_pred in original scale. Currently, y_pred contains the (x,y) coordinates of the target_shape. Remind you how the original image is transformed into a training image: original image (640, 480) â‡¨ image in bounding box (xF, yF, wF,hF) â‡¨ the value of bx and by depends on the image resize the image to target_shape Therefore, in order to convert the (x',y') coordinates of y_pred into the original image scale (x_orig,y_orig), we need to: x_orig = x' * wF/target_shape[0] + xF y_orig = y' * hF/target_shape[1] + yF In the following code, y_pred_orig contains the (x,y) coordinates of the estimated facial keypoints in the original image scale. In [55]: df_label_test = df_label . loc [ df_label [ \"testTF\" ]] y_pred_orig = [] for irow in range ( y_pred . shape [ 0 ]): yp = copy ( y_pred [ irow ]) ## y_pred contains (x-RE, y-RE, x-LE, y-LE,....) row = df_label_test . iloc [ irow ,:] yp [ 0 :: 2 ] = yp [ 0 :: 2 ] * row [ \"wF\" ] / float ( target_shape [ 0 ]) + row [ \"xF\" ] yp [ 1 :: 2 ] = yp [ 1 :: 2 ] * row [ \"hF\" ] / float ( target_shape [ 1 ]) + row [ \"yF\" ] y_pred_orig . append ( yp ) y_pred_orig = np . array ( y_pred_orig ) assert y_pred_orig . shape [ 0 ] == np . sum ( df_label [ \"testTF\" ]) assert len ( imgs ) == df_label . shape [ 0 ] Evaluate the model performance in terms of the normalized mean Euclidean distances between the true facial keypoint and the estimated one. Let ($x_i,y_i$) be the facial keypoint of interest e.g., nose coordinate or left eye coordiante. Let also ($\\widehat{x}_i,\\widehat{y}_i$) be their predicted values. Then for each landmark, I report the mean normalized euclidean distance between the predicted and the true landmark. The normalization is done by the inter-pupil distance. $$ \\frac{1}{N}\\sum_{i=1}&#94;N \\frac{ \\sqrt{(x_i - \\widehat{x}_i)&#94;2 + (y_i - \\widehat{y}_i)&#94;2} }{ \\textrm{inter pupil distance}_i } $$ where $$ \\textrm{inter pupil distance}_i = \\sqrt{ (x_i&#94;{\\textrm{Right}} - x_i&#94;{\\textrm{Left}} )&#94;2 + (y_i&#94;{\\textrm{Right}} - y_i&#94;{\\textrm{Left}} )&#94;2 } $$ You can think of the normalized ED as the average relative error and 100% means that the average error is as large as the distance between the eyes. Calculate inter pupil distance for each image: In [56]: df_label [ \"IPD\" ] = np . sqrt (( df_label [ \"xRE\" ] - df_label [ \"xLE\" ]) ** 2 + ( df_label [ \"yRE\" ] - df_label [ \"yLE\" ]) ** 2 ) Calculate the euclidean distance (ED) between the predicted facial keypoint and the true facial keypoint for RE, LE, N, RM and LM. As the predicted facial keypoints, I consider both the model and the baseline. In [57]: for ii , facialkp in enumerate ( labels_to_keep ): i = ii * 2 ## Model prediction df_label [ \"Model_\" + facialkp ] = np . NaN xterm = ( y_pred [:, i ] - y_test0 [:, i ]) ** 2 yterm = ( y_pred [:, i + 1 ] - y_test0 [:, i + 1 ]) ** 2 df_label [ \"Model_\" + facialkp ] . loc [ df_label [ \"testTF\" ] . values ] = np . sqrt ( xterm + yterm ) ## baseline df_label [ \"Baseline_\" + facialkp ] = np . NaN xterm = ( y_pred_baseline [ i ] - y_test0 [:, i ]) ** 2 yterm = ( y_pred_baseline [ i + 1 ] - y_test0 [:, i + 1 ]) ** 2 df_label [ \"Baseline_\" + facialkp ] . loc [ df_label [ \"testTF\" ] . values ] = np . sqrt ( xterm + yterm ) Finally, I am ready to plot some results on testing data. I will plot the predicted landmarks on the original image (640 x 480) and also on the resized bounded box (target_shape). I will also report the normalized euclidean distance between the true and predicted landmarks for all 5 facial keypoints. In [58]: def plot_test_subject ( X_test_row , y_pred_row , row , img_row , y_pred_orig_row , figureimg = None ): s = 100 marker = \"X\" fig = plt . figure ( figsize = ( 20 , 10 )) fig . subplots_adjust ( hspace = 0 , wspace = 0 ) ax = plt . subplot2grid (( 2 , 3 ), ## (Nrow, Ncol) ( 0 , 0 ), ## ( row, col) colspan = 2 , rowspan = 2 ) ax . grid ( False ) ax . imshow ( img_row / 255 ) for p in range ( 0 , y_pred_orig . shape [ 1 ], 2 ): ax . scatter ( y_pred_orig_row [ p ], y_pred_orig_row [ p + 1 ], c = \"green\" , s = s , marker = marker ) ax = plt . subplot2grid (( 2 , 3 ), ( 0 , 2 ), colspan = 1 , rowspan = 1 ) ax . imshow ( X_test_row ) ax . grid ( False ) for p in range ( 0 , len ( y_pred_row ), 2 ): ax . scatter ( y_pred_row [ p ], y_pred_row [ p + 1 ], c = \"green\" , s = s , marker = marker ) ax = plt . subplot2grid (( 2 , 3 ), ( 1 , 2 ), colspan = 1 , rowspan = 1 ) ax . axis ( 'off' ) ax . plot () ax . set_xlim ( 0 , 1 ) ax . set_ylim ( 0 , len ( labels_to_keep )) for i , lab in enumerate ( labels_to_keep ): ax . text ( 0 , i , \"normalized ED {:3.2} {:4.2f}%\" . format ( lab , row [ \"Model_\" + lab ] / row [ \"IPD\" ] * 100 ), fontsize = 30 ) if figureimg is not None : plt . savefig ( figureimg + '.png' , bbox_inches = 'tight' , pad_inches = 0 ) else : plt . show () for irow in [ 1 , 80 ]: irow_orig = np . where ( df_label [ \"testID\" ] == irow )[ 0 ][ 0 ] row = df_label . iloc [ irow_orig ,:] img_row = imgs [ irow_orig ] plot_test_subject ( X_test [ irow ], y_pred [ irow ], row , img_row , y_pred_orig [ irow ], figureimg = None ) Create gif for the test subject driver Rather than plotting all the images in ipython notebook, let's create gif out of them! To do this, we first need to save all the frames as .png. In [59]: Nimage = y_pred . shape [ 0 ] dir_test_subject = \"test_subject/\" try : os . mkdir ( dir_test_subject ) except : pass for irow in range ( Nimage ): irow_orig = np . where ( df_label [ \"testID\" ] == irow )[ 0 ][ 0 ] row = df_label . iloc [ irow_orig ,:] img_row = imgs [ irow_orig ] plot_test_subject ( X_test [ irow ], y_pred [ irow ], row , img_row , y_pred_orig [ irow ], figureimg = dir_test_subject + \"/fig{:05.0f}\" . format ( irow )) print ( Nimage ) 90 Check the number of figures that are saved in the test_subject folder. This has to be the same as the number of images. In [60]: ls - lrth $ dir_test_subject | grep \".png\" | wc - l 90 Combine all the png images and create a single gif. This is the gif at the very beginning of this blog post. In [61]: import imageio filenames = np . sort ( os . listdir ( dir_test_subject )) filenames = [ fnm for fnm in filenames if \".png\" in fnm ] with imageio . get_writer ( dir_test_subject + '/test_subject.gif' , mode = 'I' ) as writer : for filename in filenames : image = imageio . imread ( dir_test_subject + filename ) writer . append_data ( image ) Box plot to compare the model performance with the baseline method OK. THe model performance looks reasonable for this test subject. But how much of the model performance is attributable to the model and how much is from having bounding box? In [62]: collabels = [] for nm in labels_to_keep : collabels . extend ([ \"Model_\" + nm , \"Baseline_\" + nm ]) df_eval = df_label [ collabels ] In [63]: df_eval = df_eval . dropna () ## NA is recorded for the training image ## un-pivot so that there is a a single column containing box plot values ## this un-pivot is necessary for seaborn boxplot df_boxplot = pd . melt ( df_eval , value_vars = collabels ) v = np . array ([ term . split ( \"_\" ) for term in df_boxplot [ \"variable\" ]]) df_boxplot [ \"procedure\" ] = v [:, 0 ] df_boxplot [ \"keypoints\" ] = v [:, 1 ] The table below shows the median normalized euclidean distances and the proportion of the normalized euclidean distances less than 10% for each landmark and for each procedure. Clearly, the model performance is better by using CNN for all the landmark except the left mouth edge . Sadly, the left mouth edge is predicted better by the baseline! We need to do more data augmentation to improve the model performance for this landmark! In [64]: def proplessthan ( vec ): values = [ np . median ( vec [ \"value\" ]), np . mean ( vec [ \"value\" ] < 10 ) * 100 ] return ( pd . Series ( values , index = [ \"Median normalized ED\" , \"% (normalized ED < 10%)\" ] )) df_boxplot_summary = df_boxplot . groupby ([ \"keypoints\" , \"procedure\" ]) . apply ( proplessthan ) . reset_index () df_boxplot_summary Out[64]: keypoints procedure Median normalized ED % (normalized ED < 10%) 0 LE Baseline 5.216987 80.000000 1 LE Model 4.184403 95.555556 2 LM Baseline 3.862965 97.777778 3 LM Model 4.128279 97.777778 4 N Baseline 7.346033 65.555556 5 N Model 5.725722 87.777778 6 RE Baseline 6.890649 78.888889 7 RE Model 3.794306 95.555556 8 RM Baseline 5.090090 97.777778 9 RM Model 3.349447 100.000000 Boxplot for assessing the model performance on test subject In [66]: import seaborn as sns fig = plt . figure ( figsize = ( 20 , 10 )) ax = fig . add_subplot ( 1 , 2 , 1 ) bp = sns . boxplot ( x = \"keypoints\" , y = \"value\" , hue = \"procedure\" , data = df_boxplot , palette = \"Set3\" , ax = ax ) ax . set_title ( \"Mean Square Error/ inter pupil range\" ) ax = fig . add_subplot ( 1 , 2 , 2 ) ax . plot () ax . set_xlim ( 0 , 1 ) ax . axis ( \"off\" ) ax . set_ylim ( 0 , df_boxplot_summary . shape [ 0 ]) for i in range ( df_boxplot_summary . shape [ 0 ]): ax . text ( 0 , i , \"{:8} {:10} Median normalized ED {:4.2f}%, %(normalized ED < 10%)={:4.2f}% \" . format ( * df_boxplot_summary . iloc [ i ,:] . values ) , fontsize = 20 ) plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Driver's facial keypoint detection"},{"url":"create-jpgs-from-iphone-video.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In this blog, I will show how to extract image (.png) from video recorded using iphone. Preparation I recorded a 10 minutes and 21 second video using my iphone 6s. In my current directory, I have IMG7367.MOV file with: Size: 639.3 MB Dimensions: 1280 Ã— 720 You need cv2 If you do not have cv2, please make sure to pip install it as: pip3 install opencv-python==3.2.0.6 Reference in opencv official document Python - Extracting and Saving Video Frames Reading every nth frame from VideoCapture in OpenCV The location of the MOV file from iphone. In [60]: path_to_MOV = \"IMG_7367.MOV\" The location of the directory to save the file. If the folder does NOT exist, then create one. In [61]: import os dir_jpgs = path_to_MOV [: - 4 ] try : os . mkdir ( dir_jpgs ) print ( \"folder created\" ) except : pass As I recorded a 10 minutes and 21 second video using my iphone 6s, and the iphone 6s has 30fps, getting every frame will create about 18650 images, which is a bit too many. In [62]: VIDEO_MIN = 10 + 21.0 / 60 In [63]: cap = cv2 . VideoCapture ( path_to_MOV ) FPS = cap . get ( cv2 . CAP_PROP_FPS ) # Gets the frames per second print ( \"FPS: {:4.2f} \" . format ( FPS )) print ( \"The maximum number of frames that can be extracted: {} \" . format ( int ( VIDEO_MIN * 60 * FPS ))) FPS:30.02 The maximum number of frames that can be extracted: 18645 I will downsample and sample every 10 frame. In [64]: downsamplefactor = 10 print ( \"Downsampling makes FPS: {:4.2f} \" . format ( FPS / float ( downsamplefactor ))) Downsampling makes FPS: 3.00 This will create roughly 1800 images of size 1280 x 720. In [65]: import numpy as np import cv2 import time print ( cv2 . __version__ ) count , iframe = 0 , 0 success = 1 start = time . time () while ( success ): # Capture frame-by-frame success , frame = cap . read () # my original image was upside down. use flip to fix this. frame = cv2 . flip ( frame , 0 ) # fix the mirror frame = cv2 . flip ( frame , 1 ) if count % downsamplefactor == 0 : # Our operations on the frame come here cv2 . imwrite ( dir_jpgs + '/frame {:05.0f} .jpg' . format ( iframe ), frame ) iframe += 1 count += 1 if count % 2000 == 0 : print ( count ) cap . release () end = time . time () print ( \"TIME TOOK {:4.2f} MIN\" . format (( end - start ) / 60 )) 3.2.0 2000 4000 6000 8000 10000 12000 14000 16000 18000 TIME TOOK 3.27 MIN if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Extract series of JPEG files from iPhone 6S video"},{"url":"Color-gray-scale-images-and-manga-using-deep-learning.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In this blog post, I will try to create a deep learning model that can color a gray scale image. I follow this great blog post Colorizing B&W; Photos with Neural Networks . I will consider two example data to train a model: Flickr8K data Hunter x Hunter anime data Flickr8K data is a famous public data in computer vision community, and it was also previously analyzed in my blog. The downloading process is described at Develop an image captioning deep learning model using Flickr 8K data . I will first use this standard data to validate the method with small data size (8,000 images). Hunter x Hunter is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Weekly ShÅnen Jump magazine and journal since March 3, 1998. Hunter Ã— Hunter was adapted into anime television series twice, in 1999 and in 2011. While the anime ttelevision series ended, the manga is on going in Shonen Jump magazine and the new manga is produced every week (except when the author \"takes a break\"). While anime is colored, the manga is not. So my motivation is that if we can train a deep learning model with the colored anime images, we may be able to color the manga and enjoy colored manga for free!! With this motivation, I will try to create a deep learning model using Hunter x Hunter anime data. Reference Develop an image captioning deep learning model using Flickr 8K data Download all images from Google image search query using python Color space defenitions in python, RGB and LAB In [1]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import sys , time , os , warnings import numpy as np import pandas as pd from collections import Counter warnings . filterwarnings ( \"ignore\" ) print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )); del keras print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"0\" set_session ( tf . Session ( config = config )) def set_seed ( sd = 123 ): from numpy.random import seed from tensorflow import set_random_seed import random as rn ## numpy random seed seed ( sd ) ## core python's random number rn . seed ( sd ) ## tensor flow's random number set_random_seed ( sd ) Using TensorFlow backend. python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.1.3 tensorflow version 1.5.0 Flickr8k image Load the images. Here, I load the image in LAB format, and my analysis is all based in LAB scale image. The only time I will transform the images into RGB will be when I want to plot the images using pyplot.imshow() function To learn about the LAB format, please refer to my previous blog post Color space defenitions in python, RGB and LAB . In [2]: from keras.preprocessing.image import img_to_array , load_img from skimage.color import rgb2lab , lab2rgb target_size = ( 256 , 256 , 3 ) X = [] dir_data = \"../Flickr8k/Flicker8k_Dataset/\" for filenm in os . listdir ( dir_data ): imgrgb = img_to_array ( load_img ( dir_data + filenm , target_size = target_size )) / 255.0 imglab = rgb2lab ( imgrgb ) X . append ( imglab ) In [3]: X = np . array ( X ) X . shape Out[3]: (8091, 256, 256, 3) In [4]: for i in range ( X . shape [ - 1 ]): vec = X [:,:,:, i ] print ( \"MIN={:5.3f} MAX={:5.3f}\" . format ( np . min ( vec ), np . max ( vec ))) MIN=0.000 MAX=100.000 MIN=-86.183 MAX=98.233 MIN=-107.752 MAX=94.478 In [5]: def standardizeLAB ( X ): ## Standardize the LAB standX = np . zeros ( X . shape ) ## standardized one takes values between 0 and 1 standX [:,:,:, 0 ] = X [:,:,:, 0 ] / 100.0 ## standardized one takes values between -1 and 1 standX [:,:,:, 1 :] = X [:,:,:, 1 :] / 128.0 return ( standX ) standX = standardizeLAB ( X ) del X print ( standX . shape ) (8091, 256, 256, 3) Devide the standardized image between training and testing In [6]: split = int ( 0.95 * len ( standX )) Xtrain = standX [: split ] Xtest = standX [ split :] Define model In [7]: from keras.preprocessing.image import ImageDataGenerator , array_to_img , img_to_array , load_img from keras.layers import Conv2D , UpSampling2D , InputLayer , Conv2DTranspose from keras.models import Sequential def define_model (): #Design the neural network model = Sequential () model . add ( InputLayer ( input_shape = ( 256 , 256 , 1 ))) model . add ( Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )) model . add ( Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , strides = 2 )) model . add ( Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )) model . add ( Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , strides = 2 )) model . add ( Conv2D ( 256 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )) model . add ( Conv2D ( 256 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , strides = 2 )) model . add ( Conv2D ( 512 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )) model . add ( Conv2D ( 256 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )) model . add ( Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )) model . add ( UpSampling2D (( 2 , 2 ))) model . add ( Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )) model . add ( UpSampling2D (( 2 , 2 ))) model . add ( Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' )) model . add ( Conv2D ( 2 , ( 3 , 3 ), activation = 'tanh' , padding = 'same' )) #model.add(Conv2D(2, (3, 3), padding='same')) model . add ( UpSampling2D (( 2 , 2 ))) model . compile ( optimizer = 'rmsprop' , loss = 'mse' ) return ( model ) # Finish model # Image transformer datagen = ImageDataGenerator ( shear_range = 0.2 , zoom_range = 0.2 , rotation_range = 20 , horizontal_flip = True ) model = define_model () model . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 256, 256, 1) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 256, 256, 64) 640 _________________________________________________________________ conv2d_2 (Conv2D) (None, 128, 128, 64) 36928 _________________________________________________________________ conv2d_3 (Conv2D) (None, 128, 128, 128) 73856 _________________________________________________________________ conv2d_4 (Conv2D) (None, 64, 64, 128) 147584 _________________________________________________________________ conv2d_5 (Conv2D) (None, 64, 64, 256) 295168 _________________________________________________________________ conv2d_6 (Conv2D) (None, 32, 32, 256) 590080 _________________________________________________________________ conv2d_7 (Conv2D) (None, 32, 32, 512) 1180160 _________________________________________________________________ conv2d_8 (Conv2D) (None, 32, 32, 256) 1179904 _________________________________________________________________ conv2d_9 (Conv2D) (None, 32, 32, 128) 295040 _________________________________________________________________ up_sampling2d_1 (UpSampling2 (None, 64, 64, 128) 0 _________________________________________________________________ conv2d_10 (Conv2D) (None, 64, 64, 64) 73792 _________________________________________________________________ up_sampling2d_2 (UpSampling2 (None, 128, 128, 64) 0 _________________________________________________________________ conv2d_11 (Conv2D) (None, 128, 128, 32) 18464 _________________________________________________________________ conv2d_12 (Conv2D) (None, 128, 128, 2) 578 _________________________________________________________________ up_sampling2d_3 (UpSampling2 (None, 256, 256, 2) 0 ================================================================= Total params: 3,892,194 Trainable params: 3,892,194 Non-trainable params: 0 _________________________________________________________________ Training starts Notice that I removed the rgb2lab line from image_a_b_gen n that existed in Colorizing B&W; Photos with Neural Networks . The conversion is not necessary because our Xtrain is already converted to lab format. Also I found that removing this line from the image_a_b_gen made the training x10 faster. In [8]: # Generate training data batch_size = 128 def image_a_b_gen ( Xtrain , batch_size ): for batch in datagen . flow ( Xtrain , batch_size = batch_size ): X_batch = batch [:,:,:,[ 0 ]] Y_batch = batch [:,:,:, 1 :] yield ( X_batch , Y_batch ) ## create a validation data Ntrain = int ( Xtrain . shape [ 0 ] * 0.8 ) X_tr = Xtrain [: Ntrain ] X_val = Xtrain [ Ntrain :] hist = model . fit_generator ( image_a_b_gen ( X_tr , batch_size ), verbose = 2 , validation_data = ( X_val [:,:,:,[ 0 ]], X_val [:,:,:, 1 :]), steps_per_epoch = 100 , epochs = 5 ) Epoch 1/5 - 143s - loss: 0.0299 - val_loss: 0.0141 Epoch 2/5 - 120s - loss: 0.0146 - val_loss: 0.0139 Epoch 3/5 - 124s - loss: 0.0140 - val_loss: 0.0139 Epoch 4/5 - 122s - loss: 0.0140 - val_loss: 0.0135 Epoch 5/5 - 123s - loss: 0.0137 - val_loss: 0.0131 plot the loss In [9]: for key in hist . history . keys (): plt . plot ( hist . history [ key ], label = key ) plt . legend () plt . show () Evaluate the model performance using the test set In [10]: Ypred = model . predict ( Xtest [:,:,:,[ 0 ]]) print ( \"testing MSE={:4.3f}\" . format ( np . mean (( Ypred - Xtest [:,:,:, 2 :]) ** 2 ))) testing MSE=0.020 Plot the predicated testing image and true colored testing image Unfortunately, almost all the images are brown-ish. We need to increase the training data size, investigate different models, or consider narrower space of images. Nevertheless, the model seems to learn grass color is green sky is blue lake is blue In [11]: from copy import copy def plot_gray_predicted_true_images ( Xtest , Ypred , index , target_size ): Npic = len ( index ) fig = plt . figure ( figsize = ( 10 , Npic * 3 )) count = 1 for i in index : img = copy ( Xtest [ i ]) cur_pred = np . zeros ( target_size ) cur_gray = np . zeros ( target_size ) cur_pred [:,:, 0 ] = img [:,:, 0 ] * 100 cur_gray [:,:, 0 ] = img [:,:, 0 ] * 100 cur_pred [:,:, 1 :] = Ypred [ i ] * 128 rgb_cur_pred = lab2rgb ( cur_pred ) rgb_cur_gray = lab2rgb ( cur_gray ) ax = fig . add_subplot ( Npic , 3 , count ) ax . imshow ( rgb_cur_gray ) ax . set_title ( \"ID={} original (gray scale)\" . format ( i )) ax . axis ( \"off\" ) count += 1 ax = fig . add_subplot ( Npic , 3 , count ) ax . imshow ( rgb_cur_pred ) ax . axis ( \"off\" ) ax . set_title ( \"predicted\" ) count += 1 ## create original image img_o = np . zeros ( target_size ) img_o [:,:, 0 ] = img [:,:, 0 ] * 100 img_o [:,:, 1 :] = img [:,:, 1 :] * 128 ax = fig . add_subplot ( Npic , 3 , count ) rgb = lab2rgb ( img_o ) ax . imshow ( rgb ) ax . set_title ( \"original (color)\" ) ax . axis ( \"off\" ) count += 1 plt . show () Npic = 20 exampleIDs = [ 132 , 288 , 47 , 169 , 85 , 191 , 37 , 147 ] randomIDs = list ( np . random . choice ( range ( Xtest . shape [ 0 ]), Npic - len ( exampleIDs ))) index = exampleIDs + randomIDs plot_gray_predicted_true_images ( Xtest , Ypred , index , target_size ) In [12]: del standX Hunter x Hunter The Hunter x Hunter images are extracted from Google Image search, and the procedure is described in previous post Download all images from Google image search query using python . I used several queries for downloading the data: image - Hunter x Hunter image - Hunter x Hunter anime image - Hunter x Hunter color image - Hunter x Hunter gon image - Hunter x Hunter killua image - Hunter x Hunter aruka image - Hunter x Hunter kurapika gon, killua, aruka and kurapika are character's names. The text files containing URL links of each image are available in my Github . In [13]: from keras.preprocessing.image import img_to_array , load_img from skimage.color import rgb2lab , lab2rgb target_size = ( 256 , 256 , 3 ) ## try except is incdlued because dir_data = \"../HunterHunter/image/anime/\" X = [] count = 0 for folder in os . listdir ( dir_data ): for image in os . listdir ( dir_data + folder ): try : imgrgb = img_to_array ( load_img ( dir_data + folder + \"/\" + image , target_size = target_size )) / 255.0 imglab = rgb2lab ( imgrgb ) X . append ( imglab ) count += 1 except Exception as e : pass X = np . array ( X ) print ( \"The total number of images {}\" . format ( X . shape [ 0 ])) The total number of images 1470 Standardize the LAB data In [14]: standX = standardizeLAB ( X ) del X print ( standX . shape ) (1470, 256, 256, 3) Training the model The process is the same as the previous analysis with Flikr8K data. In [15]: #Split between training and testing data split = int ( 0.95 * len ( standX )) Xtrain = standX [: split ] Xtest = standX [ split :] ## create a validation data Ntrain = int ( Xtrain . shape [ 0 ] * 0.8 ) X_tr = Xtrain [: Ntrain ] X_val = Xtrain [ Ntrain :] model = define_model () batch_size = 128 ## we will use the inital weights from the Flickr8K analysis hist = model . fit_generator ( image_a_b_gen ( X_tr , batch_size ), verbose = 2 , validation_data = ( X_val [:,:,:,[ 0 ]], X_val [:,:,:, 1 :]), steps_per_epoch = 100 , epochs = 5 ) Epoch 1/5 - 126s - loss: 0.0722 - val_loss: 0.0345 Epoch 2/5 - 124s - loss: 0.0342 - val_loss: 0.0335 Epoch 3/5 - 122s - loss: 0.0335 - val_loss: 0.0327 Epoch 4/5 - 123s - loss: 0.0330 - val_loss: 0.0332 Epoch 5/5 - 123s - loss: 0.0327 - val_loss: 0.0319 Plot the validation loss In [16]: for key in hist . history . keys (): plt . plot ( hist . history [ key ], label = key ) plt . legend () plt . show () Model validation using test set In [17]: Ypred = model . predict ( Xtest [:,:,:,[ 0 ]]) print ( \"testing MSE={:4.3f}\" . format ( np . mean (( Ypred - Xtest [:,:,:, 2 :]) ** 2 ))) testing MSE=0.048 Plot the example images from test set Again, the model is not doing very good job coloring images. This time, all the images are purple-ish. The plot also show that there are some non-anime related images, indicating that I could do manual cleaning of the images. Nevertheless, there are some good things too: Killua's hair is colored purple-ish white correctly. Gon's face is colored with a skin color in ID=46. Super-power-looking-back ground colors seems appropriate. In [18]: Nsample = 30 index = [ 46 , 69 , 45 , 18 , 22 , 25 , 0 , 56 , 10 , 50 ] plot_gray_predicted_true_images ( Xtest , Ypred , index , target_size ) Can I color manga? Well, my model was not doing the best job coloring anime but what about coloring manga? Let's give it a try. Load 5 Hunter x Hunter manga images. In [19]: target_size = ( 256 , 256 , 3 ) ## try except is incdlued because dir_data = \"../HunterHunter/manga/Hunter x Hunter manga/\" X = [] count = 0 for image in [ 'image00004.jpg' , 'image00005.jpg' , 'image00006.jpg' , 'image00007.jpg' , 'image00008.jpg' ]: try : imgrgb = img_to_array ( load_img ( dir_data + \"/\" + image , target_size = target_size )) / 255.0 imglab = rgb2lab ( imgrgb ) X . append ( imglab ) count += 1 except Exception as e : pass X = np . array ( X ) print ( \"The total number of images {}\" . format ( X . shape [ 0 ])) ## standardization standX = standardizeLAB ( X ) #del X print ( standX . shape ) The total number of images 5 (5, 256, 256, 3) Use the previously built model with anime to predict the color of manga. In [20]: Ypred = model . predict ( standX [:,:,:,[ 0 ]]) Plot how it looks like Ah, we need to train model with more relevant images! We need a colored manga rather than anime to train a model for this purpose. In [21]: index = range ( standX . shape [ 0 ]) plot_gray_predicted_true_images ( standX , Ypred , index , target_size ) Next step: Consider incorporating pre-trained network. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Color gray scale images and manga using deep learning"},{"url":"Color-space-defenitions-in-python-RGB-and-LAB.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In this blog post, you will learn color spaces that are often used in image processing problems. More specifically, after reading the blog, you will be familiar with using skimage.color.rgb2lab skimage.color.lab2rgb keras.preprocessing.image.load_img keras.preprocessing.image.img_to_array matplotlib.pyplot.imshow Let's first load two example images using keras.preprocessing.image.load_img. The list dir_data contains the path to two jpg images. In [1]: dir_data = [ \"../Flickr8k/Flicker8k_Dataset/1012212859_01547e3f17.jpg\" , \"../Flickr8k/Flicker8k_Dataset/2554531876_5d7f193992.jpg\" ] Now let's read in the two images and save it into list. As you see from the printing, load_img returns an object of PIL.Image.Image with image mode RGB and size as a pre-specified target_size. In [2]: from keras.preprocessing.image import img_to_array , load_img target_size = ( 256 , 256 ) # Get images Ximg = [] for filename in dir_data : Ximg . append ( load_img ( filename , target_size = target_size )) print ( Ximg [ 0 ]) Using TensorFlow backend. Now using img_to_array we will convert each of the image into numpy array. In [3]: import numpy as np Xsub_rgb = [] for img in Ximg : Xsub_rgb . append ( img_to_array ( img )) print ( Xsub_rgb [ 0 ] . shape ) print ( Xsub_rgb [ 0 ]) ## convert the entire list to numpy array Xsub_rgb = np . array ( Xsub_rgb ) (256, 256, 3) [[[ 30. 24. 10.] [ 33. 27. 13.] [ 32. 26. 14.] ... [ 85. 101. 117.] [ 91. 103. 115.] [ 91. 105. 118.]] [[ 31. 25. 11.] [ 30. 24. 12.] [ 30. 26. 14.] ... [ 84. 98. 111.] [ 89. 96. 112.] [ 91. 99. 112.]] [[ 22. 18. 6.] [ 26. 24. 12.] [ 30. 28. 16.] ... [ 71. 79. 82.] [ 72. 81. 88.] [ 73. 82. 89.]] ... [[101. 71. 35.] [ 97. 62. 30.] [100. 74. 37.] ... [110. 86. 60.] [ 89. 76. 60.] [ 90. 77. 68.]] [[ 94. 60. 23.] [100. 67. 32.] [ 90. 65. 34.] ... [ 69. 60. 21.] [ 96. 75. 48.] [ 86. 70. 34.]] [[ 86. 50. 24.] [ 90. 59. 28.] [ 83. 60. 29.] ... [115. 88. 67.] [ 91. 69. 46.] [ 80. 61. 44.]]] RGB color images consist of three layers: a red layer, a green layer, and a blue layer. Each layer in a color image has a value from 0 - 255. The value 0 means that it has no color in this layer. If the value is 0 for all color channels, then the image pixel is black. As you see, all the R, G and B dimensions of the Xsub_rgb is in the range between 0 - 255. In [4]: ## Extrat the first few example images Nsample = Xsub_rgb . shape [ 0 ] def plotMinMax ( Xsub_rgb , labels = [ \"R\" , \"G\" , \"B\" ]): print ( \"______________________________\" ) for i , lab in enumerate ( labels ): mi = np . min ( Xsub_rgb [:,:,:, i ]) ma = np . max ( Xsub_rgb [:,:,:, i ]) print ( \"{} : MIN={:8.4f}, MAX={:8.4f}\" . format ( lab , mi , ma )) plotMinMax ( Xsub_rgb , labels = [ \"R\" , \"G\" , \"B\" ]) ______________________________ R : MIN= 0.0000, MAX=255.0000 G : MIN= 0.0000, MAX=255.0000 B : MIN= 0.0000, MAX=255.0000 Let's look at each layer of the images To visualize image in python, you can use matplotlib.pyplot.imshow. This function accept RGB image in standardized scale ranging between 0 and 1. Notice that my code below devide the image rgb by 255.0. To plot each layer separately, we can assign values zero to all the layers except the one of interests. In [5]: from copy import copy import matplotlib.pyplot as plt count = 1 fig = plt . figure ( figsize = ( 12 , 3 * Nsample )) for rgb in Xsub_rgb : ## This section plot the original rgb ax = fig . add_subplot ( Nsample , 4 , count ) ax . imshow ( rgb / 255.0 ); ax . axis ( \"off\" ) ax . set_title ( \"original RGB\" ) count += 1 for i , lab in enumerate ([ \"R\" , \"G\" , \"B\" ]): crgb = np . zeros ( rgb . shape ) crgb [:,:, i ] = rgb [:,:, 0 ] ax = fig . add_subplot ( Nsample , 4 , count ) ax . imshow ( crgb / 255.0 ); ax . axis ( \"off\" ) ax . set_title ( lab ) count += 1 plt . show () The color representation for image is not limited to RGB. In image colorization problem, for example, LAB is more common color space for image colorization problem. Colorizing B&W; Photos with Neural Networks says: L stands for lightness, and a and b for the color spectrums greenâ€“red and blueâ€“yellow. A Lab encoded image has one layer for grayscale and have packed three color layers into two. Using skimage.color.rgb2lab, we can easily convert the RGB image into LAB format and with skimage.color.lab2rgb we can inverse LAB back to RGB. However, you need to pay a bit attentions to its scale. The range of the dimensions for RGB and LAB in skimage.color.rgb2lab and lab2rgb are: rgb_lab:[0,1]x[0,1]x[0,1] -> [0,100] x [-128,128] x [-128,128] lab_rgb:[0,100] x [-128,128] x [-128,128] --> [0,1]x[0,1]x[0,1] As rgb2lab assumes that the RGB is standardized to range between 0 and 1, we will first devide Xsublab by 255.0. RGB -> LAB In [6]: Xsub_rgb01 = Xsub_rgb / 255.0 Then now we are ready to convert rgb image to lab. Notice that L, A, and B dimensions have different range. In [7]: from skimage.color import rgb2lab , lab2rgb Xsub_lab = rgb2lab ( Xsub_rgb01 ) plotMinMax ( Xsub_lab , labels = [ \"L\" , \"A\" , \"B\" ]) ______________________________ L : MIN= 0.0000, MAX= 99.9268 A : MIN=-33.0006, MAX= 62.7535 B : MIN=-80.2825, MAX= 71.6195 LAB -> RGB We can also inverse the process by using lab2rgb. In [8]: # lab2rgb has to have a dimension (-,-,3) Xsub_lab_rgb = np . zeros ( Xsub_lab . shape ) for i in range ( Xsub_lab . shape [ 0 ]): Xsub_lab_rgb [ i ] = lab2rgb ( Xsub_lab [ i ]) plotMinMax ( Xsub_lab_rgb . reshape (( 1 ,) + Xsub_lab_rgb . shape ), labels = [ \"R\" , \"G\" , \"B\" ]) ______________________________ R : MIN= 0.0000, MAX= 0.6667 G : MIN= 0.0000, MAX= 0.6314 B : MIN= 0.0000, MAX= 0.6392 Check if the RGB -> LAB -> RGB worked by plotting ax.imshow() takes standardized RGB images (ranging between 0 and 1). Using this function, I will plot the the original RGB image and inversed RGB image are generated are the same. In [9]: count = 1 fig = plt . figure ( figsize = ( 6 , 3 * Nsample )) for irgb , irgb2 in zip ( Xsub_rgb01 , Xsub_lab_rgb ): ax = fig . add_subplot ( Nsample , 2 , count ) ax . imshow ( irgb ); ax . axis ( \"off\" ) ax . set_title ( \"original RGB\" ) count += 1 ax = fig . add_subplot ( Nsample , 2 , count ) ax . imshow ( irgb2 ); ax . axis ( \"off\" ) ax . set_title ( \"RGB -> LAB -> RGB\" ) count += 1 plt . show () Check if the 0th dimension of the LAB image is showing the brightness. Finally, we can visualize each dimension of LAB image as follows: In [10]: def extract_single_dim_from_LAB_convert_to_RGB ( image , idim ): ''' image is a single lab image of shape (None,None,3) ''' z = np . zeros ( image . shape ) if idim != 0 : z [:,:, 0 ] = 80 ## I need brightness to plot the image along 1st or 2nd axis z [:,:, idim ] = image [:,:, idim ] z = lab2rgb ( z ) return ( z ) count = 1 fig = plt . figure ( figsize = ( 13 , 3 * Nsample )) for lab in Xsub_lab : ax = fig . add_subplot ( Nsample , 3 , count ) lab_rgb_gray = extract_single_dim_from_LAB_convert_to_RGB ( lab , 0 ) ax . imshow ( lab_rgb_gray ); ax . axis ( \"off\" ) ax . set_title ( \"L: lightness\" ) count += 1 ax = fig . add_subplot ( Nsample , 3 , count ) lab_rgb_gray = extract_single_dim_from_LAB_convert_to_RGB ( lab , 1 ) ax . imshow ( lab_rgb_gray ); ax . axis ( \"off\" ) ax . set_title ( \"A: color spectrums green to red\" ) count += 1 ax = fig . add_subplot ( Nsample , 3 , count ) lab_rgb_gray = extract_single_dim_from_LAB_convert_to_RGB ( lab , 2 ) ax . imshow ( lab_rgb_gray ); ax . axis ( \"off\" ) ax . set_title ( \"B: color spectrums blue to yellow\" ) count += 1 plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Color space definitions in python, RGB and LAB"},{"url":"Download-all-images-from-Google-image-search-query-using-python.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In this blog post, I describe how I download a lot of images from Google images. I followed pyimagesearch's blog post so please give credits to his blog. His method has two steps: Step 1: The first step is to gather URL links of the images that appear in Google Images when you enter a query. pyimagesearch's blog post did this using Java Script. After running his ~10 lines of Java Script code, you will download a text file named urls.txt, that contains the URL link of the images. Step 2: The second step is to download images from each URL using Python. I followed Step 1 and downloaded the urls.py. However, I wrote my own script for the Step 2. This was because his python scripts requires quite a few python libraries, and they are: from imutils import paths import argparse import cv2 import requests, os I could not pip install imutils in my environment so here I write a scripts that only require requests and os. First, let's take a look at the urls.txt file. I renamed this file to be \"urls - Hunter x Hunter anime.txt\". (This is because \"Hunter x Hunter anime\" was my query for Google Image. Hunter x Hunter is my favorite anime show.) If you are interested in my .txt files, I pushed it in my github . In [11]: path_text = \"urls - Hunter x Hunter anime.txt\" First let's look at the first 10 lines of the urls.txt and the number of lines. In [12]: o = open ( path_text , \"r\" ) url0 = o . read () o . close () ## list, containing downloaded files urls = url0 . split () print ( \"The number of urls: {} \" . format ( len ( urls ))) print ( \"____________________________\" ) for url in urls [: 10 ]: print ( url ) The number of urls: 614 ____________________________ http://img1.ak.crunchyroll.com/i/spire3/cbb55a6382682bf71e91f685c6473c5a1487736090_full.jpg https://geekandsundry.com/wp-content/uploads/2016/01/JPEG-Promo-1.png http://cdn1.theouterhaven.net/wp-content/uploads/2017/10/Hunter_x_Hunter.png http://media.comicbook.com/2017/11/hunter-x-hunter-1019647-1062187-1280x0.jpg https://myanimelist.cdn-dena.com/s/common/uploaded_files/1456110286-e4719dbe229cff118ffb4c6c2a05bfd6.png https://d37x086vserhlm.cloudfront.net/wp-content/uploads/2017/03/27180359/hunter-x-hunter.jpg https://i1.wp.com/eclipsemagazine.com/wp-content/uploads/2015/12/Hunter-X-Hunter.jpg https://i.ytimg.com/vi/INQTyrlurJE/hqdefault.jpg http://4hdwallpapers.com/wp-content/uploads/2013/09/Hunter-x-Hunter-Anime.jpg https://myanimelist.cdn-dena.com/s/common/uploaded_files/1484292524-ba145fd5de7c1c852334fa88ed95b0a0.jpeg Next download the images from each of the URL using requests.get(). I included the try catch as some requests fail with error messages. Runinng the following script create a folder data if it does not exist in the current directory and save images in jpg format. In [14]: import requests , os loc_data = \"./data/\" try : os . makedirs ( loc_data ) except : pass iimage = 0 for url in urls : try : f = open ( loc_data + 'image {:05.0f} .jpg' . format ( iimage ), 'wb' ) f . write ( requests . get ( url ) . content ) f . close () iimage += 1 except Exception as e : print ( \" \\n {} {} \" . format ( e , url )) pass HTTPConnectionPool(host='www.m5zn.com', port=80): Max retries exceeded with url: /uploads2/2012/2/17/photo/021712160250i32w9kaiuvv4y8iqq0bmzm25.png (Caused by NewConnectionError(' : Failed to establish a new connection: [Errno 65] No route to host',)) http://www.m5zn.com/uploads2/2012/2/17/photo/021712160250i32w9kaiuvv4y8iqq0bmzm25.png So you are done with downloading the images. You do not need the codes hearafter if your solo purpose is to downloading the data, but if you want to take a look at some of the images, here is the codes for peaking the first 9 images. In [16]: from keras.preprocessing.image import load_img import matplotlib.pyplot as plt fnames = os . listdir ( loc_data ) fig = plt . figure ( figsize = ( 10 , 10 )) count = 1 for fnm in fnames [: 9 ]: img = load_img ( loc_data + fnm , target_size = ( 400 , 400 )) ax = fig . add_subplot ( 3 , 3 , count ) count += 1 ax . imshow ( img ) ax . axis ( \"off\" ) plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Download all images from Google image search query using python"},{"url":"Semantic-model-built-with-tensorflow-deployed-with-Flask-and-Heroku-Part-2.html.html","text":"In the previous blog I created a small semantic classification model that can be developed in Heroku. The web application is hosted here in Heroku. In this short blog, I will briefly discuss my deployment process. Despite that there are many public clouds that offers GPU instances, such as Amazon Web Service , Google Cloud , I decided to go with Heroku , which does not offer GPU instances. This is because Heroku is VERY easy to set up and it allows to host at most 5 applications FOR FREE. In the previous blog , I created a simple deep learning model for sentiment analysis using tensor-flow. The model takes a tweet or short text as a sentence and return the likelihood of happiness. My focus in the post was to simplify the model so that every calculation can be done within 30 seconds and the memory stays within 500MB. Reference Deep learning semantic model built with tensorflow deployed using Flask and Heroku Part 1 (Model Development) Deep learning semantic model built with tensorflow deployed using Flask and Heroku Part 2 (Deployment) My Github repo containing Flask app Deployment As my daily job focuses on model development, I do not have as much experience in deployment (and that is the reason why I am spending weekend to write this blog). For someone like me, the learning curve may be too sharp if tring to learn everything at once. But if you take steps, every step is really easy. Here are the steps I followed to deploy deep learning model in Heroku. Step 0: Learn HTML HTML stands for HyperText Markup Language. Every webpage you look at is written in a language called HTML. You can think of HTML as the skeleton that gives every webpage structure. If you do not know about HTML, take a codeacademy's course about HTML & CSS (Free) and try your best to go as far as possible (until you get bored). Step 1: Learn Flask Flask is a micro web framework written in Python. For a quick start, follow this tutorial , it will let you create your first Flask web application and run it locally on your own computer in 10 minutes. Step 2: Learn Jinja2 jinja2 is a popular templating engine for Python. A web templating system combines a template with a certain data source to render dynamic web pages. With jinja2, you can pass variable between your python script and html. I recommend you to read Template Designer Documentation. If you are too lazy to read it, here is the quick cheat sheet for its syntax: {% ... %} for Statements {{ ... }} for Expressions to print to the template output {# ... #} for Comments not included in the template output # ... ## for Line Statements Step 3: Learn Heroku In Step 1, you already run the web application locally on your computer. With a cloud service like Heroku, you are able to put your websites onto the public World Wide Web, with a publicly accessible URL. This datademofun's github repo seems the simplest walkthrough on how to get set up with Heroku, its toolkit and then how to deploy a simple web application (for free) on the Heroku Cloud. Step 4: Learn Heroku + deep learning My Flask app related codes are created by modifying datademofun's github repo . While datademofun's Flask app only requires Flask and gunicorn, I need some additional python modules for pre processing data and to use deep learning functions. My requirements.txt contains: You see that I have tensorflow as one of the requirements.txt but not Keras, my favorite deep learning framework. This is because I could not import Keras: As the Flask application with Keras worked locally in my computer but failed in Heroku, I believe that this was Hiroku related problem. More specifically, using Heroku, I could add it as one of the requirements.txt but when I add a line \"import keras\" in backend.py, the application failed with H13 error. What is the H13 error? Here is a quote from H13 - Connection closed without response. This error is thrown when a process in your web dyno accepts a connection, but then closes the socket without writing anything to it.... One example where this might happen is when a Unicorn web server is configured with a timeout shorter than 30s and a request has not been processed by a worker before the timeout happens. In this case, Unicorn closes the connection before any data is written, resulting in an H13. I solved this problem in very hacky way: I imported Keras through tensorflow. You can see this in my backend.py: I agree this is not a very good solution but it was a quick go around!","tags":"Blog","title":"Semantic model built with tensorflow deployed with Flask and Heroku - Part 2 -"},{"url":"Semantic-model-built-with-tensorflow-deployed-with-Flask-and-Heroku-Part-1.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } Once you create a cool deep learning application, next step is to deploy it so that anyone all over the world can use your cool application. In this blog and the next blog , I will explore simple ways to deploy deep learning models into a public cloud platform. This blog will focus on the model development phase. Quick google search shows there are various public clouds that let us deploy our model: Amazon Web Service Google Cloud Heroku The great advantages of AWS and Google cloud is that they offer GPU instances. However, it requires DevOps work needed to set up a server i.e., it takes longer for setting up the server. While Heroku does not offer GPU instance, it is VERY easy to set up and host at most 5 applications FOR FREE. It seems that Heroku is a good place to learn deployment in general for the first time. So this blog will use Heroku as a tool. In the first blog post, I will create a simple deep learning model for sentiment analysis using tensor-flow. The model takes tweet or short text as a sentence and return the likelihood of happiness. Although it is relatively easy to deploy a prediction model to Heroku, this comes with some costs: for example, maximum slug size is 500 MB or a request has not been processed by a worker within 30 secounds . Because of these restrictions, my focus in this blog was to simplify the model and to reduce the size of the model weights. In the next blog , I will discuss how to deploy this model in Heroku. If you are not interested in the model development, I recommend you to jump to the second phase. To motivate the readers, click here to see my deployed web app Reference: Deep learning semantic model built with tensorflow deployed using Flask and Heroku Part 1 (Model Development) Deep learning semantic model built with tensorflow deployed using Flask and Heroku Part 2 (Deployment) Sentiment analysis model development Social media is a great place to share your thoughts about anything to anyone. Social media became a important place to learn public opinions. Especially twitter is considered one of the most important place for this purpose because of its scale, diversity of topics and public access to the contents. SemEval offers public tweet dataset that researchers can share task on Sentiment Analysis on Twitter. The task ran in 2013, 2014, 2015 and 2016, attracting over 40+ participating teams in all four editions. I will use their public labeled data to develop a model that tells the happiness level. I will first create a simgle layer LSTM model. Later, I will try transfer learning using GloVe weights. To start, I import necessary modules. In [1]: import matplotlib.pyplot as plt import sys , time , os , warnings import numpy as np import pandas as pd from collections import Counter import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"0\" #\"2,3\" GPU 2 and 3 is only visible import tensorflow as tf warnings . filterwarnings ( \"ignore\" ) print ( \"python {}\" . format ( sys . version )) print ( \"tensorflow version {}\" . format ( tf . __version__ )) def set_seed ( sd = 123 ): from numpy.random import seed from tensorflow import set_random_seed import random as rn ## numpy random seed seed ( sd ) ## core python's random number rn . seed ( sd ) ## tensor flow's random number set_random_seed ( sd ) python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] tensorflow version 1.5.0 Download data from SemEval website The data is downloaded from here and located at the following directory. In [2]: dir_data = \"../Sentiment/2017_English_final/GOLD/Subtask_A/\" In [3]: ls $ dir_data livejournal-2014test-A.tsv twitter-2014test-A.txt README.txt twitter-2015test-A.txt sms-2013test-A.tsv twitter-2015train-A.txt twitter-2013dev-A.txt twitter-2016dev-A.txt twitter-2013test-A.txt twitter-2016devtest-A.txt twitter-2013train-A.txt twitter-2016test-A.txt twitter-2014sarcasm-A.txt twitter-2016train-A.txt Load dataset I will only use the training data because only training data contains labels. In [4]: def extractData ( path_data ): file = open ( path_data , 'r' ) text = file . read () file . close () texts = text . split ( \" \\n \" ) data = [] for line in texts : cols = line . split ( \" \\t \" ) data . append ( cols ) data = pd . DataFrame ( data ) return ( data ) d = {} for i in [ 3 , 5 , 6 ]: path_data = dir_data + \"twitter-201{}train-A.txt\" . format ( i ) d [ path_data ] = extractData ( path_data ) print ( \"combine the data\" ) data = pd . concat ( d ) data = data . reset_index () data = data [[ 1 , 2 ]] data . columns = [ \"class\" , \"text\" ] ## remove NaN data = data . loc [ ~ data [ \"class\" ] . isnull (),:] combine the data Descriptive analysis The distribution of the positive, neutral and negative tweets. Roughly the same number of positive and negative tweets. In [5]: c = Counter ( data [ \"class\" ] . values ) tot = np . sum ( c . values ()) labels = [ str ( k ) + \" \" + str ( np . round ( i * 100 / tot , 3 )) + \"%\" for k , i in c . items ()] x = range ( len ( c )) plt . bar ( x , c . values ()) plt . xticks ( x , labels ) plt . show () Example tweets Let's take a look at 20 randomly selected raw tweets. In [6]: Ntweet = 20 index = np . random . choice ( range ( len ( data )), Ntweet ) for i in index : row = data . iloc [ i ,:] print ( \"{:6}: {:}\" . format ( row . iloc [ 0 ], row . iloc [ 1 ])) positive: @owyposadas see you on Monday!!! SM Bacolod!!! woooohhhh neutral: @KennethBartlet9 Rush said the other day that HRC was worst S.O.S. ever but Kerry is catching up..(may have been quoting someone) positive: We are excited to announce our next Day on the Green on Sunday January 27th - Australia Day Long Weekend! We are... http://t.co/P8BNE6NV positive: Astrology may be a blasphemous concept to some\\u002c but I am the absolute epitome of my sign. Every last detail. positive: @USC_Zack @broncopeyton @gsf_23 lmfao when May comes around\\u002c watch Zachary cheer for the Bulls or Celtics lololol positive: @jurxssicpratt i just ordered one from Amazon it'll be here on Friday let's fucking rOLL negative: \"Ask for Lil Wayne, get Michael Jackson. Bug me about Wiz Khalifa, get Guns & Roses. The point of the night is to step outside the matrix..\" neutral: Chelsea need to beat Ajax to keep up there battle with Ajax for 2nd place #NextGenSeries neutral: @Steilo_T2G lol yea I did nigga ! & I said wassup _ was tryin to see was u going up to fairground when the sun leave & shit neutral: @mac_b_from_tn yeah but we have Memorial Day\\u002c 4th of July\\u002c Cinco de Mayo\\u002c Thanksgiving\\u002c Presidents Day\\u002c MLK day\\u002c and do they do Labor Day? negative: Think imma go see Flight tomorrow.. Denzel\\u2019s the shit positive: #nowplaying Bob Marley - Sun Is Shining on Surf Shack Radio, The Best Mix of Music on the Beach neutral: Anyone going to Siberia for the HOD show on Saturday? positive: RT @JakeAustinXXX: @tinroom tonight! Come see me and the hottest gogo boys in Dallas!! @gaypornfanatic @bonepilot positive: OMG I can't wait to see what Beyonce is gonna do at the VMA'S Sunday August 28th! http://twitpic.com/62q0w9 neutral: Photo: Flatbush Zombies\\u002c Santos Party House October 17th! #ACIDALUMNI (Taken with Instagram) http://t.co/PYtQP9ej neutral: Chelsea are going to write a blank check to Juventus for Pogba tomorrow. negative: \"\\\"\"\"\"No you may not kick it.\\\"\"\"\" -Tribe Called Quest answering a text from Billy Cundiff\" neutral: Geoff Shackelford: Honda Classic Ratings Up 78%: Michael Hiestand with the http://t.co/bhTkmp2i http://t.co/lpceXHmM positive: Ravens play eagles on the 11th at 730! #GETPUMPED Give integer labels to indicate the negative, neutral and positive tweets To simplify the problem and focus only on whether the tweet is happy, I will combine the negative and neutral tweets. Negative : 0 Neutral : 0 Positive : 1 This means that the 42% of tweets are 0 and 58% are 1. In [7]: classes = [] for cl in data [ \"class\" ]: if cl == \"negative\" : classes . append ( 0 ) elif cl == \"positive\" : classes . append ( 1 ) elif cl == \"neutral\" : classes . append ( 0 ) else : print ( \"SHOULD NOT BE HERE\" ) data [ \"class\" ] = classes Text cleaning I need to admit the this text cleaning section needs serious improvement... for example, I should remove URL or hash tags. We should also combine grammatically same words, e.g. \"I've\", \"I have\", \"I hav\" should be all treated the same. Serious text cleaning will improve the model performance substantially. I should also For now, I just do: get rid of quotes from the text assign a single space between word and !. So that ! is treated as a single word. Do the same for ?. In [8]: import re from copy import copy def clean_text ( texts_original ): texts = [] for text in texts_original : otext = copy ( text ) text = otext . replace ( \"!\" , \" !\" ) . replace ( \"?\" , \" ?\" ) text = text . replace ( '\"' , \"\" ) text = text . replace ( '\"' , \"\" ) text = text . replace ( '\"' , \"\" ) if text == \"\" : print ( otext ) texts . append ( text ) return ( texts ) data [ \"text\" ] = clean_text ( data [ \"text\" ] . values ) Tokenizer Here I use Keras's preprocessing tokenizer to create a dictionary that maps word string to index ID. I will only extract the 5000 most common words and everything else is ignored. In [9]: from tensorflow.contrib.keras import preprocessing nb_words = 5000 tokenizer = preprocessing . text . Tokenizer ( nb_words ) tokenizer . fit_on_texts ( data [ \"text\" ] . values ) vocab_size = nb_words + 1 index_word = { v : k for k , v in tokenizer . word_index . items ()} print ( \"The sentence has {} unique words\" . format ( len ( np . unique ( tokenizer . word_index . keys ())))) print ( \"> vocab_size={}\" . format ( vocab_size )) The sentence has 33795 unique words > vocab_size=5001 Record tokenizer The tokenizer takes a string sentence, and create a list containing index IDs. This tokenizer also needs to be used during prediction to process the input sentence. So I need do save it. In general, tokenizer object can be saved and loaded using pickle as discussed in stack overflow : import pickle # saving with open('tokenizer.pickle', 'wb') as handle: pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL) # loading with open('tokenizer.pickle', 'rb') as handle: tokenizer = pickle.load(handle) However, this create 1.5MB pickle object. This may or may not be too large for Heroku application, which only allows 500MB memory space and 30 seconds for every backend calculation. In practice, this tokenizer function is quite simple: it just maps the word and the word index. So instead of saving the tokenizer object itself, I will just record the words in the order of ID and save it into a csv file. This file size is as small as 33KB. In [10]: mytokenizer = [] for i in range ( 1 , nb_words + 1 ): mytokenizer . append ( index_word [ i ]) pd . DataFrame ({ \"tokenizer\" : mytokenizer }) . to_csv ( \"tokenizer.csv\" , index = False ) The following code contains extract word_index dictionary and index_word dictionary from the csv. In [11]: def get_word_index_from_csv (): tokenizer = pd . read_csv ( \"tokenizer.csv\" )[ \"tokenizer\" ] . values word_index = {} index_word = {} for index , word in enumerate ( tokenizer , 1 ): word_index [ word ] = index index_word [ index ] = word return ( word_index , index_word ) word_index , index_word = get_word_index_from_csv () Now using word_index, encode each sentence. In [12]: def texts_to_sequences ( line ): out = [] for l in line . split (): llower = l . lower () if llower in word_index . keys (): out . append ( word_index [ llower ]) return ( out ) N = data . shape [ 0 ] prop_train = 0.8 Ntrain = int ( N * prop_train ) Ntest = N - Ntrain sequences , index_train , index_test = [], [], [] count = 0 for irow , line in enumerate ( data [ \"text\" ]): encoded = texts_to_sequences ( line ) sequences . append ( encoded ) if irow < Ntrain : index_train . append ( count ) else : index_test . append ( count ) count += 1 print ( 'Total Sequences: %d ' % ( len ( sequences ))) Total Sequences: 16173 Let's look at the example encoded tweets. In [13]: def print_text ( encoded ): ''' encoded : a list containing index e.g. [1, 300, 2] index_word : dictionary {0 : \"am\", 1 : \"I\", 2 : \".\",..} ''' for k in encoded : print ( \"{}({})\" . format ( index_word [ k ], k )), print ( \"\" ) set_seed ( 1 ) random_index = np . random . choice ( data . shape [ 0 ], 20 ) for irow , line in enumerate ( data [ \"text\" ] . iloc [ random_index ]): encoded = texts_to_sequences ( line ) print ( \"irow={}\" . format ( random_index [ irow ])) print_text ( encoded ) print ( \"\" ) irow=13349 was(32) having(462) trouble(3840) yesterday(386) downloading(4818) edition(1409) using(1101) google(154) loads(3527) the(1) 1st(46) page(1020) then(100) irow=235 paid(1291) a(6) few(482) for(13) a(6) jim(924) white(392) record(612) if(42) i(3) recall(2421) but(30) not(43) in(5) the(1) so(37) i(3) may(21) well(157) be(20) missing(973) the(1) irow=12172 watching(142) tomorrow(18) just(28) because(186) conor(419) mcgregor(443) and(8) faber(4685) are(45) the(1) irow=5192 going(31) to(2) be(20) here(120) tomorrow(18) have(25) lots(959) to(2) swap(2022) london(450) book(388) swap(2022) via(261) irow=16127 anyone(244) want(85) to(2) come(90) to(2) kenny(2470) chesney(3899) and(8) jason(685) at(17) metlife(405) on(4) saturday(52) i(3) have(25) an(80) extra(1029) ticket(604) and(8) by(66) extra(1029) i(3) mean(603) a(6) plus(569) one(61) irow=905 safe(896) and(8) sound(729) at(17) airport(2197) feels(1512) good(69) to(2) be(20) back(94) at(17) the(1) land(2103) of(12) rising(1502) sun(103) irow=10955 android(695) poor(1737) cousin(3438) to(2) the(1) apple(119) of(12) the(1) a(6) of(12) the(1) irow=7813 on(4) the(1) 7(158) hour(662) journey(1439) from(41) uni(3224) to(2) park(200) my(22) real(242) come(90) on(4) newcastle(1983) my(22) 1st(46) european(2447) night(33) at(17) sjp(2993) irow=2895 once(706) it(16) can(59) never(246) be(20) i(3) was(32) in(5) constitution(1145) hall(611) last(75) got(96) so(37) much(202) history(767) was(32) almost(642) lost(488) irow=5056 enter(2131) our(122) competition(1753) to(2) win(153) a(6) london(450) map(1881) puzzle(3378) just(28) follow(521) and(8) friday(47) irow=144 midnight(1695) on(4) the(1) east(979) coast(3032) which(315) means(694) its(151) birthday(147) happy(140) 20th(672) birthday(147) nick(747) j(830) irow=4225 allstar(2953) game(53) weekend(236) in(5) texas(1229) for(13) the(1) 21st(750) yea(1359) irow=7751 march(237) 16(413) luke(3480) bryan(3586) is(14) gonna(138) at(17) the(1) houston(925) i(3) have(25) to(2) its(151) a(6) must(590) irow=10989 en(2035) f(1376) we(40) fail(2291) 1st(46) we(40) chase(2248) 4(121) top(264) 4(121) finish(676) at(17) the(1) end(286) like(49) arsenal(227) irow=14844 appeared(3737) on(4) thursday(83) 3(82) at(17) the(1) 14th(692) place(247) in(5) the(1) top20(4672) of(12) irow=3462 kevin(756) rudd(1200) out(36) and(8) lives(2015) it(16) up(48) at(17) herald(2628) kevin(756) rudd(1200) out(36) irow=15641 lexus(331) 300(4285) black(159) with(19) tan(3466) wednesday(135) it(16) now(64) irow=9394 man(201) will(29) be(20) performing(680) at(17) next(110) friday(47) for(13) the(1) contact(1538) for(13) tickets(211) irow=5396 yeah(490) best(105) way(197) yeah(490) i(3) had(132) my(22) first(118) one(61) on(4) 2(65) days(278) after(99) my(22) really(160) good(69) love(92) it(16) went(280) on(4) a1(2665) yesterday(386) xx(983) irow=5374 hey(403) are(45) you(15) going(31) to(2) blue(1018) diamond(2952) tomorrow(18) and(8) are(45) you(15) driving(1758) Add zero padding to the sentence that is shorter than the maximum length. This zero padding function will be also necessary in deployment. X X has a shape (N tweets, max length of tweet) X has zero padding when the original tweet length is less than max length of tweet. In [14]: def pad_pre_sequences ( arr , maxlen ): lines = [] for iline in range ( len ( arr )): oline = arr [ iline ] lo = len ( oline ) if maxlen > lo : line = [ 0 ] * ( maxlen - lo ) + list ( oline ) else : line = oline [: maxlen ] lines . append ( line ) if len ( line ) != maxlen : print ( maxlen ) print ( line ) lines = np . array ( lines ) return ( lines ) max_length = max ([ len ( seq ) for seq in sequences ]) X = pad_pre_sequences ( sequences , maxlen = max_length ) print ( 'Max Sequence Length: %d ' % max_length ) Max Sequence Length: 33 One-hot encoding to create y y[i,0] == 1 if the i&#94;th tweet is negative or neutral otherwise 0 y[i,1] == 1 if the i&#94;th tweet is positive otherwise 0 In [15]: from keras.utils import to_categorical y = to_categorical ( data [ \"class\" ] . values , num_classes = 2 ) Using TensorFlow backend. Split between training and testing data In [16]: X_train , y_train , X_test , y_test = X [ index_train ], y [ index_train ], X [ index_test ], y [ index_test ] Model definition Here, I define a deep learning model with embedding layer + single layer LSTM followed by a single dense layer. Notice that I am using tensorflow to extract Keras's models and layers. I could have extracted these modules directly from Keras. However, somehow Heroku gave me warning when I try to import keras. Importing tensorflow did not yield the same error. This is the reason why I am using tensorflow in this blog post. In [17]: from tensorflow.contrib.keras import models from tensorflow.contrib.keras import layers def define_model ( input_length , Embedding , dim_out = 2 ): hidden_unit_LSTM = 4 main_input = layers . Input ( shape = ( input_length ,), dtype = 'int32' , name = 'main_input' ) embedding = Embedding ( main_input ) x = layers . LSTM ( hidden_unit_LSTM )( embedding ) main_output = layers . Dense ( dim_out , activation = 'softmax' )( x ) model = models . Model ( inputs = [ main_input ], outputs = [ main_output ]) # compile network model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) print ( model . summary ()) return ( model ) dim_dense_embedding = 50 Embedding1 = layers . Embedding ( vocab_size , dim_dense_embedding ) model1 = define_model ( X . shape [ 1 ], Embedding1 ) WARNING:tensorflow:From /home/fairy/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/backend.py:1456: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead WARNING:tensorflow:From /home/fairy/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= main_input (InputLayer) (None, 33) 0 _________________________________________________________________ embedding_1 (Embedding) (None, 33, 50) 250050 _________________________________________________________________ lstm_1 (LSTM) (None, 4) 880 _________________________________________________________________ dense_1 (Dense) (None, 2) 10 ================================================================= Total params: 250,940 Trainable params: 250,940 Non-trainable params: 0 _________________________________________________________________ None Training starts here: In [18]: import time start = time . time () hist1 = model1 . fit ( X_train , y_train , validation_data = ( X_test , y_test ), epochs = 10 , batch_size = 64 , verbose = 2 ) end = time . time () print ( \"Time took: {:3.2f}MIN\" . format (( end - start ) / 60.0 )) Train on 12938 samples, validate on 3235 samples Epoch 1/10 - 15s - loss: 0.6708 - acc: 0.5999 - val_loss: 0.7330 - val_acc: 0.4686 Epoch 2/10 - 13s - loss: 0.6289 - acc: 0.6450 - val_loss: 0.7690 - val_acc: 0.5168 Epoch 3/10 - 13s - loss: 0.5290 - acc: 0.7462 - val_loss: 0.7103 - val_acc: 0.5913 Epoch 4/10 - 13s - loss: 0.4512 - acc: 0.8008 - val_loss: 0.7766 - val_acc: 0.5938 Epoch 5/10 - 13s - loss: 0.4024 - acc: 0.8337 - val_loss: 0.8021 - val_acc: 0.5963 Epoch 6/10 - 13s - loss: 0.3726 - acc: 0.8508 - val_loss: 0.8317 - val_acc: 0.6031 Epoch 7/10 - 13s - loss: 0.3475 - acc: 0.8673 - val_loss: 0.8986 - val_acc: 0.5963 Epoch 8/10 - 13s - loss: 0.3235 - acc: 0.8818 - val_loss: 1.0821 - val_acc: 0.5598 Epoch 9/10 - 13s - loss: 0.3093 - acc: 0.8878 - val_loss: 0.9846 - val_acc: 0.5892 Epoch 10/10 - 13s - loss: 0.2869 - acc: 0.8987 - val_loss: 0.9319 - val_acc: 0.6003 Time took: 2.17MIN Validation loss and validation accuracies over epochs The validation loss increases very quickly. Model is overfitting. In [19]: def plot_loss ( hist1 ): for label in hist1 . history . keys (): plt . plot ( hist1 . history [ label ], label = label ) plt . legend () plt . show () plot_loss ( hist1 ) Attempt 2: GloVe pre-trained weights The model performance was not very good. Let's try to improve the model performance by transfer learning. The researchers behind GloVe method provide a suite of pre-trained word embeddings on their website released under a public domain license. See: GloVe: Global Vectors for Word Representation The smallest package of embeddings is called \"glove.6B.zip\". Weights are trained with 6 billion tokens (words) with 400,000 vocabularies. There are four dimensions of embedding vectors, i.e., 50d, 100d, 200d, and 300d. wget 'nlp.stanford.edu/data/glove.6B.zip' unzipping gives four .txt files: 164M Aug 4 2014 glove.6B.50d.txt 332M Aug 4 2014 glove.6B.100d.txt 662M Aug 4 2014 glove.6B.200d.txt 990M Aug 27 2014 glove.6B.300d.txt Reference nice example of how to use pre_trained word embedding with Keras In [20]: dim_embedding = 50 path_GloVe = \"../output/glove.6B.{}d.txt\" . format ( dim_embedding ) def GloVe_embedding ( path_GloVe ): embeddings_index = {} with open ( path_GloVe ) as f : for line in f : values = line . split () word = values [ 0 ] coefs = np . asarray ( values [ 1 :], dtype = 'float32' ) embeddings_index [ word ] = coefs print ( \"{} vocabs \" . format ( len ( embeddings_index ))) return ( embeddings_index ) GloVe_embedding = GloVe_embedding ( path_GloVe ) 400000 vocabs Let's see 50 example tokens in GloVe Some tokens contain digits, all the tokens are in lower case. In [21]: set_seed ( 10 ) isamples = np . random . choice ( len ( GloVe_embedding ), 50 ) def print_words ( words ): count = 1 for word in words : print ( \"{:20}\" . format ( word )), if count % 5 == 0 : print ( \"\" ) count += 1 print_words ( np . array ( GloVe_embedding . keys ())[ isamples ]) ksl kruszwica muzhakhoyeva oberscharfÃ¼hrer toder bagbazar frasca disengaged minicomputer embraer palcy jicheng mym casariego vestments lifejackets amirian markie lemisch cuisine rocked flatterer nilin edmonson safarova biking unfavorable tpn8 cupa cobleskill laryngology stadtmuseum colleagues hÆ°ng countervailing drillers prinn jaywalker castaÃ±os 384 wtvh first-rate tennesse alania-d mazraeh-ye hentschel pollsters eluding 2xcd lapu-lapu Now let's look at the vocabularies that do not appear in the GloVe but appear in the top 5000 most frequent words in my SemEval data. In [22]: word_not_in_GloVe = np . array ( list ( set ( word_index . keys ()) - set ( GloVe_embedding . keys ()))) Nvoc = len ( tokenizer . word_index . keys ()) print ( \"Out of {} vocabulary in original data, {} exists in the vocabulary of GloVe\" . format ( Nvoc , Nvoc - len ( word_not_in_GloVe ))) print ( \"Following {} tokens ({:4.2f}%) do not exist in GloVe!!\" . format ( len ( word_not_in_GloVe ), len ( word_not_in_GloVe ) * 100 / float ( Nvoc ))) print ( \"-\" * 100 ) isamples = np . random . choice ( len ( word_not_in_GloVe ), 50 ) print_words ( word_not_in_GloVe [ isamples ]) Out of 33795 vocabulary in original data, 33613 exists in the vocabulary of GloVe Following 182 tokens (0.54%) do not exist in GloVe!! ---------------------------------------------------------------------------------------------------- you're madonna's ibm's we've samsung's i've trndnl i'm monday's ticketek justinbieber coleswindell oneil beiber cobain's amandawanxo hahaha 30pm trndnl i'm i'm federer's hillary's gears3 tomcc we've i'm monthsary sanders' grimmie 15pm pinkcity bush's tonight's they'll xfactor hillary's mufc 00am retweet federer's isn't he'll beyonce's wasn't jindal's rella don't obama's everyone's Not all the embedding vector is necessary for training. I will simply extract the embedding vectors that appear in our tweeter data. In [23]: # prepare embedding matrix num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , dim_embedding )) count = 0 for word , i in tokenizer . word_index . items (): if i >= nb_words : continue embedding_vector = GloVe_embedding . get ( word ) if embedding_vector is None : count += 1 else : # words not found in embedding index will be all-zeros. embedding_matrix [ i ] = embedding_vector print ( \" {} tokens did not exist\" . format ( count )) 182 tokens did not exist Define the model This time, I will provide embedding matrix from GloVe and freeze the parameters. Notice that the model summary shows that there are only 890 trainable weights. These are the weights from LSTM and Dense layers. In [24]: dim_dense_embedding = embedding_matrix . shape [ 1 ] Embedding2 = layers . Embedding ( vocab_size , dim_dense_embedding , weights = [ embedding_matrix ], trainable = False ) model2 = define_model ( X . shape [ 1 ], Embedding2 ) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= main_input (InputLayer) (None, 33) 0 _________________________________________________________________ embedding_2 (Embedding) (None, 33, 50) 250050 _________________________________________________________________ lstm_2 (LSTM) (None, 4) 880 _________________________________________________________________ dense_2 (Dense) (None, 2) 10 ================================================================= Total params: 250,940 Trainable params: 890 Non-trainable params: 250,050 _________________________________________________________________ None Model training In [25]: start = time . time () hist2 = model2 . fit ( X_train , y_train , epochs = 10 , batch_size = 64 , validation_data = ( X_test , y_test ), verbose = 2 ) end = time . time () print ( \"Time took: {:3.1f}MIN\" . format (( end - start ) / 60.0 )) Train on 12938 samples, validate on 3235 samples Epoch 1/10 - 13s - loss: 0.6900 - acc: 0.5524 - val_loss: 0.7272 - val_acc: 0.4730 Epoch 2/10 - 13s - loss: 0.6645 - acc: 0.6072 - val_loss: 0.7178 - val_acc: 0.4943 Epoch 3/10 - 13s - loss: 0.6508 - acc: 0.6244 - val_loss: 0.6940 - val_acc: 0.5459 Epoch 4/10 - 13s - loss: 0.6214 - acc: 0.6567 - val_loss: 0.6883 - val_acc: 0.5861 Epoch 5/10 - 13s - loss: 0.5978 - acc: 0.6855 - val_loss: 0.6833 - val_acc: 0.5917 Epoch 6/10 - 13s - loss: 0.5831 - acc: 0.6985 - val_loss: 0.6498 - val_acc: 0.6278 Epoch 7/10 - 13s - loss: 0.5738 - acc: 0.7050 - val_loss: 0.6779 - val_acc: 0.6068 Epoch 8/10 - 12s - loss: 0.5641 - acc: 0.7143 - val_loss: 0.6725 - val_acc: 0.6207 Epoch 9/10 - 12s - loss: 0.5586 - acc: 0.7201 - val_loss: 0.6688 - val_acc: 0.6213 Epoch 10/10 - 13s - loss: 0.5537 - acc: 0.7211 - val_loss: 0.6696 - val_acc: 0.6142 Time took: 2.1MIN Validation loss and validation accuracies over epochs The validation loss decreases more by using weights from GloVe. In [26]: plot_loss ( hist2 ) Save model 2 Save the model weights. In the deployment, the model weights together with the tokenizer's word index are extracted during the prediction. In [27]: model2 . save_weights ( 'sentiment_weights.h5' ) Model validation with example tweets Hummm the happiness level makes some sense??! In the next blog post, I will deploy this model to web app. In [28]: def predict ( line , model ): encoded = texts_to_sequences ( line ) sequences = pad_pre_sequences ([ encoded ], maxlen = max_length ) probs = model . predict ( sequences )[ 0 ] ## The probability of positive tweet is recorded in the 1st position return ( probs [ 1 ]) model = define_model ( X . shape [ 1 ], Embedding1 ) model . load_weights ( 'sentiment_weights.h5' ) word_index , _ = get_word_index_from_csv () texts = [ \"I feel happy!\" , \"What a great day!\" , \"Going to work.\" , \"I broke up with my boyfriend.\" , \"happy happy\" , \"happy happy happy happy so happy\" , \"Life sucks\" , \"I want to kill myself\" , \"kill\" , \"kill hate hate hate kill\" ] for text in texts : print ( \"Prob(Happy Tweet)={:5.3f}, {:20}\" . format ( predict ( text , model2 ), text )) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= main_input (InputLayer) (None, 33) 0 _________________________________________________________________ embedding_1 (Embedding) (None, 33, 50) 250050 _________________________________________________________________ lstm_3 (LSTM) (None, 4) 880 _________________________________________________________________ dense_3 (Dense) (None, 2) 10 ================================================================= Total params: 250,940 Trainable params: 250,940 Non-trainable params: 0 _________________________________________________________________ None Prob(Happy Tweet)=0.632, I feel happy! Prob(Happy Tweet)=0.665, What a great day! Prob(Happy Tweet)=0.350, Going to work. Prob(Happy Tweet)=0.343, I broke up with my boyfriend. Prob(Happy Tweet)=0.914, happy happy Prob(Happy Tweet)=0.962, happy happy happy happy so happy Prob(Happy Tweet)=0.447, Life sucks Prob(Happy Tweet)=0.426, I want to kill myself Prob(Happy Tweet)=0.198, kill Prob(Happy Tweet)=0.073, kill hate hate hate kill if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Semantic model built with tensorflow deployed with Flask and Heroku - Part 1 -"},{"url":"Develop_an_image_captioning_deep_learning_model_using_Flickr_8K_data.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } Image captioning is an interesting problem, where you can learn both computer vision techniques and natural language processing techniques. In this blog post, I will follow How to Develop a Deep Learning Photo Caption Generator from Scratch and create an image caption generation model using Flicker 8K data. This model takes a single image as input and output the caption to this image. To evaluate the model performance, I will use bilingual evaluation understudy BLEU . For this purpose, I will review the calculation of BLEU by going through its calculation step by step. Reference How to Develop a Deep Learning Photo Caption Generator from Scratch In [1]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import sys , time , os , warnings import numpy as np import pandas as pd from collections import Counter warnings . filterwarnings ( \"ignore\" ) print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )); del keras print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"0\" set_session ( tf . Session ( config = config )) def set_seed ( sd = 123 ): from numpy.random import seed from tensorflow import set_random_seed import random as rn ## numpy random seed seed ( sd ) ## core python's random number rn . seed ( sd ) ## tensor flow's random number set_random_seed ( sd ) Using TensorFlow backend. python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.1.3 tensorflow version 1.5.0 Download the Flickr8K Dataset Flilckr8K contains 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. The images were chosen from six different Flickr groups, and tend not to contain any well-known people or locations, but were manually selected to depict a variety of scenes and situations. The dataset can be downloaded by submitting the request form . In [2]: ## The location of the Flickr8K_ photos dir_Flickr_jpg = \"../Flickr8k/Flicker8k_Dataset/\" ## The location of the caption file dir_Flickr_text = \"../Flickr8k/Flickr8k.token.txt\" jpgs = os . listdir ( dir_Flickr_jpg ) print ( \"The number of jpg flies in Flicker8k: {}\" . format ( len ( jpgs ))) The number of jpg flies in Flicker8k: 8091 Preliminary analysis Import caption data Load the text data and save it into a panda dataframe df_txt. filename : jpg file name index : unique ID for each caption for the same image caption : string of caption, all in lower case In [3]: ## read in the Flickr caption data file = open ( dir_Flickr_text , 'r' ) text = file . read () file . close () datatxt = [] for line in text . split ( ' \\n ' ): col = line . split ( ' \\t ' ) if len ( col ) == 1 : continue w = col [ 0 ] . split ( \"#\" ) datatxt . append ( w + [ col [ 1 ] . lower ()]) df_txt = pd . DataFrame ( datatxt , columns = [ \"filename\" , \"index\" , \"caption\" ]) uni_filenames = np . unique ( df_txt . filename . values ) print ( \"The number of unique file names : {}\" . format ( len ( uni_filenames ))) print ( \"The distribution of the number of captions for each image:\" ) Counter ( Counter ( df_txt . filename . values ) . values ()) The number of unique file names : 8092 The distribution of the number of captions for each image: Out[3]: Counter({5: 8092}) Let's have a look at some of the pictures together with the captions. The 5 captions for each image share many common words and very similar meaning. Some sentences finish with \".\" but not all. In [4]: from keras.preprocessing.image import load_img , img_to_array npic = 5 npix = 224 target_size = ( npix , npix , 3 ) count = 1 fig = plt . figure ( figsize = ( 10 , 20 )) for jpgfnm in uni_filenames [: npic ]: filename = dir_Flickr_jpg + '/' + jpgfnm captions = list ( df_txt [ \"caption\" ] . loc [ df_txt [ \"filename\" ] == jpgfnm ] . values ) image_load = load_img ( filename , target_size = target_size ) ax = fig . add_subplot ( npic , 2 , count , xticks = [], yticks = []) ax . imshow ( image_load ) count += 1 ax = fig . add_subplot ( npic , 2 , count ) plt . axis ( 'off' ) ax . plot () ax . set_xlim ( 0 , 1 ) ax . set_ylim ( 0 , len ( captions )) for i , caption in enumerate ( captions ): ax . text ( 0 , i , caption , fontsize = 20 ) count += 1 plt . show () Data preparation We prepare text and image data separately. Text preparation We create a new dataframe dfword to visualize distribution of the words. It contains each word and its frequency in the entire tokens in decreasing order. In [5]: def df_word ( df_txt ): vocabulary = [] for txt in df_txt . caption . values : vocabulary . extend ( txt . split ()) print ( 'Vocabulary Size: %d ' % len ( set ( vocabulary ))) ct = Counter ( vocabulary ) dfword = pd . DataFrame ({ \"word\" : ct . keys (), \"count\" : ct . values ()}) dfword = dfword . sort ( \"count\" , ascending = False ) dfword = dfword . reset_index ()[[ \"word\" , \"count\" ]] return ( dfword ) dfword = df_word ( df_txt ) dfword . head ( 3 ) Vocabulary Size: 8918 Out[5]: word count 0 a 62989 1 . 36581 2 in 18975 The most and least frequently appearing words The most common words are articles such as \"a\", or \"the\", or punctuations. These words do not have much infomation about the data. In [6]: topn = 50 def plthist ( dfsub , title = \"The top 50 most frequently appearing words\" ): plt . figure ( figsize = ( 20 , 3 )) plt . bar ( dfsub . index , dfsub [ \"count\" ]) plt . yticks ( fontsize = 20 ) plt . xticks ( dfsub . index , dfsub [ \"word\" ], rotation = 90 , fontsize = 20 ) plt . title ( title , fontsize = 20 ) plt . show () plthist ( dfword . iloc [: topn ,:], title = \"The top 50 most frequently appearing words\" ) plthist ( dfword . iloc [ - topn :,:], title = \"The least 50 most frequently appearing words\" ) In order to clean the caption, I will create three functions that: remove punctuation remove single character remove numeric characters To see how these functions work, I will process a single example string using these three functions. In [7]: import string text_original = \"I ate 1000 apples and a banana. I have python v2.7. It's 2:30 pm. Could you buy me iphone7?\" print ( text_original ) print ( \" \\n Remove punctuations..\" ) def remove_punctuation ( text_original ): text_no_punctuation = text_original . translate ( None , string . punctuation ) return ( text_no_punctuation ) text_no_punctuation = remove_punctuation ( text_original ) print ( text_no_punctuation ) print ( \" \\n Remove a single character word..\" ) def remove_single_character ( text ): text_len_more_than1 = \"\" for word in text . split (): if len ( word ) > 1 : text_len_more_than1 += \" \" + word return ( text_len_more_than1 ) text_len_more_than1 = remove_single_character ( text_no_punctuation ) print ( text_len_more_than1 ) print ( \" \\n Remove words with numeric values..\" ) def remove_numeric ( text , printTF = False ): text_no_numeric = \"\" for word in text . split (): isalpha = word . isalpha () if printTF : print ( \" {:10} : {:}\" . format ( word , isalpha )) if isalpha : text_no_numeric += \" \" + word return ( text_no_numeric ) text_no_numeric = remove_numeric ( text_len_more_than1 , printTF = True ) print ( text_no_numeric ) I ate 1000 apples and a banana. I have python v2.7. It's 2:30 pm. Could you buy me iphone7? Remove punctuations.. I ate 1000 apples and a banana I have python v27 Its 230 pm Could you buy me iphone7 Remove a single character word.. ate 1000 apples and banana have python v27 Its 230 pm Could you buy me iphone7 Remove words with numeric values.. ate : True 1000 : False apples : True and : True banana : True have : True python : True v27 : False Its : True 230 : False pm : True Could : True you : True buy : True me : True iphone7 : False ate apples and banana have python Its pm Could you buy me Clean all captions Using the three functions, I will clean all captions. In [8]: def text_clean ( text_original ): text = remove_punctuation ( text_original ) text = remove_single_character ( text ) text = remove_numeric ( text ) return ( text ) for i , caption in enumerate ( df_txt . caption . values ): newcaption = text_clean ( caption ) df_txt [ \"caption\" ] . iloc [ i ] = newcaption After cleaning, the vocabularly size get reduced by about 200. In [9]: dfword = df_word ( df_txt ) plthist ( dfword . iloc [: topn ,:], title = \"The top 50 most frequently appearing words\" ) plthist ( dfword . iloc [ - topn :,:], title = \"The least 50 most frequently appearing words\" ) Vocabulary Size: 8763 Add start and end sequence tokens In [10]: from copy import copy def add_start_end_seq_token ( captions ): caps = [] for txt in captions : txt = 'startseq ' + txt + ' endseq' caps . append ( txt ) return ( caps ) df_txt0 = copy ( df_txt ) df_txt0 [ \"caption\" ] = add_start_end_seq_token ( df_txt [ \"caption\" ]) df_txt0 . head ( 5 ) del df_txt Image preparation Create features for each image using VGG16's pre-trained networks Read in the pre-trained network. Notice that this network takes input of size (224,224,3). The output layer contains 1,000 nodes. I downloaded weights from here to my local computer at ../output/ by: wget 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5' --no-check-certificate In [11]: from keras.applications import VGG16 modelvgg = VGG16 ( include_top = True , weights = None ) ## load the locally saved weights modelvgg . load_weights ( \"../output/vgg16_weights_tf_dim_ordering_tf_kernels.h5\" ) modelvgg . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 224, 224, 3) 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 _________________________________________________________________ flatten (Flatten) (None, 25088) 0 _________________________________________________________________ fc1 (Dense) (None, 4096) 102764544 _________________________________________________________________ fc2 (Dense) (None, 4096) 16781312 _________________________________________________________________ predictions (Dense) (None, 1000) 4097000 ================================================================= Total params: 138,357,544 Trainable params: 138,357,544 Non-trainable params: 0 _________________________________________________________________ VGG16 is developed to classify images into 1,000 different classes. As I am not using VGG16 for the sake of the classification but I just need it for extracting features, I will remove the last layer from the network. In [12]: from keras import models modelvgg . layers . pop () modelvgg = models . Model ( inputs = modelvgg . inputs , outputs = modelvgg . layers [ - 1 ] . output ) ## show the deep learning model modelvgg . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 224, 224, 3) 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 _________________________________________________________________ flatten (Flatten) (None, 25088) 0 _________________________________________________________________ fc1 (Dense) (None, 4096) 102764544 _________________________________________________________________ fc2 (Dense) (None, 4096) 16781312 ================================================================= Total params: 134,260,544 Trainable params: 134,260,544 Non-trainable params: 0 _________________________________________________________________ Images In [13]: from keras.preprocessing.image import load_img , img_to_array from keras.applications.vgg16 import preprocess_input from collections import OrderedDict images = OrderedDict () npix = 224 target_size = ( npix , npix , 3 ) data = np . zeros (( len ( jpgs ), npix , npix , 3 )) for i , name in enumerate ( jpgs ): # load an image from file filename = dir_Flickr_jpg + '/' + name image = load_img ( filename , target_size = target_size ) # convert the image pixels to a numpy array image = img_to_array ( image ) nimage = preprocess_input ( image ) y_pred = modelvgg . predict ( nimage . reshape ( ( 1 ,) + nimage . shape [: 3 ])) images [ name ] = y_pred . flatten () Visualization of the VGG16 features For each image, 4096 features are created. As I cannot visualize the 4096 dimensional space, I will create 2 dimentional representation of the space using PCA and visualize the distribution of the sample images. In [14]: from sklearn.decomposition import PCA encoder = np . array ( images . values ()) pca = PCA ( n_components = 2 ) y_pca = pca . fit_transform ( encoder ) Do photo features make sense? I manually selected similar images that are creating clusters. Namely I created red, green, magenta, blue, yellow and purple clusters. From each cluster, I plotted the original images. As you see, images from the same clusters tend to be very similar: red: many people are in one image green: dogs on green, yard or on bed magenta: dogs in snow or with water splash blue: guys doing sports with helmets yellow: not sure what these are?? I see many pictures are densely clustered around this area and Photo features seem to make sense! In [15]: ## some selected pictures that are creating clusters picked_pic = OrderedDict () picked_pic [ \"red\" ] = [ 1502 , 5298 , 2070 , 7545 , 1965 ] picked_pic [ \"green\" ] = [ 746 , 7627 , 6640 , 2733 , 4997 ] picked_pic [ \"magenta\" ] = [ 5314 , 5879 , 310 , 5303 , 3784 ] picked_pic [ \"blue\" ] = [ 4644 , 4209 , 7326 , 7290 , 4394 ] picked_pic [ \"yellow\" ] = [ 5895 , 9 , 27 , 62 , 123 ] picked_pic [ \"purple\" ] = [ 5087 ] fig , ax = plt . subplots ( figsize = ( 15 , 15 )) ax . scatter ( y_pca [:, 0 ], y_pca [:, 1 ], c = \"white\" ) for irow in range ( y_pca . shape [ 0 ]): ax . annotate ( irow , y_pca [ irow ,:], color = \"black\" , alpha = 0.5 ) for color , irows in picked_pic . items (): for irow in irows : ax . annotate ( irow , y_pca [ irow ,:], color = color ) ax . set_xlabel ( \"pca embedding 1\" , fontsize = 30 ) ax . set_ylabel ( \"pca embedding 2\" , fontsize = 30 ) plt . show () ## plot of images fig = plt . figure ( figsize = ( 16 , 20 )) count = 1 for color , irows in picked_pic . items (): for ivec in irows : name = jpgs [ ivec ] filename = dir_Flickr_jpg + '/' + name image = load_img ( filename , target_size = target_size ) ax = fig . add_subplot ( len ( picked_pic ), 5 , count , xticks = [], yticks = []) count += 1 plt . imshow ( image ) plt . title ( \"{} ({})\" . format ( ivec , color )) plt . show () Link the text and image data In this dataset, a single image has 5 captions. I will only use one caption out of 5 for simplicity. Each row of the dtexts and dimages contain the same info. Remove captions (or images) that do not have corresponding images (or captions). In [16]: dimages , keepindex = [],[] df_txt0 = df_txt0 . loc [ df_txt0 [ \"index\" ] . values == \"0\" ,: ] for i , fnm in enumerate ( df_txt0 . filename ): if fnm in images . keys (): dimages . append ( images [ fnm ]) keepindex . append ( i ) fnames = df_txt0 [ \"filename\" ] . iloc [ keepindex ] . values dcaptions = df_txt0 [ \"caption\" ] . iloc [ keepindex ] . values dimages = np . array ( dimages ) Tokenizer Change character vector to integer vector using Tokenizer In [17]: from keras.preprocessing.text import Tokenizer ## the maximum number of words in dictionary nb_words = 8000 tokenizer = Tokenizer ( nb_words = nb_words ) tokenizer . fit_on_texts ( dcaptions ) vocab_size = len ( tokenizer . word_index ) + 1 print ( \"vocabulary size : {}\" . format ( vocab_size )) dtexts = tokenizer . texts_to_sequences ( dcaptions ) print ( dtexts [: 5 ]) vocabulary size : 4476 [[1, 38, 3, 66, 144, 7, 124, 52, 406, 9, 367, 3, 24, 2351, 522, 2], [1, 12, 8, 5, 752, 8, 17, 368, 2], [1, 48, 15, 170, 3, 584, 101, 3, 41, 9, 551, 1198, 11, 55, 213, 3, 1076, 2], [1, 10, 621, 6, 150, 27, 23, 8, 101, 46, 112, 2], [1, 10, 3, 24, 82, 96, 1199, 19, 162, 2]] Split between training and testing data In [18]: prop_test , prop_val = 0.2 , 0.2 N = len ( dtexts ) Ntest , Nval = int ( N * prop_test ), int ( N * prop_val ) def split_test_val_train ( dtexts , Ntest , Nval ): return ( dtexts [: Ntest ], dtexts [ Ntest : Ntest + Nval ], dtexts [ Ntest + Nval :]) dt_test , dt_val , dt_train = split_test_val_train ( dtexts , Ntest , Nval ) di_test , di_val , di_train = split_test_val_train ( dimages , Ntest , Nval ) fnm_test , fnm_val , fnm_train = split_test_val_train ( fnames , Ntest , Nval ) The maximume length of captions In [19]: maxlen = np . max ([ len ( text ) for text in dtexts ]) The final preprocessing so that the data can be used as input and output of the Keras model. In [20]: from keras.preprocessing.sequence import pad_sequences from keras.utils import to_categorical def preprocessing ( dtexts , dimages ): N = len ( dtexts ) print ( \"# captions/images = {}\" . format ( N )) assert ( N == len ( dimages )) Xtext , Ximage , ytext = [],[],[] for text , image in zip ( dtexts , dimages ): for i in range ( 1 , len ( text )): in_text , out_text = text [: i ], text [ i ] in_text = pad_sequences ([ in_text ], maxlen = maxlen ) . flatten () out_text = to_categorical ( out_text , num_classes = vocab_size ) Xtext . append ( in_text ) Ximage . append ( image ) ytext . append ( out_text ) Xtext = np . array ( Xtext ) Ximage = np . array ( Ximage ) ytext = np . array ( ytext ) print ( \" {} {} {}\" . format ( Xtext . shape , Ximage . shape , ytext . shape )) return ( Xtext , Ximage , ytext ) Xtext_train , Ximage_train , ytext_train = preprocessing ( dt_train , di_train ) Xtext_val , Ximage_val , ytext_val = preprocessing ( dt_val , di_val ) # pre-processing is not necessary for testing data #Xtext_test, Ximage_test, ytext_test = preprocessing(dt_test,di_test) # captions/images = 4855 (49631, 30) (49631, 4096) (49631, 4476) # captions/images = 1618 (16353, 30) (16353, 4096) (16353, 4476) Model This model takes two inputs: 4096-dimensional image features from pre-trained VGG model tokenized captions up to $t$th word. The single output is: tokenized $t+1$th word of caption Prediction Given the caption prediction up to the $t$th word, the model can predict the $t+1$st word in the caption, and then the input caption can be augmented with the predicted word to contain the caption up to the $t+1$th word. The augmented caption up to the $t+1$st word can, in turn, be used as input to predict the $t+2$nd word in caption. The process is repeated until the \"endseq\" is predicted. A bit more detail: [Image] 4096-dimensional image features from pre-trained VGG model The image feature is passed to fully connected layer with 256 hidden units. [Caption up to $t$] tokenized captions up to $t$th word. The tokenized caption up to $t$th time point is passed to embedding layer where each word is represented with a \"dim_embedding\" dimensional vector. This means that a single caption is represented as many time series of length \"max_len\". The time series are passed to LSTM with 256 hidden states, and then a single output at the final time point is passed to the higher layer. The networks are merged by simply adding the two vectors of size 256. The vector of length 256 is passed to two dense layers and the final dense layer return probability that the $t+1$st word is $k$th word in the vocabulary ($k=1,...,4476$). In [21]: from keras import layers print ( vocab_size ) ## image feature dim_embedding = 64 input_image = layers . Input ( shape = ( Ximage_train . shape [ 1 ],)) fimage = layers . Dense ( 256 , activation = 'relu' , name = \"ImageFeature\" )( input_image ) ## sequence model input_txt = layers . Input ( shape = ( maxlen ,)) ftxt = layers . Embedding ( vocab_size , dim_embedding , mask_zero = True )( input_txt ) ftxt = layers . LSTM ( 256 , name = \"CaptionFeature\" )( ftxt ) ## combined model for decoder decoder = layers . add ([ ftxt , fimage ]) decoder = layers . Dense ( 256 , activation = 'relu' )( decoder ) output = layers . Dense ( vocab_size , activation = 'softmax' )( decoder ) model = models . Model ( inputs = [ input_image , input_txt ], outputs = output ) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' ) print ( model . summary ()) 4476 __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_3 (InputLayer) (None, 30) 0 __________________________________________________________________________________________________ embedding_1 (Embedding) (None, 30, 64) 286464 input_3[0][0] __________________________________________________________________________________________________ input_2 (InputLayer) (None, 4096) 0 __________________________________________________________________________________________________ CaptionFeature (LSTM) (None, 256) 328704 embedding_1[0][0] __________________________________________________________________________________________________ ImageFeature (Dense) (None, 256) 1048832 input_2[0][0] __________________________________________________________________________________________________ add_1 (Add) (None, 256) 0 CaptionFeature[0][0] ImageFeature[0][0] __________________________________________________________________________________________________ dense_1 (Dense) (None, 256) 65792 add_1[0][0] __________________________________________________________________________________________________ dense_2 (Dense) (None, 4476) 1150332 dense_1[0][0] ================================================================================================== Total params: 2,880,124 Trainable params: 2,880,124 Non-trainable params: 0 __________________________________________________________________________________________________ None Model training In [22]: # fit model start = time . time () hist = model . fit ([ Ximage_train , Xtext_train ], ytext_train , epochs = 5 , verbose = 2 , batch_size = 64 , validation_data = ([ Ximage_val , Xtext_val ], ytext_val )) end = time . time () print ( \"TIME TOOK {:3.2f}MIN\" . format (( end - start ) / 60 )) Train on 49631 samples, validate on 16353 samples Epoch 1/5 - 60s - loss: 5.3167 - val_loss: 4.7659 Epoch 2/5 - 58s - loss: 4.3323 - val_loss: 4.4667 Epoch 3/5 - 58s - loss: 3.8796 - val_loss: 4.3870 Epoch 4/5 - 58s - loss: 3.5683 - val_loss: 4.3412 Epoch 5/5 - 58s - loss: 3.3066 - val_loss: 4.4118 TIME TOOK 4.90MIN In [23]: print ( Ximage_train . shape , Xtext_train . shape , ytext_train . shape ) ((49631, 4096), (49631, 30), (49631, 4476)) Validation loss and training loss over epochs The model over fit very quickly. This makes sense considering the small size of our data. In [24]: for label in [ \"loss\" , \"val_loss\" ]: plt . plot ( hist . history [ label ], label = label ) plt . legend () plt . xlabel ( \"epochs\" ) plt . ylabel ( \"loss\" ) plt . show () Prediction Prediction of testing image makes sense! In [25]: index_word = dict ([( index , word ) for word , index in tokenizer . word_index . items ()]) def predict_caption ( image ): ''' image.shape = (1,4462) ''' in_text = 'startseq' for iword in range ( maxlen ): sequence = tokenizer . texts_to_sequences ([ in_text ])[ 0 ] sequence = pad_sequences ([ sequence ], maxlen ) yhat = model . predict ([ image , sequence ], verbose = 0 ) yhat = np . argmax ( yhat ) newword = index_word [ yhat ] in_text += \" \" + newword if newword == \"endseq\" : break return ( in_text ) npic = 5 npix = 224 target_size = ( npix , npix , 3 ) count = 1 fig = plt . figure ( figsize = ( 10 , 20 )) for jpgfnm , image_feature in zip ( fnm_test [: npic ], di_test [: npic ]): ## images filename = dir_Flickr_jpg + '/' + jpgfnm image_load = load_img ( filename , target_size = target_size ) ax = fig . add_subplot ( npic , 2 , count , xticks = [], yticks = []) ax . imshow ( image_load ) count += 1 ## captions caption = predict_caption ( image_feature . reshape ( 1 , len ( image_feature ))) ax = fig . add_subplot ( npic , 2 , count ) plt . axis ( 'off' ) ax . plot () ax . set_xlim ( 0 , 1 ) ax . set_ylim ( 0 , 1 ) ax . text ( 0 , 0.5 , caption , fontsize = 20 ) count += 1 plt . show () Bilingual evaluation understudy (BLEU) I want to evaluate the model performance in the prediction. Is there any good metric? BLEU is a well-acknowledged metric to measure the similarly of one hypothesis sentence to multiple reference sentences. Given a single hypothesis sentence and multiple reference sentences, it returns value between 0 and 1. The metric close to 1 means that the two are very similar. The metric was introduced in 2002 BLEU: a Method for Automatic Evaluation of Machine Translation . Although there are many problems in this metric, for example grammatical correctness are not taken into account, BLEU is very well accepted partly because it is easy to calculate. Basic idea of BLEU The authors of the paper say that: The primary programming task for a BLEU implementor is to compare n-grams of the candidate with the n-grams of the reference translation and count the number of matches. These matches are position-independent. The more the matches, the better the candidate translation is. For example let's consider the scenario where I want to compare the similarly between the hypothesis sentence and the reference sentence. The hypothesis sentence is I like dogs and the reference sentence is: I do like dogs From the hypothesis sentence, I can create 2 2-grams: (I, like) (like, dogs) From the reference sentence, I can create 3 2-grams: (I, do) (do, like) (like, dogs) Which 2-gram from hypothesis exists in reference? The reference sentence has (like, dog) that is one of the 2-gram from hypothesis. This means that out of the two 2-grams from hypothesis, 1 exists in the reference i.e., 50% of 2-gram from the hypothesis exists in reference. This 50% is what's called \"modified precision\". This modified precision has the problem that when the number of words is larger than the hypothesis length, then the modified precision tends to be larger. For example, if the reference sentence contains 10000 words and its sub-sentence happened to contain \"I do like dogs\" sentence, this also gives the modified precision of 50%. However, the reference sentence and the hypothesis sentence should be considered less similar. To account for this, BLEU calculation penalizes the long reference sentence. The penalty is simply $$ exp\\left( 1- \\frac{\\textrm{lengh of reference} }{\\textrm{length of hypothesis} } \\right) $$ Modified precision is multiplied by this reference factor to yield 2-gram BLEU. In this particular example, the penalty is $exp(1 - 4/3)=0.717$ so the 2-gram BLEU is 0.717*0.5 = 0.3585. Finally, 1-gram BLEU, 2-gram BLEU, 3-gram BLEU and 4-gram BLEU are calculated and its average is reported as BLEU. Implementation BLEU score calculation is implemented in nltk.util and the source code is available . I found this source code is the easiest way to understand this metric. So I will go over the script to understand how the calculation works. Let's introduce one hypothesis string and one reference string, and see how similar they are according to BLEU. In [26]: hypothesis = \"I like dog\" hypothesis = hypothesis . split () reference = \"I do like dog\" references = [ reference . split ()] ## references must be a list containing list. According to BLEU the two sentences are reasonably similar. In [27]: from nltk.translate.bleu_score import sentence_bleu print ( \"BLEU={:4.3f}\" . format ( sentence_bleu ( references , hypothesis ))) BLEU=0.603 If I change the hypothesis sentence a bit, this worsen the BLEU. In [28]: hypothesis2 = \"I love dog!\" . split () print ( \"BLEU={:4.3f}\" . format ( sentence_bleu ( references , hypothesis2 ))) BLEU=0.544 2-gram BLEU calculation step by step The main calculation of BLEU is done in modified_precision(references, hypothesis, n) . So Let's try to understand this method. As the first step, given \"n\" of n-gram, compute n-gram and count the frequency of each n-gram. In [29]: from nltk.util import ngrams n = 2 # Extracts all ngrams in hypothesis # Set an empty Counter if hypothesis is empty. counts = Counter ( ngrams ( hypothesis , n )) if len ( hypothesis ) >= n else Counter () # Extract a union of references' counts. counts Out[29]: Counter({('I', 'like'): 1, ('like', 'dog'): 1}) max_counts is a dictionary containing the same key as counts; i.e. the n-gram from the hypothesis is a key. It records how many of the ngram from hypothesis exists in each of the reference sentences. In [30]: max_counts = {} for reference in references : reference_counts = Counter ( ngrams ( reference , n )) if len ( reference ) >= n else Counter () for ngram in counts : ## ngram from hypothesis max_counts [ ngram ] = max ( max_counts . get ( ngram , 0 ), reference_counts [ ngram ]) max_counts Out[30]: {('I', 'like'): 0, ('like', 'dog'): 1} Modified precision is: In [31]: # Assigns the intersection between hypothesis and references' counts. clipped_counts = { ngram : min ( count , max_counts [ ngram ]) for ngram , count in counts . items ()} numerator = sum ( clipped_counts . values ()) # Ensures that denominator is minimum 1 to avoid ZeroDivisionError. # Usually this happens when the ngram order is > len(reference). denominator = max ( 1 , sum ( counts . values ())) modified_precision = numerator / float ( denominator ) print ( modified_precision ) 0.5 Compute the penalty: In [32]: ref_len = len ( reference ) hyp_len = float ( len ( hypothesis )) brevity_penalty = np . exp ( 1 - ref_len / hyp_len ) print ( \"reference length = {:1.0f}, hypothesis length = {:1.0f}, penalty = {:4.3f}\" . format ( ref_len , hyp_len , brevity_penalty )) reference length = 4, hypothesis length = 3, penalty = 0.717 2-gram BLEU is: In [33]: brevity_penalty * modified_precision Out[33]: 0.35826565528689464 We can also calculate the 2-gram BLEU using the sentence_bleu function by setting only the second position in weight as 1. In [34]: print ( \"2-gram result:{}\" . format ( sentence_bleu ( references , hypothesis , weights = [ 0 , 1 , 0 , 0 ]))) 2-gram result:0.358265655287 Back to image captioning problem Now we understand what BLEU does and we are ready to calculate the BLEU for our test set. In [35]: index_word = dict ([( index , word ) for word , index in tokenizer . word_index . items ()]) nkeep = 5 pred_good , pred_bad , bleus = [], [], [] count = 0 for jpgfnm , image_feature , tokenized_text in zip ( fnm_test , di_test , dt_test ): count += 1 if count % 200 == 0 : print ( \" {:4.2f} % i s done..\" . format ( 100 * count / float ( len ( fnm_test )))) caption_true = [ index_word [ i ] for i in tokenized_text ] caption_true = caption_true [ 1 : - 1 ] ## remove startreg, and endreg ## captions caption = predict_caption ( image_feature . reshape ( 1 , len ( image_feature ))) caption = caption . split () caption = caption [ 1 : - 1 ] ## remove startreg, and endreg bleu = sentence_bleu ([ caption_true ], caption ) bleus . append ( bleu ) if bleu > 0.7 and len ( pred_good ) < nkeep : pred_good . append (( bleu , jpgfnm , caption_true , caption )) elif bleu < 0.3 and len ( pred_bad ) < nkeep : pred_bad . append (( bleu , jpgfnm , caption_true , caption )) 12.36% is done.. 24.72% is done.. 37.08% is done.. 49.44% is done.. 61.80% is done.. 74.17% is done.. 86.53% is done.. 98.89% is done.. The mean BLEU value for testing data In [36]: print ( \"Mean BLEU {:4.3f}\" . format ( np . mean ( bleus ))) Mean BLEU 0.370 Plot the images with good captions (BLEU > 0.9) and bad captions (BLEU < 0.1) In [37]: def plot_images ( pred_bad ): def create_str ( caption_true ): strue = \"\" for s in caption_true : strue += \" \" + s return ( strue ) npix = 224 target_size = ( npix , npix , 3 ) count = 1 fig = plt . figure ( figsize = ( 10 , 20 )) npic = len ( pred_bad ) for pb in pred_bad : bleu , jpgfnm , caption_true , caption = pb ## images filename = dir_Flickr_jpg + '/' + jpgfnm image_load = load_img ( filename , target_size = target_size ) ax = fig . add_subplot ( npic , 2 , count , xticks = [], yticks = []) ax . imshow ( image_load ) count += 1 caption_true = create_str ( caption_true ) caption = create_str ( caption ) ax = fig . add_subplot ( npic , 2 , count ) plt . axis ( 'off' ) ax . plot () ax . set_xlim ( 0 , 1 ) ax . set_ylim ( 0 , 1 ) ax . text ( 0 , 0.7 , \"true:\" + caption_true , fontsize = 20 ) ax . text ( 0 , 0.4 , \"pred:\" + caption , fontsize = 20 ) ax . text ( 0 , 0.1 , \"BLEU: {}\" . format ( bleu ), fontsize = 20 ) count += 1 plt . show () print ( \"Bad Caption\" ) plot_images ( pred_bad ) print ( \"Good Caption\" ) plot_images ( pred_good ) Bad Caption Good Caption if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Develop an image captioning deep learning model using Flickr 8K data"},{"url":"Stateful-LSTM-model-training-in-Keras.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In the previous post, titled Extract weights from Keras's LSTM and calcualte hidden and cell states , I discussed LSTM model. In this blog post, I would like to discuss the stateful flag in Keras's recurrent model. If you google \"stateful LSTM\" or \"stateful RNN\", google shows many blog posts discussing and puzzling about this notorious parameter, e.g.: Don't use stateful LSTM unless you know what it does Simple stateful LSTM example Keras - stateful vs stateless LSTMs Convert LSTM model from stateless to stateful I hope to give some understanding of stateful prediction through this blog. Stateful flag is Keras All the RNN or LSTM models are stateful in theory . These models are meant to remember the entire sequence for prediction or classification tasks. However, in practice, you need to create a batch to train a model with backprogation algorithm, and the gradient can't backpropagate between batches. This means that if you have a long time series which does not fit into a single batch, you need to divide the time series into multiple sub-time series and each sub time series goes to separate batch. Then LSTM only remember what happened within a batch. At the initial time point of every batch, states are initialized and set to 0. No previous information. This is very unfortunate because RNN or LSTM are introduced to remember all the past history to predict the next time point. Nope, it only remembers what happened within the batch time series length! Stateful flag in Keras is introduced to circumvent these problems during training, and make the model remember what happened in the previous batch by passing states from the previous batch to the next batch. Also, stateful = True makes a lot of sense during the prediction phase because otherwise the RNN trained with batch time series length $T$ assumes that the hidden states are initialized to zero at every $T$ steps. This means that in order to predict the value at time point $t$, we need to feed foward the network assuming that $h_{t-T}=0$ for $T$ times and in order to predict the value at time point $t+1$, we AGAIN need to feed forward the network for $T$ times assuming that $h_{t-T+1}=0$. This point was discussed greatly in my previous blog. History of Stateful flag Training with Stateful = True Here, I introduce two nice blogs discussing stateful flag. Philippe Remy's blog post provided nice introduction to understand the stateful flag of Keras's RNN model. It discusses how to train a model with stateful = True. However, although I like this blog post, it also contributed for the confusion because the example uses batch_input_shape=(1, 1, 1) which simply does not work for most of the time series examples even when you train a model with stateful = True. This means that your batch only contains a single time point from a single time series. Jason Brownlee's blog is another great blog post that gives good stateful examples. Jason says: \" [Stateful = True] is truly how the LSTM networks are intended to be used. We find that by allowing the network itself to learn the dependencies between the characters, that we need a smaller network (half the number of units) and fewer training epochs (almost half). \" As he only try fitting stateful LSTM to a small simple example, I cannot generalize his comment, but it is nice to learn that training with stateful=True works for his example. In this blog post, I would like to also discuss the training with stateful = True. Training with Stateful = False and Prediction with Stateful = True. The usefulness of stateful method during the prediction is also discussed. There are lots of discussions online that try to use stateful = True only during the prediction phase: \" Why anybody would want to have a stateful model during training is beyond me, so this part I can agree with. But during testing, when you want to let the model predict some output on some data, then stateful makes a lot more sense. For example, it might be a part of a larger system that works on video frames. It might be required to perform some action instantly after each frame, instead of waiting for a sufficiently long sequence of video frames before being fed to the network. It would be really nice if you could train the network stateless with a time-depth of X (say 16), and then use those weights on a stateful network with a time-depth of 1 during prediction. In my experience however, this does not work in Keras. \" -- ahrnbom commented on Sep 19 2017 \" The length of my input sequences is variable and has a quite high variance (from very short sequences to nearly 1000 long sequences). At training time I can just divide the sequences in batches of fixed sizes but at test time it would be useful to feed the whole sequence and possibly get prediction at every time step.Is it possible to train the model in stateless mode feeding fixed length sequences and then make predictions in a stateful fashion? Does this even make sense or am I missing something? \"-- fedebecat commented on Mar 11 2017 I was also fascinated with these ideas. In fact, writing scripts for stateful training is a bit cumbersome because you have to reset sequence by yourself. However, after what I have seen in my previous post titled Understand Keras's RNN behind the scenes with a sin wave example - Stateful and Stateless prediction - , I am very skeptical about this. Remember, stateful prediction and stateless prediction returns different results when model is trained stateless! And it was such a simple data (sin wave). Therefore, in this blog post, I will train model in stateful setting and show how the results are different from a model trained in stateless setting. Reference in this blog Understand Keras's RNN behind the scenes with a sin wave example - Stateful and Stateless prediction - Extract weights from Keras's LSTM and calcualte hidden and cell states In [121]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import sys , time import numpy as np import seaborn as sns import warnings warnings . filterwarnings ( \"ignore\" ) print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )) print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"0\" set_session ( tf . Session ( config = config )) def set_seed ( sd = 123 ): from numpy.random import seed from tensorflow import set_random_seed import random as rn ## numpy random seed seed ( sd ) ## core python's random number rn . seed ( sd ) ## tensor flow's random number set_random_seed ( sd ) python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.1.3 tensorflow version 1.5.0 I will create a synthetic long time series data just as in Extract weights from Keras's LSTM and calcualte hidden and cell states . But this time, I will make the data more complex by using much larger $D$. Create synthetic long time series data I will generate a time series $X_{t}$ ($t=1,...,T$) as my independent feature. As the target, or dependent time series, I will create a time series $Y_{t}$ as a function of a single time series $\\{ X_{k} \\}_{k=1}&#94;t$. Given integers $D$ and $T$, the time series $X_{t}$ an $Y_t$ are generated as: $$ C \\sim \\textrm{Multinomial}(5,6,...,99)\\\\ U \\sim \\textrm{Unif}([0,1])\\\\ X_{t} = -\\frac{t}{T}U\\left[ \\textrm{Cos}\\left(\\frac{t}{1+C}\\right) \\right]\\\\ Y_{t} = X_{t-2} X_{t-\\textrm{D}} \\textrm{ for t > D else } Y_t = 0 $$ We consider a long time series $T = 1,000$. The parameter $D$ determines the time lag. The larger $D$ is, the longer it takes for the $Y_{t}$ to show the effect of $X_{t}$, and the longer memory that the deep learning model needs to remember. For this exercise, I will consider $D=100$, and generate 1,000 sets of time series, independently. Generate training data In [122]: def random_sample ( len_ts = 3000 , D = 1001 ): c_range = range ( 5 , 100 ) c1 = np . random . choice ( c_range ) u = np . random . random ( 1 ) const = - 1.0 / len_ts ts = np . arange ( 0 , len_ts ) x1 = np . cos ( ts / float ( 1.0 + c1 )) x1 = x1 * ts * u * const y1 = np . zeros ( len_ts ) for t in range ( D , len_ts ): ## the output time series depend on input as follows: y1 [ t ] = x1 [ t - 2 ] * x1 [ t - D ] y = np . array ([ y1 ]) . T X = np . array ([ x1 ]) . T return y , X def generate_data ( D = 1001 , Nsequence = 1000 , T = 4000 , seed = 123 ): X_train = [] y_train = [] set_seed ( sd = seed ) for isequence in range ( Nsequence ): y , X = random_sample ( T , D = D ) X_train . append ( X ) y_train . append ( y ) return np . array ( X_train ), np . array ( y_train ) D = 100 T = 1000 X , y = generate_data ( D = D , T = T , Nsequence = 1000 ) print ( X . shape , y . shape ) ((1000, 1000, 1), (1000, 1000, 1)) Plot examples of the generated time series Notice that in every time series, the later 500 seconds are more variable. In [160]: def plot_examples ( X , y , ypreds = None , nm_ypreds = None ): fig = plt . figure ( figsize = ( 16 , 10 )) fig . subplots_adjust ( hspace = 0.32 , wspace = 0.15 ) count = 1 n_ts = 16 for irow in range ( n_ts ): ax = fig . add_subplot ( n_ts / 4 , 4 , count ) ax . set_ylim ( - 0.5 , 0.5 ) ax . plot ( X [ irow ,:, 0 ], \"--\" , label = \"x1\" ) ax . plot ( y [ irow ,:,:], label = \"y\" , linewidth = 3 , alpha = 0.5 ) ax . set_title ( \"{:}th time series sample\" . format ( irow )) if ypreds is not None : for ypred , nm in zip ( ypreds , nm_ypreds ): ax . plot ( ypred [ irow ,:,:], label = nm ) count += 1 plt . legend () plt . show () if ypreds is not None : for y_pred , nm_ypred in zip ( ypreds , nm_ypreds ): loss = np . mean ( ( y_pred [:, D :,:] . flatten () - y [:, D :,:] . flatten ()) ** 2 ) print ( \"The final validation loss of {} is {:7.6f}\" . format ( nm_ypred , loss )) plot_examples ( X , y , ypreds = None , nm_ypreds = None ) Split between training and testing Define weights just like Extract weights from Keras's LSTM and calcualte hidden and cell states . In [124]: prop_train = 0.8 ntrain = int ( X . shape [ 0 ] * prop_train ) w = np . zeros ( y . shape [: 2 ]) w [:, D :] = 1 w_train = w X_train , X_val = X [: ntrain ], X [ ntrain :] y_train , y_val = y [: ntrain ], y [ ntrain :] w_train , w_val = w [: ntrain ], w [ ntrain :] Define LSTM model (stateful and stateless) Now I define stateful LSTM model. In [125]: from keras import models , layers def define_model ( len_ts , hidden_neurons = 10 , nfeature = 1 , batch_size = None , stateful = False ): in_out_neurons = 1 inp = layers . Input ( batch_shape = ( batch_size , len_ts , nfeature ), name = \"input\" ) rnn = layers . LSTM ( hidden_neurons , return_sequences = True , stateful = stateful , name = \"RNN\" )( inp ) dens = layers . TimeDistributed ( layers . Dense ( in_out_neurons , name = \"dense\" ))( rnn ) model = models . Model ( inputs = [ inp ], outputs = [ dens ]) model . compile ( loss = \"mean_squared_error\" , sample_weight_mode = \"temporal\" , optimizer = \"rmsprop\" ) return ( model ,( inp , rnn , dens )) Stateless model as a reference Define a model. In the Extract weights from Keras's LSTM and calcualte hidden and cell states , the number of hidden units was as less as 3. However, this time, the dependencies between $Y_t$ and $X_t$ are more complex because of the magnitude of $D$. The LSTM model needs to remember longer history of $X_t$. In [152]: hunits = 64 model_stateless , _ = define_model ( hidden_neurons = hunits , len_ts = X_train . shape [ 1 ]) model_stateless . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input (InputLayer) (None, 1000, 1) 0 _________________________________________________________________ RNN (LSTM) (None, 1000, 64) 16896 _________________________________________________________________ time_distributed_18 (TimeDis (None, 1000, 1) 65 ================================================================= Total params: 16,961 Trainable params: 16,961 Non-trainable params: 0 _________________________________________________________________ Model training In [153]: start = time . time () history = model_stateless . fit ( X_train , y_train , batch_size = 400 , epochs = 100 , verbose = 2 , sample_weight = w_train , validation_data = ( X_val , y_val , w_val )) end = time . time () print ( \"Time Took :{:3.2f} min\" . format ( ( end - start ) / 60 )) Train on 800 samples, validate on 200 samples Epoch 1/100 - 6s - loss: 0.0076 - val_loss: 0.0072 Epoch 2/100 - 4s - loss: 0.0070 - val_loss: 0.0072 Epoch 3/100 - 4s - loss: 0.0070 - val_loss: 0.0072 Epoch 4/100 - 4s - loss: 0.0070 - val_loss: 0.0072 Epoch 5/100 - 4s - loss: 0.0070 - val_loss: 0.0072 Epoch 6/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 7/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 8/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 9/100 - 4s - loss: 0.0070 - val_loss: 0.0072 Epoch 10/100 - 3s - loss: 0.0070 - val_loss: 0.0071 Epoch 11/100 - 4s - loss: 0.0069 - val_loss: 0.0071 Epoch 12/100 - 4s - loss: 0.0069 - val_loss: 0.0071 Epoch 13/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 14/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 15/100 - 4s - loss: 0.0069 - val_loss: 0.0073 Epoch 16/100 - 4s - loss: 0.0070 - val_loss: 0.0072 Epoch 17/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 18/100 - 4s - loss: 0.0069 - val_loss: 0.0071 Epoch 19/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 20/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 21/100 - 4s - loss: 0.0070 - val_loss: 0.0071 Epoch 22/100 - 4s - loss: 0.0069 - val_loss: 0.0071 Epoch 23/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 24/100 - 3s - loss: 0.0069 - val_loss: 0.0072 Epoch 25/100 - 3s - loss: 0.0069 - val_loss: 0.0071 Epoch 26/100 - 4s - loss: 0.0069 - val_loss: 0.0072 Epoch 27/100 - 4s - loss: 0.0069 - val_loss: 0.0070 Epoch 28/100 - 3s - loss: 0.0067 - val_loss: 0.0071 Epoch 29/100 - 4s - loss: 0.0066 - val_loss: 0.0069 Epoch 30/100 - 4s - loss: 0.0063 - val_loss: 0.0068 Epoch 31/100 - 4s - loss: 0.0067 - val_loss: 0.0070 Epoch 32/100 - 4s - loss: 0.0067 - val_loss: 0.0064 Epoch 33/100 - 4s - loss: 0.0060 - val_loss: 0.0060 Epoch 34/100 - 4s - loss: 0.0058 - val_loss: 0.0062 Epoch 35/100 - 3s - loss: 0.0062 - val_loss: 0.0059 Epoch 36/100 - 4s - loss: 0.0057 - val_loss: 0.0056 Epoch 37/100 - 4s - loss: 0.0055 - val_loss: 0.0059 Epoch 38/100 - 4s - loss: 0.0065 - val_loss: 0.0062 Epoch 39/100 - 3s - loss: 0.0058 - val_loss: 0.0056 Epoch 40/100 - 4s - loss: 0.0054 - val_loss: 0.0053 Epoch 41/100 - 4s - loss: 0.0053 - val_loss: 0.0070 Epoch 42/100 - 4s - loss: 0.0072 - val_loss: 0.0062 Epoch 43/100 - 4s - loss: 0.0058 - val_loss: 0.0048 Epoch 44/100 - 4s - loss: 0.0048 - val_loss: 0.0046 Epoch 45/100 - 4s - loss: 0.0048 - val_loss: 0.0059 Epoch 46/100 - 3s - loss: 0.0056 - val_loss: 0.0044 Epoch 47/100 - 4s - loss: 0.0045 - val_loss: 0.0046 Epoch 48/100 - 4s - loss: 0.0048 - val_loss: 0.0052 Epoch 49/100 - 4s - loss: 0.0052 - val_loss: 0.0047 Epoch 50/100 - 4s - loss: 0.0052 - val_loss: 0.0046 Epoch 51/100 - 3s - loss: 0.0045 - val_loss: 0.0044 Epoch 52/100 - 4s - loss: 0.0048 - val_loss: 0.0051 Epoch 53/100 - 4s - loss: 0.0052 - val_loss: 0.0042 Epoch 54/100 - 4s - loss: 0.0047 - val_loss: 0.0040 Epoch 55/100 - 3s - loss: 0.0043 - val_loss: 0.0050 Epoch 56/100 - 3s - loss: 0.0049 - val_loss: 0.0041 Epoch 57/100 - 4s - loss: 0.0047 - val_loss: 0.0042 Epoch 58/100 - 4s - loss: 0.0048 - val_loss: 0.0043 Epoch 59/100 - 4s - loss: 0.0044 - val_loss: 0.0046 Epoch 60/100 - 4s - loss: 0.0048 - val_loss: 0.0046 Epoch 61/100 - 4s - loss: 0.0044 - val_loss: 0.0040 Epoch 62/100 - 4s - loss: 0.0050 - val_loss: 0.0045 Epoch 63/100 - 4s - loss: 0.0045 - val_loss: 0.0037 Epoch 64/100 - 4s - loss: 0.0042 - val_loss: 0.0053 Epoch 65/100 - 4s - loss: 0.0052 - val_loss: 0.0034 Epoch 66/100 - 4s - loss: 0.0037 - val_loss: 0.0033 Epoch 67/100 - 4s - loss: 0.0045 - val_loss: 0.0043 Epoch 68/100 - 4s - loss: 0.0048 - val_loss: 0.0034 Epoch 69/100 - 4s - loss: 0.0039 - val_loss: 0.0048 Epoch 70/100 - 4s - loss: 0.0044 - val_loss: 0.0029 Epoch 71/100 - 4s - loss: 0.0034 - val_loss: 0.0050 Epoch 72/100 - 4s - loss: 0.0060 - val_loss: 0.0054 Epoch 73/100 - 4s - loss: 0.0053 - val_loss: 0.0047 Epoch 74/100 - 4s - loss: 0.0048 - val_loss: 0.0042 Epoch 75/100 - 4s - loss: 0.0043 - val_loss: 0.0037 Epoch 76/100 - 4s - loss: 0.0039 - val_loss: 0.0035 Epoch 77/100 - 4s - loss: 0.0041 - val_loss: 0.0043 Epoch 78/100 - 4s - loss: 0.0047 - val_loss: 0.0037 Epoch 79/100 - 4s - loss: 0.0040 - val_loss: 0.0032 Epoch 80/100 - 4s - loss: 0.0035 - val_loss: 0.0031 Epoch 81/100 - 4s - loss: 0.0037 - val_loss: 0.0039 Epoch 82/100 - 4s - loss: 0.0048 - val_loss: 0.0040 Epoch 83/100 - 4s - loss: 0.0039 - val_loss: 0.0026 Epoch 84/100 - 4s - loss: 0.0030 - val_loss: 0.0026 Epoch 85/100 - 3s - loss: 0.0032 - val_loss: 0.0038 Epoch 86/100 - 4s - loss: 0.0046 - val_loss: 0.0032 Epoch 87/100 - 4s - loss: 0.0034 - val_loss: 0.0028 Epoch 88/100 - 4s - loss: 0.0034 - val_loss: 0.0051 Epoch 89/100 - 4s - loss: 0.0047 - val_loss: 0.0034 Epoch 90/100 - 4s - loss: 0.0035 - val_loss: 0.0029 Epoch 91/100 - 4s - loss: 0.0036 - val_loss: 0.0029 Epoch 92/100 - 4s - loss: 0.0035 - val_loss: 0.0050 Epoch 93/100 - 4s - loss: 0.0044 - val_loss: 0.0027 Epoch 94/100 - 4s - loss: 0.0031 - val_loss: 0.0027 Epoch 95/100 - 4s - loss: 0.0034 - val_loss: 0.0030 Epoch 96/100 - 4s - loss: 0.0033 - val_loss: 0.0027 Epoch 97/100 - 4s - loss: 0.0033 - val_loss: 0.0036 Epoch 98/100 - 4s - loss: 0.0047 - val_loss: 0.0035 Epoch 99/100 - 4s - loss: 0.0035 - val_loss: 0.0025 Epoch 100/100 - 4s - loss: 0.0032 - val_loss: 0.0035 Time Took :6.36 min Validation and training loss In [154]: for label in [ \"loss\" , \"val_loss\" ]: plt . plot ( histroy . history [ label ], label = label ) plt . legend () plt . show () Generate testing data In [145]: X_test , y_test = generate_data ( D = D , T = T , Nsequence = 5000 ) In [161]: y_pred_stateless = model_stateless . predict ( X_test ) plot_examples ( X_test , y_test , ypreds = [ y_pred_stateless ], nm_ypreds = [ \"y_pred stateless\" ]) The final validation loss of y_pred stateless is 0.004058 Stateful Model Training The stateful model gives flexibility of resetting states so you can pass states from batch to batch. However, as a consequence, stateful model requires some book keeping during the training: a set of original time series needs to be trained in the sequential manner and you need to specify when the batch with new sequence starts. For example, consider the scenario: batch_size = 250, time_series_length_in_batch = 500. Then the original 250 time series of length 1,000 sec are divided into two groups: the first 500 sec of all the 250 time series goes to batch 1 and the remaining 500 sec of all the 250 time series goes to the batch 2. Batch 3 will contain the first 500 sec of the next 250 time series and the remaining 500 sec goes to the batch 4. As batch 2 contains the continuation of time series in batch 1, states at the 500th time point from batch 1 needs to be passed to the states in batch 2. On the other hand, batch 3 contains completely new time series so states should not be passed from batch 2 to batch 3. [batch 1]--> pass states --> [batch 2] --> reset states [batch 3]--> pass states --> [batch 4] --> reset states [batch 5]--> pass states --> [batch 6] --> reset states [batch 7]--> pass states --> [batch 8] --> reset states In this formulation, the random batch creation has some constraint: sub time series in batch 2 cannot be trained with sub time series in batch 1. And batch 1 needs to be trained before batch 3. The following class define the methods for training procedure. (Note that this training script save the weights at every back propagation for the sake of algorithm convergence analysis.) In [131]: class statefulModel ( object ): def __init__ ( self , model , print_val_every = 500 ): ''' model must be stateful keras model object batch_input_shape must be specified ''' bis = model . layers [ 0 ] . get_config ()[ \"batch_input_shape\" ] print ( \"batch_input_shape={}\" . format ( bis )) self . batch_size = bis [ 0 ] self . ts = bis [ 1 ] self . Nfeat = bis [ 2 ] self . model = model self . print_val_every = print_val_every def get_mse ( self , true , est , w = None ): ''' calculate MSE for weights == 1 ''' if w is None : w = np . zeros ( true . shape ) w [:] = 1 ytrue = true [ w == 1 ] . flatten () yest = est [ w == 1 ] . flatten () SSE = np . sum (( ytrue - yest ) ** 2 ) N = np . sum ( w == 1 ) MSE = SSE / N return MSE , ( SSE , N ) def X_val_shape_adj ( self , X , X_val_orig , y_val_orig , w_val_orig ): ''' Make the dimension of X_val the same as the dimension of X by adding zeros. It is assumed that: X_val.shape[i] < X.shape[i] i = 0, 1, 2 ''' X_val = np . zeros ( X . shape ) X_val [: X_val_orig . shape [ 0 ]] = X_val_orig myshape = list ( y_val_orig . shape ) myshape [ 0 ] = X . shape [ 0 ] y_val = np . zeros ( myshape ) y_val [: y_val_orig . shape [ 0 ]] = y_val_orig myshape = list ( w_val_orig . shape ) myshape [ 0 ] = X . shape [ 0 ] w_val = np . zeros ( myshape ) w_val [: w_val_orig . shape [ 0 ]] = w_val_orig return X_val , y_val , w_val def train1epoch ( self , X , y , w , epoch = None ): ''' devide the training set of time series into batches. ''' print \" Training..\" batch_index = np . arange ( X . shape [ 0 ]) ## shuffle to create batch containing different time series np . random . shuffle ( batch_index ) count = 1 for ibatch in range ( self . batch_size , X . shape [ 0 ] + 1 , self . batch_size ): print \" Batch {:02d}\" . format ( count ) pick = batch_index [( ibatch - self . batch_size ): ibatch ] if len ( pick ) < self . batch_size : continue X_batch = X [ pick ] y_batch = y [ pick ] w_batch = w [ pick ] self . fit_across_time ( X_batch , y_batch , w_batch , epoch , ibatch ) count += 1 def fit_across_time ( self , X , y , w , epoch = None , ibatch = None ): ''' training for the given set of time series It always starts at the time point 0 so we need to reset states to zero. ''' self . model . reset_states () for itime in range ( self . ts , X . shape [ 1 ] + 1 , self . ts ): ## extract sub time series Xtime = X [:, itime - self . ts : itime ,:] ytime = y [:, itime - self . ts : itime ,:] wtime = w [:, itime - self . ts : itime ] if np . all ( wtime == 0 ): continue val = self . model . fit ( Xtime , ytime , nb_epoch = 1 , ## no shuffling across rows (i.e. time series) shuffle = False , ## use all the samples in one epoch batch_size = X . shape [ 0 ], sample_weight = wtime , verbose = False ) if itime % self . print_val_every == 0 : print \" {start:4d}:{end:4d} loss={val:.3f}\" . format ( start = itime - self . ts , end = itime , val = val . history [ \"loss\" ][ 0 ]) sys . stdout . flush () ## uncomment below if you do not want to save weights for every epoch every batch and every time if epoch is not None : self . model . save_weights ( \"weights_epoch{:03d}_batch{:01d}_time{:04d}.hdf5\" . format ( epoch , ibatch , itime )) def validate1epoch ( self , X_val_adj , y_val_adj , w_val_adj ): batch_index = np . arange ( X_val_adj . shape [ 0 ]) print \" Validating..\" val_loss = 0 count = 1 for ibatch in range ( self . batch_size , X_val_adj . shape [ 0 ] + 1 , self . batch_size ): pick = batch_index [( ibatch - self . batch_size ): ibatch ] if len ( pick ) < self . batch_size : continue X_val_adj_batch = X_val_adj [ pick ] y_val_adj_batch = y_val_adj [ pick ] w_val_adj_batch = w_val_adj [ pick ] if np . all ( w_val_adj_batch == 0 ): continue print \" Batch {}\" . format ( count ) SSE , N = self . validate_across_time ( X_val_adj_batch , y_val_adj_batch , w_val_adj_batch ) val_loss += SSE count += N val_loss /= count return val_loss def validate_across_time ( self , X_val_adj , y_val_adj , w_val_adj ): y_pred_adj = np . zeros ( y_val_adj . shape ) y_pred_adj [:] = np . NaN self . model . reset_states () for itime in range ( self . ts , X_val_adj . shape [ 1 ] + 1 , self . ts ): y_pred_adj [:, itime - self . ts : itime ,:] = self . model . predict ( X_val_adj [:, itime - self . ts : itime ,:], batch_size = X_val_adj . shape [ 0 ]) loss , _ = self . get_mse ( y_pred_adj [:, itime - self . ts : itime ,:], y_val_adj [:, itime - self . ts : itime ,:], w_val_adj [:, itime - self . ts : itime ]) if itime % self . print_val_every == 0 : print \" {start:4d}:{end:4d} val_loss={a:.3f}\" . format ( start = itime - self . ts , end = itime , a = loss ) sys . stdout . flush () ## comment out the lines below if you do not want to see the trajectory plots fig = plt . figure ( figsize = ( 12 , 2 )) nplot = 3 for i in range ( nplot ): ax = fig . add_subplot ( 1 , nplot , i + 1 ) ax . plot ( y_pred_adj [ i ,:, 0 ], label = \"ypred\" ) ax . plot ( y_val_adj [ i ,:, 0 ], label = \"yval\" ) ax . set_ylim ( - 0.5 , 0.5 ) plt . legend () plt . show () _ , ( SSE , N ) = self . get_mse ( y_pred_adj [:,: itime ,:], y_val_adj [:,: itime ,:], w_val_adj [:,: itime ]) return SSE , N def fit ( self , X , y , w , X_val , y_val , w_val , Nepoch = 300 ): X_val_adj , y_val_adj , w_val_adj = self . X_val_shape_adj ( X , X_val , y_val , w_val ) past_val_loss = np . Inf history = [] for iepoch in range ( Nepoch ): self . model . reset_states () print \"__________________________________\" print \"Epoch {}\" . format ( iepoch + 1 ) self . train1epoch ( X , y , w , iepoch ) val_loss = self . validate1epoch ( X_val_adj , y_val_adj , w_val_adj ) print \"-----------------> Epoch {iepoch:d} overall valoss={loss:.6f}\" . format ( iepoch = iepoch + 1 , loss = val_loss ), ## uncomments here if you want to save weights only when the weights resulted in lower validation loss ##if val_loss < past_val_loss: ## print \">>SAVED<<\" ## self.model.save_weights(\"/model.h5\") ## past_val_loss = val_loss ## print \"\" history . append ( val_loss ) return history Define a stateful model I consider batch size = 400 just as in the stateless model. In [132]: model_stateful , _ = define_model ( hidden_neurons = hunits , batch_size = 400 , stateful = True , len_ts = 500 ) model_stateful . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input (InputLayer) (400, 500, 1) 0 _________________________________________________________________ RNN (LSTM) (400, 500, 64) 16896 _________________________________________________________________ time_distributed_16 (TimeDis (400, 500, 1) 65 ================================================================= Total params: 16,961 Trainable params: 16,961 Non-trainable params: 0 _________________________________________________________________ Training The log shows the training loss and validation loss for the first 500 sec of time series and the next 500 sec of time series for each batch separately. It is clear that the model performance is lower in the last 500 sec in every epoch. This makes sense because the data shows more variability in that region. Three example validation time series are also plotted. In [133]: smodel = statefulModel ( model = model_stateful , print_val_every = 500 ) start = time . time () history_stateful = smodel . fit ( X_train , y_train , w_train , X_val , y_val , w_val , Nepoch = 100 ) end = time . time () print ( \"Time Took {:3.2f} min\" . format (( end - start ) / 60 )) batch_input_shape=(400, 500, 1) __________________________________ Epoch 1 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.015 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 1 overall valoss=0.007259 __________________________________ Epoch 2 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 2 overall valoss=0.007213 __________________________________ Epoch 3 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 3 overall valoss=0.007170 __________________________________ Epoch 4 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 4 overall valoss=0.007212 __________________________________ Epoch 5 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 5 overall valoss=0.007183 __________________________________ Epoch 6 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 6 overall valoss=0.007309 __________________________________ Epoch 7 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 7 overall valoss=0.007225 __________________________________ Epoch 8 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 8 overall valoss=0.007235 __________________________________ Epoch 9 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 9 overall valoss=0.007191 __________________________________ Epoch 10 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 10 overall valoss=0.007153 __________________________________ Epoch 11 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 11 overall valoss=0.007200 __________________________________ Epoch 12 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 12 overall valoss=0.007400 __________________________________ Epoch 13 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 13 overall valoss=0.007281 __________________________________ Epoch 14 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 14 overall valoss=0.007183 __________________________________ Epoch 15 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 15 overall valoss=0.007143 __________________________________ Epoch 16 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 16 overall valoss=0.007282 __________________________________ Epoch 17 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 17 overall valoss=0.007223 __________________________________ Epoch 18 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 18 overall valoss=0.007235 __________________________________ Epoch 19 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 19 overall valoss=0.007136 __________________________________ Epoch 20 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 20 overall valoss=0.007123 __________________________________ Epoch 21 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 21 overall valoss=0.007147 __________________________________ Epoch 22 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.012 -----------------> Epoch 22 overall valoss=0.007098 __________________________________ Epoch 23 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 23 overall valoss=0.007309 __________________________________ Epoch 24 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.012 -----------------> Epoch 24 overall valoss=0.007086 __________________________________ Epoch 25 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.001 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.013 -----------------> Epoch 25 overall valoss=0.007330 __________________________________ Epoch 26 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.014 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.013 -----------------> Epoch 26 overall valoss=0.007194 __________________________________ Epoch 27 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.012 -----------------> Epoch 27 overall valoss=0.007048 __________________________________ Epoch 28 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.012 -----------------> Epoch 28 overall valoss=0.006954 __________________________________ Epoch 29 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.013 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.012 -----------------> Epoch 29 overall valoss=0.006786 __________________________________ Epoch 30 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.011 -----------------> Epoch 30 overall valoss=0.006396 __________________________________ Epoch 31 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.010 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 31 overall valoss=0.005889 __________________________________ Epoch 32 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.009 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.020 -----------------> Epoch 32 overall valoss=0.011183 __________________________________ Epoch 33 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.014 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.012 -----------------> Epoch 33 overall valoss=0.006942 __________________________________ Epoch 34 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.013 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.012 -----------------> Epoch 34 overall valoss=0.006642 __________________________________ Epoch 35 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.011 -----------------> Epoch 35 overall valoss=0.006194 __________________________________ Epoch 36 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.010 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 36 overall valoss=0.005879 __________________________________ Epoch 37 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.009 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.012 -----------------> Epoch 37 overall valoss=0.006595 __________________________________ Epoch 38 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.011 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.011 -----------------> Epoch 38 overall valoss=0.006454 __________________________________ Epoch 39 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 39 overall valoss=0.005579 __________________________________ Epoch 40 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 40 overall valoss=0.006312 __________________________________ Epoch 41 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.012 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.013 -----------------> Epoch 41 overall valoss=0.007898 __________________________________ Epoch 42 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.014 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 42 overall valoss=0.005934 __________________________________ Epoch 43 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.010 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 43 overall valoss=0.005574 __________________________________ Epoch 44 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.010 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 44 overall valoss=0.006649 __________________________________ Epoch 45 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.010 -----------------> Epoch 45 overall valoss=0.006031 __________________________________ Epoch 46 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.010 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 46 overall valoss=0.006568 __________________________________ Epoch 47 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.009 Batch 02 0: 500 loss=0.001 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 47 overall valoss=0.006723 __________________________________ Epoch 48 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.010 Batch 02 0: 500 loss=0.001 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 48 overall valoss=0.006586 __________________________________ Epoch 49 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 49 overall valoss=0.006294 __________________________________ Epoch 50 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 50 overall valoss=0.006118 __________________________________ Epoch 51 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.010 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.011 -----------------> Epoch 51 overall valoss=0.006206 __________________________________ Epoch 52 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.008 -----------------> Epoch 52 overall valoss=0.004871 __________________________________ Epoch 53 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.008 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 53 overall valoss=0.005124 __________________________________ Epoch 54 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.008 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 54 overall valoss=0.006700 __________________________________ Epoch 55 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.010 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 55 overall valoss=0.005238 __________________________________ Epoch 56 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.009 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 56 overall valoss=0.006500 __________________________________ Epoch 57 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.008 -----------------> Epoch 57 overall valoss=0.004975 __________________________________ Epoch 58 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.009 Batch 02 0: 500 loss=0.001 500:1000 loss=0.016 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.012 -----------------> Epoch 58 overall valoss=0.006789 __________________________________ Epoch 59 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.012 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 59 overall valoss=0.005800 __________________________________ Epoch 60 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 60 overall valoss=0.005546 __________________________________ Epoch 61 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.009 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.009 -----------------> Epoch 61 overall valoss=0.005259 __________________________________ Epoch 62 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.009 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.009 -----------------> Epoch 62 overall valoss=0.005019 __________________________________ Epoch 63 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.008 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 63 overall valoss=0.005753 __________________________________ Epoch 64 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.010 Batch 02 0: 500 loss=0.001 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.008 -----------------> Epoch 64 overall valoss=0.004636 __________________________________ Epoch 65 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.008 -----------------> Epoch 65 overall valoss=0.004421 __________________________________ Epoch 66 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 66 overall valoss=0.005629 __________________________________ Epoch 67 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.008 -----------------> Epoch 67 overall valoss=0.004751 __________________________________ Epoch 68 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.009 Batch 02 0: 500 loss=0.001 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 68 overall valoss=0.005471 __________________________________ Epoch 69 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 69 overall valoss=0.004975 __________________________________ Epoch 70 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.008 -----------------> Epoch 70 overall valoss=0.004494 __________________________________ Epoch 71 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 71 overall valoss=0.005566 __________________________________ Epoch 72 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.010 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.008 -----------------> Epoch 72 overall valoss=0.004722 __________________________________ Epoch 73 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 73 overall valoss=0.005049 __________________________________ Epoch 74 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.009 -----------------> Epoch 74 overall valoss=0.005097 __________________________________ Epoch 75 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.010 -----------------> Epoch 75 overall valoss=0.005747 __________________________________ Epoch 76 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.010 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 76 overall valoss=0.005440 __________________________________ Epoch 77 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.018 -----------------> Epoch 77 overall valoss=0.010407 __________________________________ Epoch 78 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.011 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.009 -----------------> Epoch 78 overall valoss=0.005452 __________________________________ Epoch 79 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.010 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 79 overall valoss=0.005298 __________________________________ Epoch 80 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.009 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 80 overall valoss=0.005597 __________________________________ Epoch 81 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 81 overall valoss=0.005489 __________________________________ Epoch 82 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.008 -----------------> Epoch 82 overall valoss=0.005087 __________________________________ Epoch 83 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.008 Batch 02 0: 500 loss=0.000 500:1000 loss=0.007 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.009 -----------------> Epoch 83 overall valoss=0.005339 __________________________________ Epoch 84 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.008 -----------------> Epoch 84 overall valoss=0.004870 __________________________________ Epoch 85 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.008 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.008 -----------------> Epoch 85 overall valoss=0.004850 __________________________________ Epoch 86 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.007 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.007 -----------------> Epoch 86 overall valoss=0.004247 __________________________________ Epoch 87 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.006 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.007 -----------------> Epoch 87 overall valoss=0.004450 __________________________________ Epoch 88 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.006 Batch 02 0: 500 loss=0.000 500:1000 loss=0.009 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.008 -----------------> Epoch 88 overall valoss=0.004752 __________________________________ Epoch 89 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.006 -----------------> Epoch 89 overall valoss=0.003650 __________________________________ Epoch 90 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.006 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.006 -----------------> Epoch 90 overall valoss=0.003619 __________________________________ Epoch 91 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.007 Batch 02 0: 500 loss=0.000 500:1000 loss=0.006 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.011 -----------------> Epoch 91 overall valoss=0.006676 __________________________________ Epoch 92 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.007 Batch 02 0: 500 loss=0.000 500:1000 loss=0.010 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.009 -----------------> Epoch 92 overall valoss=0.005037 __________________________________ Epoch 93 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.008 Batch 02 0: 500 loss=0.001 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.006 -----------------> Epoch 93 overall valoss=0.003392 __________________________________ Epoch 94 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.007 Batch 02 0: 500 loss=0.000 500:1000 loss=0.006 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.008 -----------------> Epoch 94 overall valoss=0.004478 __________________________________ Epoch 95 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.007 Batch 02 0: 500 loss=0.001 500:1000 loss=0.006 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.012 -----------------> Epoch 95 overall valoss=0.007137 __________________________________ Epoch 96 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.009 Batch 02 0: 500 loss=0.000 500:1000 loss=0.007 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.014 -----------------> Epoch 96 overall valoss=0.008292 __________________________________ Epoch 97 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.008 Batch 02 0: 500 loss=0.000 500:1000 loss=0.008 Validating.. Batch 1 0: 500 val_loss=0.001 500:1000 val_loss=0.010 -----------------> Epoch 97 overall valoss=0.005924 __________________________________ Epoch 98 Training.. Batch 01 0: 500 loss=0.001 500:1000 loss=0.007 Batch 02 0: 500 loss=0.000 500:1000 loss=0.005 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.006 -----------------> Epoch 98 overall valoss=0.003494 __________________________________ Epoch 99 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.006 Batch 02 0: 500 loss=0.001 500:1000 loss=0.007 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.009 -----------------> Epoch 99 overall valoss=0.004981 __________________________________ Epoch 100 Training.. Batch 01 0: 500 loss=0.000 500:1000 loss=0.008 Batch 02 0: 500 loss=0.000 500:1000 loss=0.007 Validating.. Batch 1 0: 500 val_loss=0.000 500:1000 val_loss=0.006 -----------------> Epoch 100 overall valoss=0.003355 Time Took 7.10 min The validation loss plot, stateful and stateless models The validation loss plot shows that the back propagation algorithm of the stateless model seems less stable. In [178]: for label in [ \"loss\" , \"val_loss\" ]: plt . plot ( histroy . history [ label ], label = label + \" - stateless\" ) plt . plot ( history_stateful , label = \"val loss - stateful\" ) plt . legend () plt . show () Prediction phase For model testing, I want to predict $Y_t$ at every time point, one at a time. Therefore, I re-define a model with batch_size = (1000,1,2). As I am concerned about the unstable behavior of the back propagation algorithm in the stateful training, I will provide the last 4 training weights obtained during the final epoch. In [174]: model_pred1 , _ = define_model ( len_ts = 1 , hidden_neurons = hunits , batch_size = X_test . shape [ 0 ], stateful = True ) model_pred2 , _ = define_model ( len_ts = 1 , hidden_neurons = hunits , batch_size = X_test . shape [ 0 ], stateful = True ) model_pred3 , _ = define_model ( len_ts = 1 , hidden_neurons = hunits , batch_size = X_test . shape [ 0 ], stateful = True ) model_pred4 , _ = define_model ( len_ts = 1 , hidden_neurons = hunits , batch_size = X_test . shape [ 0 ], stateful = True ) ## load the final weights model_pred1 . load_weights ( \"weights_epoch099_batch800_time1000.hdf5\" ) model_pred2 . load_weights ( \"weights_epoch099_batch800_time0500.hdf5\" ) model_pred3 . load_weights ( \"weights_epoch099_batch400_time1000.hdf5\" ) model_pred4 . load_weights ( \"weights_epoch099_batch400_time0500.hdf5\" ) model_pred1 . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input (InputLayer) (5000, 1, 1) 0 _________________________________________________________________ RNN (LSTM) (5000, 1, 64) 16896 _________________________________________________________________ time_distributed_25 (TimeDis (5000, 1, 1) 65 ================================================================= Total params: 16,961 Trainable params: 16,961 Non-trainable params: 0 _________________________________________________________________ Define the prediction function for the stateful model. In [175]: def stateful_prediction ( mm , X_test , ntarget = 1 ): #expecting.. bis = mm . layers [ 0 ] . get_config ()[ \"batch_input_shape\" ] batch_size , ts , nfeat = bis assert ( X_test . shape [ 0 ] % batch_size == 0 ) assert ( X_test . shape [ 1 ] % ts == 0 ) y_pred = np . zeros (( X_test . shape [ 0 ], X_test . shape [ 1 ], ntarget )) y_pred [:] = np . NaN for ipat in range ( 0 , X_test . shape [ 0 ], batch_size ): mm . reset_states () for itime in range ( 0 , X_test . shape [ 1 ], ts ): X_testi = X_test [ ipat :( ipat + batch_size ), itime :( itime + ts ),:] y_pred [ ipat :( ipat + batch_size ), itime :( itime + ts ),:] = mm . predict ( X_testi , batch_size = batch_size ) return y_pred Prediction In [176]: y_pred_stateful1 = stateful_prediction ( mm = model_pred1 , X_test = X_test ) y_pred_stateful2 = stateful_prediction ( mm = model_pred2 , X_test = X_test ) y_pred_stateful3 = stateful_prediction ( mm = model_pred3 , X_test = X_test ) y_pred_stateful4 = stateful_prediction ( mm = model_pred4 , X_test = X_test ) Evaluate the model performance on testing data The stateful model outperform stateless twice, and perform much worse once. In [177]: plot_examples ( X_test , y_test , ypreds = [ y_pred_stateful1 , y_pred_stateful2 , y_pred_stateful3 , y_pred_stateful4 , y_pred_stateless ], nm_ypreds = [ \"ypred stateful1\" , \"ypred stateful2\" , \"ypred stateful3\" , \"ypred stateful4\" , \"ypred stateless\" ]) The final validation loss of ypred stateful1 is 0.003775 The final validation loss of ypred stateful2 is 0.004036 The final validation loss of ypred stateful3 is 0.004058 The final validation loss of ypred stateful4 is 0.004992 The final validation loss of ypred stateless is 0.004058 Let's see how the weights are changing over epochs. In [139]: def get_LSTM_UWb ( weight ): ''' weight must be output of LSTM's layer.get_weights() W: weights for input U: weights for hidden states b: bias ''' warr , uarr , barr = weight gates = [ \"i\" , \"f\" , \"c\" , \"o\" ] hunit = uarr . shape [ 0 ] U , W , b = {},{},{} for i1 , i2 in enumerate ( range ( 0 , len ( barr ), hunit )): W [ gates [ i1 ]] = warr [:, i2 : i2 + hunit ] U [ gates [ i1 ]] = uarr [:, i2 : i2 + hunit ] b [ gates [ i1 ]] = barr [ i2 : i2 + hunit ] . reshape ( hunit , 1 ) return ( W , U , b ) def get_LSTMweights ( model1 ): for layer in model1 . layers : if \"LSTM\" in str ( layer ): w = layer . get_weights () W , U , b = get_LSTM_UWb ( w ) break return W , U , b def vectorize_with_labels ( W , U , b ): bs , bs_label , ws , ws_label , us , us_label = [],[],[],[],[],[] for k in [ \"i\" , \"f\" , \"c\" , \"o\" ]: temp = list ( W [ k ] . flatten ()) ws_label . extend ([ \"W_\" + k ] * len ( temp )) ws . extend ( temp ) temp = list ( U [ k ] . flatten ()) us_label . extend ([ \"U_\" + k ] * len ( temp )) us . extend ( temp ) temp = list ( b [ k ] . flatten ()) bs_label . extend ([ \"b_\" + k ] * len ( temp )) bs . extend ( temp ) weight = ws + us + bs wlabel = ws_label + us_label + bs_label return ( weight , wlabel ) In [210]: from copy import copy import pandas as pd df = {} ibatch = 800 for epoch in np . arange ( 50 , 100 , 1 ): for itime in [ 500 , 1000 ]: title = \"weights_epoch{:03d}_batch{:01d}_time{:04d}.hdf5\" . format ( epoch , ibatch , itime ) model_stateful . load_weights ( title ) WUb = get_LSTMweights ( model_stateful ) weight , wlabel = vectorize_with_labels ( * WUb ) df [ \"epoch{:03d} time{:04d}\" . format ( epoch , itime )] = copy ( weight ) df = pd . DataFrame ( df , index = [ lab + \"_\" + str ( i ) for i , lab in enumerate ( wlabel )]) dfT = df . T Plot the weight change over every back propagation Observation and conclusions: Stateful procedure seems to be working in the sense that it returns comparable results as stateless procedure. The bias for cell states, forget gate, input gate and output gate seem to be oscillating between the back propagation of the batch with the first 500 sec and the next 500 sec. This observation again confirms the the unstable convergence of back propagation algorithm. Stateful training does not allow shuffling between the sub time series of the first 500 sec and the next 500 sec. This may be causing the back propagation algorithm unstable. Unless a single time series is too large to fit in to a single batch, I do not recommend to use the stateful training method as stateless model outperforms the stateful model. In [211]: unilabel = np . unique ( wlabel ) fig = plt . figure ( figsize = ( 25 , 105 )) fig . subplots_adjust ( hspace = 0.5 , wspace = 0.01 ) count = 1 for weight_type in unilabel : ax = fig . add_subplot ( 12 , 1 , count ) nweight = 0 for colnm in dfT . columns : if weight_type in colnm : ax . plot ( dfT [ colnm ] . values , label = weight_type + \"_\" + str ( count )) ax . set_title ( weight_type ) ax . set_xticks ( range ( dfT . shape [ 0 ]) ) ax . set_xticklabels ( list ( dfT . index . values ), rotation = \"vertical\" ) nweight += 1 if nweight > 5 : break count += 1 plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Stateful LSTM model training in Keras"},{"url":"Extract-weights-from-Keras's-LSTM-and-calcualte-hidden-and-cell-states.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In this blog post, I will review the famous long short-term memory (LSTM) model and try to understand how it is implemented in Keras. If you know nothing about recurrent deep learning model, please read my previous post about recurrent neural network . If you know reccurent neural network (RNN) but not LSTM, you should first read Colah's great blog post . The LSTM outperforms Simple RNN model because it is designed to remember longer time series. In this blog, I will discuss: how to fit a LSTM model to predict a point in time series given another time series. how to extract weights for forget gates, input gates and output gates from the LSTM's model. how to calculate hidden and cell states from the weight outputs. To start learning LSTM, let's create a synthetic time series data. Having a data in front help understood the meaning of each parameter in LSTM. In [39]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import seaborn as sns import pandas as pd import sys , time import numpy as np import warnings warnings . filterwarnings ( \"ignore\" ) print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )) print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"1\" set_session ( tf . Session ( config = config )) def set_seed ( sd = 123 ): from numpy.random import seed from tensorflow import set_random_seed import random as rn ## numpy random seed seed ( sd ) ## core python's random number rn . seed ( sd ) ## tensor flow's random number set_random_seed ( sd ) python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.1.3 tensorflow version 1.5.0 Create synthetic long time series data I will generate a time series $X_{t}$ ($t=1,...,T$) as my independent feature. As the target, or dependent time series, I will create a time series $Y_{t}$ as a function of a single time series $\\{ X_{k} \\}_{k=1}&#94;t$. Given integers $D$ and $T$, the time series $X_{t}$ an $Y_t$ are generated as: $$ C \\sim \\textrm{Multinomial}(5,6,...,99)\\\\ U \\sim \\textrm{Unif}([0,1])\\\\ X_{t} = -\\frac{t}{T}U\\left[ \\textrm{Cos}\\left(\\frac{t}{1+C}\\right) \\right]\\\\ Y_{t} = X_{t-2} X_{t-\\textrm{D}} \\textrm{ for t > D else } Y_t = 0 $$ We consider a long time series $T = 1,000$. The parameter $D$ determines the time lag. The larger $D$ is, the longer it takes for the $Y_{t}$ to show the effect of $X_{t}$, and the longer memory that the deep learning model needs to remember. For this exercise, I will consider $D=10$, and generate 1,000 sets of time series, independently. Plot examples of the generated time series. The frequency of the waves vary across samples. The first 10 $Y_t$ are set to zero ($Y_1=...=Y_{10} = 0$) In [40]: def random_sample ( len_ts = 3000 , D = 1001 ): c_range = range ( 5 , 100 ) c1 = np . random . choice ( c_range ) u = np . random . random ( 1 ) const = - 1.0 / len_ts ts = np . arange ( 0 , len_ts ) x1 = np . cos ( ts / float ( 1.0 + c1 )) x1 = x1 * ts * u * const y1 = np . zeros ( len_ts ) for t in range ( D , len_ts ): ## the output time series depend on input as follows: y1 [ t ] = x1 [ t - 2 ] * x1 [ t - D ] y = np . array ([ y1 ]) . T X = np . array ([ x1 ]) . T return y , X def generate_data ( D = 1001 , Nsequence = 1000 , T = 4000 , seed = 123 ): X_train = [] y_train = [] set_seed ( sd = seed ) for isequence in range ( Nsequence ): y , X = random_sample ( T , D = D ) X_train . append ( X ) y_train . append ( y ) return np . array ( X_train ), np . array ( y_train ) D = 10 T = 1000 X , y = generate_data ( D = D , T = T , Nsequence = 1000 ) print ( X . shape , y . shape ) ((1000, 1000, 1), (1000, 1000, 1)) Plot examples of the generated time series. The frequency of the waves vary across samples. The first 10 $Y_t$ are set to zero ($Y_1=...=Y_{10} = 0$) In [41]: def plot_examples ( X , y , ypreds = None , nm_ypreds = None ): fig = plt . figure ( figsize = ( 16 , 10 )) fig . subplots_adjust ( hspace = 0.32 , wspace = 0.15 ) count = 1 n_ts = 16 for irow in range ( n_ts ): ax = fig . add_subplot ( n_ts / 4 , 4 , count ) ax . set_ylim ( - 0.5 , 0.5 ) ax . plot ( X [ irow ,:, 0 ], \"--\" , label = \"x1\" ) ax . plot ( y [ irow ,:,:], label = \"y\" , linewidth = 3 , alpha = 0.5 ) ax . set_title ( \"{:}th time series sample\" . format ( irow )) if ypreds is not None : for ypred , nm in zip ( ypreds , nm_ypreds ): ax . plot ( ypred [ irow ,:,:], label = nm ) count += 1 plt . legend () plt . show () plot_examples ( X , y , ypreds = None , nm_ypreds = None ) LSTM model I create a LSTM model to predict $Y_t$ using the time series $X_k, k=1,...,t$ (t=1,...,1,000). Difference with RNN model The RNN layer updates the hidden states $h_t$ in a simple formula with two unknown weights and a bias, $w_{1x}$, $w_{1h}$ and $b_1$ given the previous hidden state $h_{t-1}$ and input $x_t$. The update process can be written in a single line as: $$ h_t = \\textrm{tanh}(x_t&#94;T w_{1x} + h_{t-1}&#94;T w_{1h} + b_1) $$ The hidden state $h_t$ is passed to the next cell as well as the next layer as inputs. The LSTM model also have hidden states that are updated between recurrent cells. In fact, the LSTM layer has two types of states: hidden state and cell states that are passed between the LSTM cells. However, only hidden states are passed to the next layer. LSTM cell formulation Let nfeat denote the number of input time series features. In our example, nfeat = 1. Then the LSTM layer with \"hunits\" hidden units $h_{t} \\in R&#94;{\\textrm{hunits}}$ are defined with 4(hunits x hunits + hunits x nfeat + hunits x 1) parameters: $$ \\boldsymbol{W}_i \\in R&#94;{\\textrm{hunits x nfeat}}, \\boldsymbol{U}_i \\in R&#94;{\\textrm{hunits x hunits}},\\boldsymbol{b}_i \\in R&#94;{\\textrm{hunits x 1}}\\\\ \\boldsymbol{W}_f \\in R&#94;{\\textrm{hunits x nfeat}},\\boldsymbol{U}_f \\in R&#94;{\\textrm{hunits x hunits}},\\boldsymbol{b}_f \\in R&#94;{\\textrm{hunits x 1}}\\\\ \\boldsymbol{W}_c \\in R&#94;{\\textrm{hunits x nfeat}},\\boldsymbol{U}_c \\in R&#94;{\\textrm{hunits x hunits}},\\boldsymbol{b}_c \\in R&#94;{\\textrm{hunits x 1}}\\\\ \\boldsymbol{W}_o \\in R&#94;{\\textrm{hunits x nfeat}},\\boldsymbol{U}_o \\in R&#94;{\\textrm{hunits x hunits}},\\boldsymbol{b}_o \\in R&#94;{\\textrm{hunits x 1}}\\\\ $$ input gate $$ \\boldsymbol{i}_t = \\textrm{sigmoid} \\left( \\boldsymbol{W}_i\\boldsymbol{h}_{t-1} + \\boldsymbol{U}_i\\boldsymbol{x}_{t} + \\boldsymbol{b}_i \\right) $$ sigmoid is applied element wise. forget gate $$ \\boldsymbol{f}_t = \\textrm{sigmoid} \\left( \\boldsymbol{W}_f \\boldsymbol{h}_{t-1}+ \\boldsymbol{U}_f \\boldsymbol{x}_{t} + \\boldsymbol{b}_f \\right) $$ sigmoid is applied element wise. new candidate cell state $$ \\boldsymbol{\\tilde{c}} = \\textrm{tanh} \\left( \\boldsymbol{W}_c \\boldsymbol{h}_{t-1}+ \\boldsymbol{U}_c \\boldsymbol{x}_{t} + \\boldsymbol{b}_c \\right) $$ tanh is applied element wise. output gate $$ \\boldsymbol{o}_t = \\textrm{sigmoid} \\left( \\boldsymbol{W}_o \\boldsymbol{h}_{t-1}+ \\boldsymbol{U}_o \\boldsymbol{x}_{t} + \\boldsymbol{b}_o \\right) $$ sigmoid is applied element wise. cell state $$ \\boldsymbol{c}_t= \\boldsymbol{f}_t * \\boldsymbol{c}_{t-1} + \\boldsymbol{i}_t * \\boldsymbol{\\tilde{c}} $$ $*$ means element wise multiplication. hidden state $$ \\boldsymbol{h}_t= \\boldsymbol{o}_t * \\textrm{tanh}(\\boldsymbol{c}_{t}) $$ $\\boldsymbol{h}_t$ is passed as an input of higher layer. For example in our model, we pass $\\boldsymbol{h}_t$ to the fully connected layer. I will create a single layer LSTM model with 3 nodes, followed by fully connected layer. This model contains 60 parameters (= 4 (3 3+3 1 + 3 1) = 4(hunits x hunits + hunits x nfeat + hunits x 1)) in LSTM layer and 4 parameters in fully connected layer. In [42]: from keras import models from keras import layers def define_model ( len_ts , hidden_neurons = 1 , nfeature = 1 , batch_size = None , stateful = False ): in_out_neurons = 1 inp = layers . Input ( batch_shape = ( batch_size , len_ts , nfeature ), name = \"input\" ) rnn = layers . LSTM ( hidden_neurons , return_sequences = True , stateful = stateful , name = \"RNN\" )( inp ) dens = layers . TimeDistributed ( layers . Dense ( in_out_neurons , name = \"dense\" ))( rnn ) model = models . Model ( inputs = [ inp ], outputs = [ dens ]) model . compile ( loss = \"mean_squared_error\" , sample_weight_mode = \"temporal\" , optimizer = \"rmsprop\" ) return ( model ,( inp , rnn , dens )) Here I define a model. In [43]: X_train , y_train = X , y hunits = 3 model1 , _ = define_model ( hidden_neurons = hunits , len_ts = X_train . shape [ 1 ]) model1 . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input (InputLayer) (None, 1000, 1) 0 _________________________________________________________________ RNN (LSTM) (None, 1000, 3) 60 _________________________________________________________________ time_distributed_3 (TimeDist (None, 1000, 1) 4 ================================================================= Total params: 64 Trainable params: 64 Non-trainable params: 0 _________________________________________________________________ For model training, we use MSE loss. As the first 10 $Y_t$ is not defined based on $X_{t}$, I will not compute the MSE for the first 10 samples and compute it only for the t = 10,...,1000. This can be easily done in Keras by incorporating \"sample_weight\" in loss definition. The loss for each sample sequence is defined with weights as: $$ Loss = \\frac{1}{\\sum_{t=1}&#94;T w_t}\\sum_{t=1}&#94;T w_t (Y_t - \\hat{Y}_t)&#94;2 $$ where $w_t = 1$ for $t>D$ and $w_t=0$ otherwise. In [44]: w = np . zeros ( y_train . shape [: 2 ]) w [:, D :] = 1 w_train = w Model training I will save model at every epoch using call back function. In [45]: from keras.callbacks import ModelCheckpoint start = time . time () hist1 = model1 . fit ( X_train , y_train , batch_size = 2 ** 9 , epochs = 200 , verbose = False , sample_weight = w_train , validation_split = 0.05 , callbacks = [ ModelCheckpoint ( filepath = \"weights{epoch:03d}.hdf5\" )]) end = time . time () print ( \"Time took {:3.1f} min\" . format (( end - start ) / 60 )) Time took 10.5 min The validation loss plot In [46]: labels = [ \"loss\" , \"val_loss\" ] for lab in labels : plt . plot ( hist1 . history [ lab ], label = lab + \" model1\" ) plt . yscale ( \"log\" ) plt . legend () plt . show () Validate the model performance with new data In [47]: X_test , y_test = generate_data ( D = D , T = T , seed = 2 , Nsequence = 1000 ) y_pred1 = model1 . predict ( X_test ) w_test = np . zeros ( y_test . shape [: 2 ]) w_test [:, D :] = 1 In [48]: plot_examples ( X_test , y_test , ypreds = [ y_pred1 ], nm_ypreds = [ \"ypred model1\" ]) print ( \"The final validation loss is {:5.4f}\" . format ( np . mean (( y_pred1 [ w_test == 1 ] - y_test [ w_test == 1 ]) ** 2 ))) The final validation loss is 0.0003 Reproduce LSTM layer outputs by hands The best way to understand how LSTM layer calculate hidden states and cell states are to reproduce them by hands! We first extract the estimated weights of the LSTM layer from model1. In [49]: for layer in model1 . layers : if \"LSTM\" in str ( layer ): weightLSTM = layer . get_weights () warr , uarr , barr = weightLSTM warr . shape , uarr . shape , barr . shape Out[49]: ((1, 12), (3, 12), (12,)) warr is a numpy array of weights for inputs uarr is a numpy array of weights for hidden units barr is a numpy array of bias The following methods extract weights for input, forget and output gates, and the cell states. Then I calculate cell states and hidden states. In [50]: def sigmoid ( x ): return ( 1.0 / ( 1.0 + np . exp ( - x ))) def LSTMlayer ( weight , x_t , h_tm1 , c_tm1 ): ''' c_tm1 = np.array([0,0]).reshape(1,2) h_tm1 = np.array([0,0]).reshape(1,2) x_t = np.array([1]).reshape(1,1) warr.shape = (nfeature,hunits*4) uarr.shape = (hunits,hunits*4) barr.shape = (hunits*4,) ''' warr , uarr , barr = weight s_t = ( x_t . dot ( warr ) + h_tm1 . dot ( uarr ) + barr ) hunit = uarr . shape [ 0 ] i = sigmoid ( s_t [:,: hunit ]) f = sigmoid ( s_t [:, 1 * hunit : 2 * hunit ]) _c = np . tanh ( s_t [:, 2 * hunit : 3 * hunit ]) o = sigmoid ( s_t [:, 3 * hunit :]) c_t = i * _c + f * c_tm1 h_t = o * np . tanh ( c_t ) return ( h_t , c_t ) The initial values of cell states and hidden states are zero. In [51]: c_tm1 = np . array ([ 0 ] * hunits ) . reshape ( 1 , hunits ) h_tm1 = np . array ([ 0 ] * hunits ) . reshape ( 1 , hunits ) We consider three time points $X_1=0.003,X_2=0.002$ and $X_3=1$ as inputs and evaluate $\\boldsymbol{h_3}$ and $\\boldsymbol{c_3}$: In [52]: xs = np . array ([ 0.003 , 0.002 , 1 ]) for i in range ( len ( xs )): x_t = xs [ i ] . reshape ( 1 , 1 ) h_tm1 , c_tm1 = LSTMlayer ( weightLSTM , x_t , h_tm1 , c_tm1 ) print ( \"h3={}\" . format ( h_tm1 )) print ( \"c3={}\" . format ( c_tm1 )) h3=[[0.15008775 0.01919995 0.1935698 ]] c3=[[0.22576768 0.03767894 0.46717375]] We can calculate hidden states and cell states using Keras's functional API. In [53]: batch_size = 1 len_ts = len ( xs ) nfeature = X_test . shape [ 2 ] inp = layers . Input ( batch_shape = ( batch_size , len_ts , nfeature ), name = \"input\" ) rnn , s , c = layers . LSTM ( hunits , return_sequences = True , stateful = False , return_state = True , name = \"RNN\" )( inp ) states = models . Model ( inputs = [ inp ], outputs = [ s , c , rnn ]) for layer in states . layers : for layer1 in model1 . layers : if layer . name == layer1 . name : layer . set_weights ( layer1 . get_weights ()) h_t_keras , c_t_keras , rnn = states . predict ( xs . reshape ( 1 , len_ts , 1 )) print ( \"h3={}\" . format ( h_t_keras )) print ( \"c3={}\" . format ( c_t_keras )) h3=[[0.14217362 0.01967374 0.19257708]] c3=[[0.22343878 0.03877562 0.45090497]] The hidden states and cell states from Keras and from my hand calculations reasonably agree. (But the two are not identical and I am not sure why...) In [54]: fig = plt . figure ( figsize = ( 9 , 4 )) ax = fig . add_subplot ( 1 , 2 , 1 ) ax . plot ( h_tm1 . flatten (), h_t_keras . flatten (), \"p\" ) ax . set_xlabel ( \"h by hand\" ) ax . set_ylabel ( \"h by Keras\" ) ax = fig . add_subplot ( 1 , 2 , 2 ) ax . plot ( c_tm1 . flatten (), c_t_keras . flatten (), \"p\" ) ax . set_xlabel ( \"h by hand\" ) ax . set_ylabel ( \"h by Keras\" ) plt . show () Obtain weights from LSTM Philippe RÃ©my commented how to obtain weights for forgate gatesm input gates, cell states and output gates. But this method seems outdated for the latest version of Keras. Here I try to extract LSTM weights by refering to LSTMCell definition at Keras's reccurent.py . In [55]: def get_LSTM_UWb ( weight ): ''' weight must be output of LSTM's layer.get_weights() W: weights for input U: weights for hidden states b: bias ''' warr , uarr , barr = weight gates = [ \"i\" , \"f\" , \"c\" , \"o\" ] hunit = uarr . shape [ 0 ] U , W , b = {},{},{} for i1 , i2 in enumerate ( range ( 0 , len ( barr ), hunit )): W [ gates [ i1 ]] = warr [:, i2 : i2 + hunit ] U [ gates [ i1 ]] = uarr [:, i2 : i2 + hunit ] b [ gates [ i1 ]] = barr [ i2 : i2 + hunit ] . reshape ( hunit , 1 ) return ( W , U , b ) def get_LSTMweights ( model1 ): for layer in model1 . layers : if \"LSTM\" in str ( layer ): w = layer . get_weights () W , U , b = get_LSTM_UWb ( w ) break return W , U , b def vectorize_with_labels ( W , U , b ): bs , bs_label , ws , ws_label , us , us_label = [],[],[],[],[],[] for k in [ \"i\" , \"f\" , \"c\" , \"o\" ]: temp = list ( W [ k ] . flatten ()) ws_label . extend ([ \"W_\" + k ] * len ( temp )) ws . extend ( temp ) temp = list ( U [ k ] . flatten ()) us_label . extend ([ \"U_\" + k ] * len ( temp )) us . extend ( temp ) temp = list ( b [ k ] . flatten ()) bs_label . extend ([ \"b_\" + k ] * len ( temp )) bs . extend ( temp ) weight = ws + us + bs wlabel = ws_label + us_label + bs_label return ( weight , wlabel ) Weights at every 10 epochs are plotted. The weights seem to converge stably. In [56]: from copy import copy df = {} for epoch in np . arange ( 0 , 200 , 10 ): model1 . load_weights ( \"weights{:03d}.hdf5\" . format ( epoch + 1 )) WUb = get_LSTMweights ( model1 ) weight , wlabel = vectorize_with_labels ( * WUb ) df [ \"{:03d}\" . format ( epoch )] = copy ( weight ) df = pd . DataFrame ( df , index = wlabel ) df = df [ np . sort ( df . columns )] plt . figure ( figsize = ( 15 , 15 )) sns . heatmap ( df ) plt . xlabel ( 'epoch' ) plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Extract weights from Keras's LSTM and calcualte hidden and cell states"},{"url":"Understand-Keras's-RNN-behind-the-scenes-with-a-sin-wave-example.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } Recurrent Neural Network (RNN) has been successful in modeling time series data. People say that RNN is great for modeling sequential data because it is designed to potentially remember the entire history of the time series to predict values. \"In theory\" this may be true. But when it comes to implementation of the RNN model in Keras, practitioners need to specify a \"length of time series\" in batch_shape: batch_shape = (N of time series in a batch, the length of time series, N of features) Well, I was very confused with this parameter at first. Why do I need to specify the length of time series when the model is meant to handle a sequence of potentially infinite length?? Where does this parameter come into play in the definition of the RNN model? Hindsight, these questions show my lack of understanding in back propagation through time (BPTT) algorithms. Nevertheless, there are not many good, concrete and simple explanations about the role of this parameter. The goal of this blog post is to help my-past-self and someone who is stack at the similar problems in understanding Keras's RNN model. I believe that the best way to understand models is to reproduce the model script by hands. Therefore, I will use a simple example (sin wave time series) to train a simple RNN (only 5 weights!!!!) and predict the sin wave values by hands. Here I will touch the concept of \"stateful\" and \"stateless\" prediction. I hope that this blog helps you understood the Keras's sequential model better. Reference Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras Stateful LSTM in Keras æ·±å±¤å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªKerasã§RNNã‚’ä½¿ã£ã¦sinæ³¢äºˆæ¸¬ In [1]: import sys print ( sys . version ) import tensorflow print ( tensorflow . __version__ ) import keras print ( keras . __version__ ) import pandas as pd import numpy as np import math import random import matplotlib.pyplot as plt % matplotlib inline 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] 1.5.0 2.1.3 Using TensorFlow backend. Generate a sin wave with no noise First, I create a function that generates sin wave with/without noise. Using this function, I will generate a sin wave with no noise. As this sin wave is completely deterministic, I should be able to create a model that can do prefect prediction the next value of sin wave given the previous values of sin waves! Here I generate period-10 sin wave, repeating itself 500 times, and plot the first few cycles. In [2]: def noisy_sin ( steps_per_cycle = 50 , number_of_cycles = 500 , random_factor = 0.4 ): ''' random_factor : amont of noise in sign wave. 0 = no noise number_of_cycles : The number of steps required for one cycle Return : pd.DataFrame() with column sin_t containing the generated sin wave ''' random . seed ( 0 ) df = pd . DataFrame ( np . arange ( steps_per_cycle * number_of_cycles + 1 ), columns = [ \"t\" ]) df [ \"sin_t\" ] = df . t . apply ( lambda x : math . sin ( x * ( 2 * math . pi / steps_per_cycle ) + random . uniform ( - 1.0 , + 1.0 ) * random_factor )) df [ \"sin_t_clean\" ] = df . t . apply ( lambda x : math . sin ( x * ( 2 * math . pi / steps_per_cycle ))) print ( \"create period-{} sin wave with {} cycles\" . format ( steps_per_cycle , number_of_cycles )) print ( \"In total, the sin wave time series length is {}\" . format ( steps_per_cycle * number_of_cycles + 1 )) return ( df ) steps_per_cycle = 10 df = noisy_sin ( steps_per_cycle = steps_per_cycle , random_factor = 0 ) n_plot = 8 df [[ \"sin_t\" ]] . head ( steps_per_cycle * n_plot ) . plot ( title = \"Generated first {} cycles\" . format ( n_plot ), figsize = ( 15 , 3 )) create period-10 sin wave with 500 cycles In total, the sin wave time series length is 5001 Out[2]: Create a training and testing data. Here, the controversial \"length of time series\" parameter comes into play. For now, we set this parameter to 2. In [3]: def _load_data ( data , n_prev = 100 ): \"\"\" data should be pd.DataFrame() \"\"\" docX , docY = [], [] for i in range ( len ( data ) - n_prev ): docX . append ( data . iloc [ i : i + n_prev ] . as_matrix ()) docY . append ( data . iloc [ i + n_prev ] . as_matrix ()) alsX = np . array ( docX ) alsY = np . array ( docY ) return alsX , alsY length_of_sequences = 2 test_size = 0.25 ntr = int ( len ( df ) * ( 1 - test_size )) df_train = df [[ \"sin_t\" ]] . iloc [: ntr ] df_test = df [[ \"sin_t\" ]] . iloc [ ntr :] ( X_train , y_train ) = _load_data ( df_train , n_prev = length_of_sequences ) ( X_test , y_test ) = _load_data ( df_test , n_prev = length_of_sequences ) print ( X_train . shape , y_train . shape , X_test . shape , y_test . shape ) ((3748, 2, 1), (3748, 1), (1249, 2, 1), (1249, 1)) Simple RNN model As a deep learning model, I consider the simplest possible RNN model: RNN with a single hidden unit followed by fully connected layer with a single unit. The RNN layer contains 3 weights: 1 weight for input, 1 weight for hidden unit, 1 weight for bias The fully connected layer contains 2 weights: 1 weight for input (i.e., the output from the previous RNN layer), 1 weight for bias In total, there are only 5 weights in this model. Let $x_t$ be the sin wave at time point $t$, then Formally, This simple model can be formulated in two lines as: $$ h_t = \\textrm{tanh}(x_t&#94;T w_{1x} + h_{t-1}&#94;T w_{1h}+ b_1)\\\\ x_{t+1} = h_t&#94;T w_2 + b_2 \\\\ $$ Conventionally $h_0=0$. Notice that the length of time series is not involved in the definition of the RNN. The model should be able to \"remember\" the past history of $x_t$ through the hidden unit $h_t$. batch_shape needs for BPTT. Every time when the model weights are updated, the BPTT uses only the randomly selected subset of the data. This means that the each batch is treated as independent. This batch_shape determines the size of this subset. Every batch starts will the initial hidden unit $h_0=0$. As we specify the length of the time series to be 2, our model only knows about the past 2 sin wave values to predict the next sin wave value. The practical limitation of the finite length of the time series defeats the theoretical beauty of RNN: the RNN here is not a model remembeing infinite past sequence!!! Now, we define this model using Keras and show the model summary. define_model It contains the stateful flag, and its default value is set to False, because this is the default setting in SimpleRNN method. We will set this flag to True and do the prediction later. It contains batch_size flag, and its default value is set to None, meaning that the batch_size can be decided during the training. None is the default for the Input layer when stateful = False. However, this value needs to be pre-specified when stateful = True. The details are discussed later. For the model definition, I use functional API, despite that you can readily define this model using Sequential API. The reason becomes clear later. In [4]: from keras.layers import Input from keras.models import Model from keras.layers.core import Dense , Activation from keras.layers.recurrent import SimpleRNN def define_model ( length_of_sequences , batch_size = None , stateful = False ): in_out_neurons = 1 hidden_neurons = 1 inp = Input ( batch_shape = ( batch_size , length_of_sequences , in_out_neurons )) rnn = SimpleRNN ( hidden_neurons , return_sequences = False , stateful = stateful , name = \"RNN\" )( inp ) dens = Dense ( in_out_neurons , name = \"dense\" )( rnn ) model = Model ( inputs = [ inp ], outputs = [ dens ]) model . compile ( loss = \"mean_squared_error\" , optimizer = \"rmsprop\" ) return ( model ,( inp , rnn , dens )) ## use the default values for batch_size, stateful model , ( inp , rnn , dens ) = define_model ( length_of_sequences = X_train . shape [ 1 ]) model . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 2, 1) 0 _________________________________________________________________ RNN (SimpleRNN) (None, 1) 3 _________________________________________________________________ dense (Dense) (None, 1) 2 ================================================================= Total params: 5 Trainable params: 5 Non-trainable params: 0 _________________________________________________________________ Now we train the model. The script was run without GPU. In [5]: hist = model . fit ( X_train , y_train , batch_size = 600 , epochs = 1000 , verbose = False , validation_split = 0.05 ) Plot of val_loss and loss. The validation loss and loss are exactly the same because our training data is a sin wave with no noise. Both validation and training data contain identical 10-period sin waves (with different number of cycles). The final validation loss is less than 0.001. In [6]: for label in [ \"loss\" , \"val_loss\" ]: plt . plot ( hist . history [ label ], label = label ) plt . ylabel ( \"loss\" ) plt . xlabel ( \"epoch\" ) plt . title ( \"The final validation loss: {}\" . format ( hist . history [ \"val_loss\" ][ - 1 ])) plt . legend () plt . show () The plot of true and predicted sin waves look nearly identical In [7]: y_pred = model . predict ( X_test ) plt . figure ( figsize = ( 19 , 3 )) plt . plot ( y_test , label = \"true\" ) plt . plot ( y_pred , label = \"predicted\" ) plt . legend () plt . show () What are the model weights? The best way to understand the RNN model is to create a model from scratch. Let's extract the weights and try to reproduce the predicted values from the model by hands. The model weights can be readily obtained from the model.layers. In [8]: ws = {} for layer in model . layers : ws [ layer . name ] = layer . get_weights () ws Out[8]: {'RNN': [array([[-0.41514578]], dtype=float32), array([[-0.64529276]], dtype=float32), array([0.00049243], dtype=float32)], 'dense': [array([[-3.950179]], dtype=float32), array([-0.00039617], dtype=float32)], 'input_1': []} What are the predicted values of hidden units? Since we used Keras's functional API to develop a model, we can easily see the output of each layer by compiling another model with outputs specified to be the layer of interest. In order to use the .predict() function, we need to compile the model, which requires specifying loss and optimizer. You can choose any values of loss and optimizer here, as we do not actually optimize this loss function. The newly created model \"rnn_model\" shares the weights obtained by the previous model's optimization. Therefore for the purpose of visualizing the hidden unit values with the current model result, we do not need to do additional optimizations. In [9]: rnn_model = Model ( inputs = [ inp ], outputs = [ rnn ]) rnn_model . compile ( loss = \"mean_squared_error\" , optimizer = \"rmsprop\" ) hidden_units = rnn_model . predict ( X_test ) . flatten () Plot shows that the predicted hidden unit is capturing the wave shape. Scaling and shifting of the predicted hidden unit yield the predicted sin wave. In [10]: upto = 100 predicted_sin_wave = ws [ \"dense\" ][ 0 ][ 0 ][ 0 ] * hidden_units + ws [ \"dense\" ][ 1 ][ 0 ] plt . figure ( figsize = ( 19 , 3 )) plt . plot ( y_test [: upto ], label = \"y_pred\" ) plt . plot ( hidden_units [: upto ], label = \"hidden units\" ) plt . plot ( predicted_sin_wave [: upto ], \"*\" , label = \"w2 * hidden units + b2\" ) plt . legend () plt . show () Obtain predicted sin wave at the next time point given the current sin wave by hand We understand that how the predicted sin wave values can be obtained using the predicted hidden states from Keras. But how does the predicted hidden states generated from the original inputs i.e. the current sin wave? Here, stateful and stateless prediction comes into very important role. Following the definition of the RNN, we can write a script for RNNmodel as: In [11]: def RNNmodel ( ws , x , h = 0 ): ''' ws: predicted weights x : scalar current sign value h : scalar RNN hidden unit ''' h = np . tanh ( x * ws [ \"RNN\" ][ 0 ][ 0 ][ 0 ] + h * ws [ \"RNN\" ][ 1 ][ 0 ][ 0 ] + ws [ \"RNN\" ][ 2 ][ 0 ]) x = h * ws [ \"dense\" ][ 0 ][ 0 ][ 0 ] + ws [ \"dense\" ][ 1 ][ 0 ] return ( x , h ) Naturally, you can obtain the predicted sin waves ($x_1,x_2,...,x_t$) by looping around RNNmodel as: $$ x&#94;*_{t+1}, h_{t+1} = \\textrm{RNNmodel}(x_{t},h_{t}) $$ Here $x&#94;*_t$ indicates the estimated value of x at time point $t$. As our model is not so complicated, we can readily implement this algorithm as: In [12]: upto = 50 ## predict the first sin values xstars , hs_hand = [], [] for i , x in enumerate ( df_test . values ): if i == 0 : h = 0 ## initial hidden layer value is zero xstar = x print ( \"initial value of sin x_0 = {}, h_0 = {}\" . format ( x , h )) hs_hand . append ( h ) xstars . append ( xstar [ 0 ]) xstar , h = RNNmodel ( ws , x , h ) assert len ( df_test . values ) == len ( xstars ) initial value of sin x_0 = [-1.27375647e-13], h_0 = 0 In this formulation, x_stars[t] contains the prediction of sin wave at time point t just as df_test In [13]: plt . figure ( figsize = ( 18 , 3 )) plt . plot ( df_test . values [: upto ], label = \"true\" , alpha = 0.3 , linewidth = 5 ) plt . plot ( xstars [: upto ], label = \"sin prediction (xstar)\" ) plt . plot ( hs_hand [: upto ], label = \"hidden state (xstar)\" ) plt . legend () Out[13]: You can see that the model prediction is not good in the first few time points and then stabilized. OK. My model seems to over estimates the values when sin wave is going down and underestimates when the sin wave is going up. However, there is one question: this model returns almost zero validation loss. The error seems a bit high. In fact the error from the prediction above is quite large. What is going on? In [14]: \"validation loss {:3.2f}\" . format ( np . mean (( np . array ( xstars ) - df_test [ \"sin_t\" ] . values ) ** 2 )) Out[14]: 'validation loss 0.08' Let's predict the sin wave using the existing predict function from Keras. Remind you that we prepare X_test when X_train was defined. X_test contains data as: $ x_1, x_2\\\\ x_2, x_3\\\\ x_3, x_4\\\\ ... $ In [15]: y_test_from_keras = model . predict ( X_test ) . flatten () Notice that this predicted values are exactly the same as the ones calculated before. In [16]: np . all ( predicted_sin_wave == y_test_from_keras ) Out[16]: True As the prediction starts from x_3, add the 2 NaN into a predicted vector as placeholders. This is just to make sure that the length of y_test_from_keras is compatible with xtars. In [17]: y_test_from_keras = [ np . NaN , np . NaN ] + list ( y_test_from_keras . flatten ()) h_test_from_keras = [ np . NaN , np . NaN ] + list ( hidden_units . flatten ()) The plot shows that Keras's predicted values are almost perfect and the validation loss is nearly zero. Clearly xstars are different from the Keras's prediction. It seems that the predicted states from Keras and from by hand are also slightly different. Then question is, how does Keras predict the output? In [18]: plt . figure ( figsize = ( 18 , 3 )) plt . plot ( df_test . values [: upto ], label = \"true\" , alpha = 0.3 , linewidth = 5 ) plt . plot ( xstars [: upto ], label = \"sin prediction (xstar)\" ) plt . plot ( hs_hand [: upto ], label = \"hidden state (xstar)\" ) plt . plot ( y_test_from_keras [: upto ], label = \"sin prediction (keras)\" ) plt . plot ( h_test_from_keras [: upto ], label = \"hidden state (keras)\" ) plt . legend () print ( \"validation loss {:6.5f}\" . format ( np . nanmean (( np . array ( y_test_from_keras ) - df_test [ \"sin_t\" ] . values ) ** 2 ))) validation loss 0.00017 Here, the technical details of the BPTT algorithm comes in, and the time series length parameter (i.e., batch_size[1]) takes very important role. As the BPTT algorithm only passed back 2 steps, the model assumes that: the hidden units are initialized to zero every 2 steps. the prediction of the next sin value ($x_{t+1}$) is based on the hidden unit ($h_t$) which is created by updating the hidden units twice in the past assuming that $h_{t-1}=0$. $$ x_t&#94;{*} ,h_t = \\textrm{RNNmodel}(x_{t-1},0)\\\\ x_{t+1}, - = \\textrm{RNNmodel}(x_{t},h_{t}) $$ Note that the intermediate predicted sin $x_t&#94;{*} $ based on $h_{t-1}=0$ should not be used as the predicted sin value. This is because the $x_t&#94;*$ was not directly used to evaluate the loss function. Finally, obtain the Keras's predicted sin wave at the next time point given the current sin wave by hand. In [19]: def myRNNpredict ( ws , X ): X = X . flatten () h = 0 for i in range ( len ( X )): x , h = RNNmodel ( ws , X [ i ], h ) return ( x , h ) xs , hs = [], [] for i in range ( X_test . shape [ 0 ]): x , h = myRNNpredict ( ws , X_test [ i ,:,:]) xs . append ( x ) hs . append ( h ) In [20]: print ( \"All sin estimates agree with ones from Keras = {}\" . format ( np . all ( np . abs ( np . array ( xs ) - np . array ( y_test_from_keras [ 2 :]) ) < 1E-5 ))) print ( \"All hidden state estmiates agree with ones fome Keras = {}\" . format ( np . all ( np . abs ( np . array ( hs ) - np . array ( h_test_from_keras [ 2 :]) ) < 1E-5 )) ) All sin estimates agree with ones from Keras = True All hidden state estmiates agree with ones fome Keras = True Now we understand how Keras is predicting the sin wave. In fact, Keras has a way to return xstar as predicted values, using \"stateful\" flag. This stateful is a notorious parameter and many people seem to be very confused. But by now you can understand what this stateful flag is doing, at least during the prediction phase. When stateful = True, you can decide when to reset the states to 0 by yourself. In order to predict in \"stateful\" mode, we need to re-define the model with stateful = True. When stateful is True, we need to specify the exact integer for batch_size. As we only have a single sin time series, we will set the batch_size to 1. In [21]: model_stateful , _ = define_model ( length_of_sequences = 1 , batch_size = 1 , stateful = True ) model_stateful . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) (1, 1, 1) 0 _________________________________________________________________ RNN (SimpleRNN) (1, 1) 3 _________________________________________________________________ dense (Dense) (1, 1) 2 ================================================================= Total params: 5 Trainable params: 5 Non-trainable params: 0 _________________________________________________________________ Assign the trained weights into the stateful model. In [22]: for layer in model . layers : for layer_predict in model_stateful . layers : if ( layer_predict . name == layer . name ): layer_predict . set_weights ( layer . get_weights ()) break Now we predict in stateful mode. Here it is very important to reset_state() before the prediction so that $h_0 = 0$. In [51]: pred = df_test . values [ 0 ][ 0 ] stateful_sin = [] model_stateful . reset_states () for i in range ( df_test . shape [ 0 ]): stateful_sin . append ( pred ) pred = model_stateful . predict ( df_test . values [ i ] . reshape ( 1 , 1 , 1 ))[ 0 ][ 0 ] stateful_sin = np . array ( stateful_sin ) In [57]: print ( \"All predicted sin values with stateful model agree to xstars = {}\" . format ( np . all ( np . abs ( np . array ( stateful_sin ) - np . array ( xstars )) < 1E-5 ))) All predicted sin values with stateful model agree to xstars = True Now we understand that xstars is the prediction result when stateful = True. We also understand that the prediction results are way better when stateful = False at least for this sin wave example. However, the prediction with stateful = False brings to some awkwardness: what if our batch have a very long time series of length, say $K$? Do we always have to go back all the $K$ time steps, set $h_{t-K}=0$ and then feed forward $K$ steps in order to predict at the time point $t$? This may be computationally intense. Next step Assess stateful VS stateless prediction with more examples and confirm whether the stateful prediction is ALWAYS worse. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Understand Keras's RNN behind the scenes with a sin wave example -  Stateful and Stateless prediction -"},{"url":"create-a-simple-game-using-pygame.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } The above gif is my very first game. In this blog post, I will introduce a very simple game created using pygame. What is this game about? In this game, you are a \"princess\" and the goal is to rescue \"price\". But be careful, there is a snake monster that could hit you! I used pygame. This pygame library is really REALLY amazing for creating a game. I originally thought that creating game needs a \"serious\" computer science degree, and it is out of my reach. Creating graphical user interface seemed very complicated! How can my script accepts keyboard input, do calculation and then update the game screen accodingly? No worry, pygame will take care of such things, so you can focus on creating game logics (e.g., how to score points, what decides \"game over\"). In fact the game above requires only about 400 lines. Where did I start? Did I take any online courses? Well, yes and no. All I did was follow this great youtube series from thenewboston . This series has many tutorial videos. I watched the first 40 (40!). Each tutorial is nicely modularized and takes only about 5 - 10 minutes. I would love to watch the rest later in my life! Game features If you are interested in learning pygame from scratch, you should go to thenewboston . Here, I will show some features of my game, and how I did them. The keyboad input are extracted in less than 20 lines. This is not really features of my game but I want to say where this is defined. Its defined Here The game is 400 x 400 pixcels You can specify the number of pixcels by specifying npix_x and npix_y inputs of the SavePrince object. But the game display is really defined in the line: Here The snake monster hunts you down. I added this functionality to make the game more challenging. It will always move toward you. For example, if you are on the right of the snake monster and above it, then the snake monster will go up or right. The decision of going up or going right is based on randomness. Defined here The snake monster appears around prince. This is another fuctionality to make the game more challenging. While you want to go closer to prince, you need to avoid the snake that is around the prince. Defined ere The speed of the snake monster increases when it moves toward the same direction consequtively. Even if the snake seems to be far away from you, it will get you really quick if the snake is on the same row or on the same column. Defined here for princess Defined here for the snake monster How to play the game? To run the game, download the whole SavePrinceGame repository, pip install pygame, and then within the SavePrinceGame repository and on your terminal, type: python2 SavePrince.py if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Create a simple game using pygame"},{"url":"Create-deep-learning-calculators-based-on-Encoder-Decoder-RNN-using-Keras.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } Google search is great. You can ask any question you have and give you links to the potential solutions. It can sometimes give you the solution itself when the question is simple enough. One of such example is that google search can act as a calcualator. You can ask \"1 + 1\" or \"1 + 1\" to get the value 2. It somehow knows that the spaces are nuicense and return correct value. You can even ask the calculation with some strings. See pic below: In [3]: from keras.preprocessing.image import ImageDataGenerator , img_to_array , load_img load_img ( \"./pic/Capture2.PNG\" ) Using TensorFlow backend. Out[3]: Somehow, google understand that what I am asking is \"32+123\" and do the calculations. Can we do the same using deep learning model that takes a string as input? I was fascinated by one of the Keras's examples in Github called addition rnn . This script shows the implementation of sequence to sequence learning for performing addition. The script considers the summation of two 3-digit numbers, for example, 123+420=543. The input of the model is a string \"123+429\" and output is \"543\". In this blog post, we try to understand how the RNNs learn calculations. We will first consider very VERY simple example of summation of two 1-digits, then we will work on more complex senario. In [2]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import sys import warnings warnings . filterwarnings ( \"ignore\" ) print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )) print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"0\" #### 2 GPU1 #### 0 GPU3 #### 4 GPU4 #### 3 GPU2 set_session ( tf . Session ( config = config )) def set_seed ( sd = 123 ): from numpy.random import seed from tensorflow import set_random_seed import random as rn ## numpy random seed seed ( sd ) ## core python's random number rn . seed ( sd ) ## tensor flow's random number set_random_seed ( sd ) python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.0.6 tensorflow version 1.2.1 Create one-hot encoders that takes string and return one-hot encoded matrix of size as many as the number of characters in the string This class is bollowed from addition rnn with no change. In [3]: class CharacterTable ( object ): \"\"\"Given a set of characters: + Encode them to a one hot integer representation + Decode the one hot integer representation to their character output + Decode a vector of probabilities to their character output \"\"\" def __init__ ( self , chars ): \"\"\"Initialize character table. # Arguments chars: Characters that can appear in the input. \"\"\" self . chars = sorted ( set ( chars )) self . char_indices = dict (( c , i ) for i , c in enumerate ( self . chars )) self . indices_char = dict (( i , c ) for i , c in enumerate ( self . chars )) def encode ( self , C , num_rows ): \"\"\"One hot encode given string C. # Arguments num_rows: Number of rows in the returned one hot encoding. This is used to keep the # of rows for each data the same. \"\"\" x = np . zeros (( num_rows , len ( self . chars ))) for i , c in enumerate ( C ): x [ i , self . char_indices [ c ]] = 1 return x def decode ( self , x , calc_argmax = True ): if calc_argmax : x = x . argmax ( axis =- 1 ) return '' . join ( self . indices_char [ x ] for x in x ) class colors : ok = ' \\033 [92m' fail = ' \\033 [91m' close = ' \\033 [0m' RNN model for summation of two single digits. Generate 10,000 samples. As we are only considering the summation of two single digits, there are only 10 x 10 = 100 possible samples. Sampling 10,000 samples means we are sampling identical samples 100 times on average. In [4]: import numpy as np set_seed ( 123 ) # Parameters for the model and dataset. TRAINING_SIZE = 10000 DIGITS = 1 # Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of # int is DIGITS. MAXLEN = DIGITS + 1 + DIGITS # All the numbers, plus sign and space for padding. chars = '0123456789+ ' ctable = CharacterTable ( chars ) f = lambda : int ( '' . join ( np . random . choice ( list ( '0123456789' )) for i in range ( np . random . randint ( 1 , DIGITS + 1 )))) questions = [] expected = [] print ( 'Generating data...' ) while len ( questions ) < TRAINING_SIZE : if len ( questions ) % 1000 == 0 : print ( \"{} samples are generated...\" . format ( len ( questions ))) a , b = f (), f () q = '{}+{}' . format ( a , b ) query = q + ' ' * ( MAXLEN - len ( q )) ans = str ( a + b ) # Answers can be of maximum size DIGITS + 1. ans += ' ' * ( DIGITS + 1 - len ( ans )) questions . append ( query ) expected . append ( ans ) print ( 'Total addition questions:{}' . format ( len ( questions ))) Generating data... 0 samples are generated... 1000 samples are generated... 2000 samples are generated... 3000 samples are generated... 4000 samples are generated... 5000 samples are generated... 6000 samples are generated... 7000 samples are generated... 8000 samples are generated... 9000 samples are generated... Total addition questions:10000 Example data strings look like: In [5]: for q , a in zip ( questions , expected )[: 10 ]: print ( \"{:3} = {:4}\" . format ( q , a )) 2+2 = 4 6+1 = 7 3+9 = 12 6+1 = 7 0+1 = 1 9+0 = 9 0+9 = 9 3+4 = 7 0+0 = 0 4+1 = 5 Using the class previously created, we one-hot encode each character. Input There are 3 characters per sequence and each character could potential take 12 values (\"0123456789+ \"). The input of the model is 3 one-hot encoded vectors each of which represents a single character. Hence one sample sequence is represented as 3 by 12. The example of the one-hot encoded vectors ($\\boldsymbol{x}_{i,t} \\in R&#94;{12}$ The time index take values: $t=1,2,3$.) are: $\\boldsymbol{x}_{i,t_1}=[0,0,0,0,0,0,1,0,0,0,0,0]$ indicates \"6\" as the 6th position (position counting starting from 0) is nonzero. $\\boldsymbol{x}_{i,t_2}=[0,0,0,0,0,0,0,0,0,0,1,0]$ indicates \"+\". $\\boldsymbol{x}_{i,t_3}=[1,0,0,0,0,0,0,0,0,0,0,0]$ indicates \"0\". $[\\boldsymbol{x}_{i,t_1},\\boldsymbol{x}_{i,t_2},\\boldsymbol{x}_{i,t_3}]$ together represents a single sentence \"6+0\" $\\boldsymbol{x}_{i,t_4}=[0,0,0,0,0,0,0,0,0,0,0,1]$ indicates empty space i.e., \" \" Targets The targets are 2 one-hot encoded vectors, and each one-hot encoded vector must have length 12. If the solution is a single digit (for example, the solution to \"0+6\" is \"6\" and \"6\" is a single digit), then solution has to be \"6 \", which can be represetned as $[\\boldsymbol{x}_{i,t_1}, \\boldsymbol{x}_{i,t_4}]$ using the notation above. In practice we use $y$ to represent the target vectors. $ \\boldsymbol{y}_{i,k} \\in [0,1]&#94;{12}, k=1,2 $ In [6]: def one_hot_encoder ( expected , questions , x_dim , y_dim , chars , ctable ): print ( 'Vectorization...' ) x = np . zeros (( len ( questions ), x_dim , len ( chars )), dtype = np . bool ) y = np . zeros (( len ( questions ), y_dim , len ( chars )), dtype = np . bool ) for i , sentence in enumerate ( questions ): x [ i ] = ctable . encode ( sentence , x_dim ) for i , sentence in enumerate ( expected ): y [ i ] = ctable . encode ( sentence , y_dim ) return ( x , y ) x , y = one_hot_encoder ( expected , questions , MAXLEN , DIGITS + 1 , chars , ctable ) Vectorization... Split between training and testing In [7]: def split_train_test ( x , y ): # Explicitly set apart 10% for validation data that we never train over. split_at = len ( x ) - len ( x ) // 10 ( x_train , x_val ) = x [: split_at ], x [ split_at :] ( y_train , y_val ) = y [: split_at ], y [ split_at :] return ( x_train , x_val ),( y_train , y_val ) ( x_train , x_val ),( y_train , y_val ) = split_train_test ( x , y ) print ( 'Training Data:' ) print ( x_train . shape ) print ( y_train . shape ) Training Data: (9000, 3, 12) (9000, 2, 12) Model Definition We consider encoder-decoder RNN models. The RNN is previously discussed here . The encoder-decoder model is often used in the field of machine translation, and it has been explained by many blogs. So I will not give you a nice smooth introduction, but interested readers should read (or at least see the graph) Peeking into the neural network architecture used for Google's Neural Machine Translation or How Does Attention Work in Encoder-Decoder Recurrent Neural Networks . As the model's name suggests, from a high-level, the model is comprised of two sub-models: an encoder and a decoder. Encoder: The encoder is responsible for stepping through the input time steps and encoding the entire sequence into a fixed length vector called a context vector. Decoder: The decoder is responsible for stepping through the output time steps while reading from the context vector. The original addition rnn considers the solutions to the summation of two 3-or-less-digit numbers. The model used was an encoder-decoder RNN with 208,529 parameters. As there are 1 million potential combination sample (1,000 x 1,000), this number of parameters may be necessary considering the complexity of the problem. However, in my example, I only consider the summatoin of two single digit numbers, meaning that there are only 10 x 10 = 100 unique samples. The function with 100 if-else statements can do the perfect jobs in predicting the outcome. So I should be able to create a model with less than 100 parameters! Simpler model is preferable also because it is easier to understand. So for now, we consider an encoder-decoder model with 77 parameters. The model defenition follows: HIDDEN_ENCODER=1 HIDDEN_DECODER=3 In [9]: load_img ( \"./pic/encoder_decoder.png\" ) Out[9]: Encoder layer: In total there are 48 parameters ( = 12x1 + 1x1 + 1 = 12 + 1 + 1 = 14) $ \\boldsymbol{w}_{e} \\in R&#94;{\\textrm{HIDDEN_ENCODER x 12}},\\boldsymbol{w}_{h_{e}} \\in R&#94;{\\textrm{HIDDEN_ENCODER x HIDDEN_ENCODER}}, \\boldsymbol{b}_e \\in R&#94;{\\textrm{HIDDEN_ENCODER}} $ layer definition $ \\boldsymbol{e}_{i,0}=0\\in R&#94;{\\textrm{HIDDEN_ENCODER}}\\\\ \\boldsymbol{e}_{i,t} = \\textrm{tanh}(\\boldsymbol{x}_{i,t}&#94;T \\boldsymbol{w}_{e} + \\boldsymbol{e}_{i,t-1}&#94;T \\boldsymbol{w}_{e} +\\boldsymbol{b}_e) \\\\ $ Decoder layer: In total there are 21 parameters ( = 3x1 + 3x3 + 3 = 3 + 9 + 3 = 15) $ \\boldsymbol{w}_{d} \\in R&#94;{\\textrm{HIDDEN_DECODER x HIDDEN_ENCODER}},\\boldsymbol{w}_{h_{d}} \\in R&#94;{\\textrm{HIDDEN_DECODER x HIDDEN_DECODER}}, \\boldsymbol{b}_d \\in R&#94;{\\textrm{HIDDEN_DECODER}} $ layer definition $k=1,2$ $ \\boldsymbol{d}_{i,0}=0\\in R&#94;{\\textrm{HIDDEN_DECODER}}\\\\ \\boldsymbol{d}_{i,k} = \\textrm{tanh}(\\boldsymbol{e}_{i,3}&#94;T \\boldsymbol{w}_{d} + \\boldsymbol{d}_{i,k-1}&#94;T \\boldsymbol{w}_{d} +\\boldsymbol{b}_d) \\\\ $ Time-distributed Dense layer: In total there are 48 parameters ( = 12x3 + 12 = 48) $ \\boldsymbol{w}_{dense}\\in R&#94;{\\textrm{12 x HIDDEN_DECODER}}\\boldsymbol{b}_{dense}\\in R&#94;{12}\\\\ $ layer defenition $ \\boldsymbol{y}_{i,k} = \\textrm{softmax}(\\boldsymbol{d}_{i,k}&#94;T \\boldsymbol{w}_{dense} + \\boldsymbol{b}_{dense}) $ In [8]: import keras.layers as layers from keras.models import Model from keras.layers.core import Dense print ( 'Build model...' ) def define_model ( MAXLEN_x , MAXLEN_y , chars , HIDDEN_ENCODER , HIDDEN_DECODER , RNN ): def scalarToList ( a ): if not isinstance ( a , list ): a = [ a ] lena = len ( a ) return ( a , lena ) def return_seq ( nlayer , lenEncoder ): ''' return sequence must be False if this layer is the final one ''' return ( False if nlayer == lenENCODER else True ) HIDDEN_ENCODER , lenENCODER = scalarToList ( HIDDEN_ENCODER ) HIDDEN_DECODER , lenDECODER = scalarToList ( HIDDEN_DECODER ) inp = layers . Input ( batch_shape = ( None , MAXLEN_x , len ( chars )), name = \"Input\" ) # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE. encoder = RNN ( HIDDEN_ENCODER [ 0 ], return_sequences = return_seq ( 1 , lenENCODER ), name = \"encoder1\" )( inp ) nlayer = 1 if lenENCODER > 1 : for HE in HIDDEN_ENCODER [ 1 :]: nlayer += 1 encoder = RNN ( HE , return_sequences = return_seq ( nlayer , lenENCODER ), name = \"encoder\" + str ( nlayer ))( encoder ) rep_encoder = layers . RepeatVector ( MAXLEN_y , name = \"repeat_vector\" )( encoder ) # By setting return_sequences to True, return not only the last output but # all the outputs so far in the form of (num_samples, timesteps, # output_dim). This is necessary as TimeDistributed in the below expects # the first dimension to be the timesteps. decoder = RNN ( HIDDEN_DECODER [ 0 ], return_sequences = True , name = \"decoder1\" )( rep_encoder ) if lenDECODER > 1 : for HE in HIDDEN_DECODER [ 1 :]: decoder = RNN ( HE , return_sequences = True , name = name + str ( nlayer ))( decoder ) # Apply a dense layer to the every temporal slice of an input. For each of step # of the output sequence, decide which character should be chosen. #out = layers.Dense(len(chars))(decoder) out = layers . TimeDistributed ( layers . Dense ( len ( chars ), activation = \"softmax\" ), name = \"time_distributed_dense\" )( decoder ) model = Model ( inputs = [ inp ], outputs = [ out ]) encoder = Model ( inputs = [ inp ], outputs = [ rep_encoder ]) model . summary () return ( model , encoder ) HIDDEN_ENCODER = 1 #128 HIDDEN_DECODER = 3 #128 model , encoder = define_model ( MAXLEN , DIGITS + 1 , chars , HIDDEN_ENCODER , HIDDEN_DECODER , RNN = layers . SimpleRNN ) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) Build model... _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= Input (InputLayer) (None, 3, 12) 0 _________________________________________________________________ encoder1 (SimpleRNN) (None, 1) 14 _________________________________________________________________ repeat_vector (RepeatVector) (None, 2, 1) 0 _________________________________________________________________ decoder1 (SimpleRNN) (None, 2, 3) 15 _________________________________________________________________ time_distributed_dense (Time (None, 2, 12) 48 ================================================================= Total params: 77 Trainable params: 77 Non-trainable params: 0 _________________________________________________________________ Define printing functions for model training In [9]: def print_predicted ( x , y , model , ctable , colors ): for i in range ( x . shape [ 0 ]): rowx , rowy = x [[ i ]], y [[ i ]] preds = model . predict ( rowx , verbose = 0 ) preds = preds . argmax ( axis = 2 ) q = ctable . decode ( rowx [ 0 ]) correct = ctable . decode ( rowy [ 0 ]) guess = ctable . decode ( preds [ 0 ], calc_argmax = False ) print ( ' Q:{}' . format ( q )), print ( 'T:{}' . format ( correct )), print ( 'Model:{}' . format ( guess )), if correct == guess : print ( colors . ok + 'â˜‘' + colors . close ) else : print ( colors . fail + 'â˜’' + colors . close ) Model Training In [10]: def train ( model , ctable , x_train , y_train , x_val , y_val , nb_epochs , print_every , BATCH_SIZE ): history = { 'acc' :[], 'loss' :[], 'val_acc' :[], 'val_loss' :[]} for iteration in range ( 1 , nb_epochs ): hist = model . fit ( x_train , y_train , batch_size = BATCH_SIZE , epochs = 1 , verbose = False , validation_data = ( x_val , y_val )) ## printing if iteration % print_every == 0 : print ( '-' * 50 ) print ( 'Iteration {}' . format ( iteration )) for key in hist . history . keys (): history [ key ] . append ( hist . history [ key ][ 0 ]) if iteration % print_every == 0 : print ( \"{}:{:4.3f}\" . format ( key , hist . history [ key ][ 0 ])), if iteration % print_every == 0 : print ( \"\" ) # Select 10 samples from the validation set at random so we can visualize # errors. if iteration % print_every == 0 : index = np . random . randint ( 0 , len ( x_val ), 10 ) print_predicted ( x_val [ index ], y_val [ index ], model , ctable , colors ) return ( history ) In [11]: print ( x_train . shape , y_train . shape , x_val . shape , y_val . shape ) set_seed () history = train ( model , ctable , x_train , y_train , x_val , y_val , nb_epochs = 2001 , print_every = 400 , BATCH_SIZE = 128 ) ((9000, 3, 12), (9000, 2, 12), (1000, 3, 12), (1000, 2, 12)) -------------------------------------------------- Iteration 400 acc:0.909 loss:0.435 val_acc:0.908 val_loss:0.437 Q:6+0 T:6 Model:6 â˜‘ Q:6+1 T:7 Model:7 â˜‘ Q:3+9 T:12 Model:12 â˜‘ Q:2+8 T:10 Model:10 â˜‘ Q:2+9 T:11 Model:11 â˜‘ Q:5+9 T:14 Model:13 â˜’ Q:8+6 T:14 Model:13 â˜’ Q:6+9 T:15 Model:12 â˜’ Q:0+0 T:0 Model:5 â˜’ Q:0+6 T:6 Model:6 â˜‘ -------------------------------------------------- Iteration 800 acc:0.958 loss:0.233 val_acc:0.950 val_loss:0.234 Q:4+2 T:6 Model:6 â˜‘ Q:9+2 T:11 Model:11 â˜‘ Q:8+4 T:12 Model:12 â˜‘ Q:3+9 T:12 Model:12 â˜‘ Q:2+7 T:9 Model:9 â˜‘ Q:0+3 T:3 Model:4 â˜’ Q:3+3 T:6 Model:6 â˜‘ Q:6+0 T:6 Model:6 â˜‘ Q:6+3 T:9 Model:9 â˜‘ Q:6+9 T:15 Model:15 â˜‘ -------------------------------------------------- Iteration 1200 acc:0.985 loss:0.142 val_acc:0.985 val_loss:0.142 Q:3+2 T:5 Model:5 â˜‘ Q:1+7 T:8 Model:8 â˜‘ Q:7+4 T:11 Model:11 â˜‘ Q:6+7 T:13 Model:13 â˜‘ Q:3+1 T:4 Model:4 â˜‘ Q:5+8 T:13 Model:13 â˜‘ Q:9+1 T:10 Model:10 â˜‘ Q:6+5 T:11 Model:11 â˜‘ Q:5+7 T:12 Model:12 â˜‘ Q:1+2 T:3 Model:3 â˜‘ -------------------------------------------------- Iteration 1600 acc:0.993 loss:0.103 val_acc:0.985 val_loss:0.105 Q:3+4 T:7 Model:7 â˜‘ Q:5+6 T:11 Model:11 â˜‘ Q:4+7 T:11 Model:11 â˜‘ Q:0+4 T:4 Model:4 â˜‘ Q:1+4 T:5 Model:5 â˜‘ Q:3+9 T:12 Model:12 â˜‘ Q:5+6 T:11 Model:11 â˜‘ Q:3+5 T:8 Model:8 â˜‘ Q:8+8 T:16 Model:16 â˜‘ Q:2+0 T:2 Model:3 â˜’ -------------------------------------------------- Iteration 2000 acc:1.000 loss:0.082 val_acc:1.000 val_loss:0.083 Q:2+7 T:9 Model:9 â˜‘ Q:3+3 T:6 Model:6 â˜‘ Q:2+5 T:7 Model:7 â˜‘ Q:1+2 T:3 Model:3 â˜‘ Q:1+2 T:3 Model:3 â˜‘ Q:9+6 T:15 Model:15 â˜‘ Q:3+7 T:10 Model:10 â˜‘ Q:5+2 T:7 Model:7 â˜‘ Q:2+3 T:5 Model:5 â˜‘ Q:8+4 T:12 Model:12 â˜‘ The validation loss and the training loss This small model performs surprisingly well. Since both training and testing data contain all possible samples, the model performance in the two data is the same. In [12]: def plot_loss_acc ( history ): fig = plt . figure ( figsize = ( 20 , 8 )) labels = [[ \"acc\" , \"val_acc\" ],[ \"loss\" , \"val_loss\" ]] count = 1 for ilab in range ( len ( labels )): ax = fig . add_subplot ( 2 , 1 , count ) count += 1 for label in labels [ ilab ]: ax . plot ( history [ label ], label = label ) ax . set_xlabel ( \"epochs\" ) plt . legend () plt . show () plot_loss_acc ( history ) Plot encoders My encoder only has 1 dimention. I plot the values of this encoder for every possible sample values. The plot shows that 1 + 3, 2 + 2 and 3 + 1 receive the same encoder value. Similarly, 1 + 5, 2 + 4, 3 + 3, 4 + 2 and 1 + 5 receive the same encoder value. In fact, plot shows that the ecoder seems to record the (scaled) values of summation. Encoders discard the infomation of what original digits were and remember only the summation values. In [13]: import seaborn as sns import pandas as pd hidden = { \"encoder\" : [], \"num1\" :[], \"num2\" :[]} for num1 in range ( 10 ): for num2 in range ( 10 ): string = \"{}+{}\" . format ( num1 , num2 ) myx = ctable . encode ( string , MAXLEN ) e = encoder . predict ( myx . reshape ( 1 , myx . shape [ 0 ], myx . shape [ 1 ]))[ 0 ] ## notice that the predicted encoders are duplicated twice k = 1 , 2 ## because of repeatVector ## we only extract one of the vector. if ~ np . all ( e [ 0 ] == e [ 1 ]): ## should never be TRUE! print ( \"ERROR!\" ) h = e [ 0 ] hidden [ \"encoder\" ] . append ( h [ 0 ]) hidden [ \"num1\" ] . append ( num1 ) hidden [ \"num2\" ] . append ( num2 ) hidden = pd . DataFrame ( hidden ) sns . set () plt . figure ( figsize = ( 10 , 8 )) sns . heatmap ( hidden . pivot ( \"num1\" , \"num2\" , \"encoder\" )) plt . title ( \"encoder \\n num1 + num2\" ) plt . show () Deep learning calculators for more complex calculations Now I consider creating a deep learning calculator for more complex calculations: Mupltiple operations: summation (+), negation (-), multiplication (*) and devision (/). Number of digits up to 3 (i.e., 0 - 999) as an input Allow \" \" to appear in the input calculation e.g., \"1 23 + 3 12\" = \"123+312\". However, output is clean: it does not contain space in between digits. This results in more characters in my character dictionary. In addition to '0123456789+ ', now I have '-*/Na'. Na arises because when a number is devided by 0 the output sould be \"NaN \". I consider integer devision. This means, for example, 1/3 = 0, 5/3=1. I tried to train a model with the trainig data having the same size, i.e., 10,000. But it seemd that 10,000 samples is not enough to create a generalizable model. This makes sense because previously we only have 100 combinatiosn of problems (=10x10). But now, there are 4 million (4,000,000 = 4*1,000&#94;2) combinations of problems (even excluding the spaces that can randomly appear in the left hand side of the equation). The problem is much more complex! I demonstrate the data training with 100,000 samples. In [20]: ## \"N\" and \"a\" are set_seed () TRAINING_SIZE = 100000 chars2 = '0123456789+-*/Na ' print ( \"The number of characters in dictionary: {}\" . format ( len ( chars2 ))) ctable2 = CharacterTable ( chars2 ) DIGITS = 3 MAXLEN_x = DIGITS + 1 + DIGITS ## Maximum length of the output would occur ## when 99999x99999=9,999,800,001 which has length 10 MAXLEN_y = int ( np . ceil ( np . log10 ( (( 10 ** DIGITS ) - 1 ) ** 2 ))) def frand (): st = '' for i in range ( DIGITS ): st += str ( np . random . choice ( list ( '0123456789 ' ))) if st == ' ' * DIGITS : st = frand () return st ## arithmetic operation foperation = lambda : np . random . choice ( list ( '+/*-' )) def get_answer ( a , b , oper ): clean = lambda a : int ( a . replace ( \" \" , \"\" )) a = clean ( a ) b = clean ( b ) if oper == \"+\" : out = a + b elif oper == \"-\" : out = a - b elif oper == \"*\" : out = a * b elif oper == \"/\" : if b == 0 : out = \"NaN\" else : out = a / b return ( out ) questions = [] expected = [] print ( 'Generating data...' ) while len ( questions ) < TRAINING_SIZE : if len ( questions ) % 10000 == 0 : print ( \"{} samples are generated...\" . format ( len ( questions ))) a , b , oper = frand (), frand (), foperation () # Pad the data with spaces such that it is always MAXLEN. q = '{}{}{}' . format ( a , oper , b ) query = q + ' ' * ( MAXLEN_x - len ( q )) ans = str ( get_answer ( a , b , oper )) # Answers can be of maximum size MAXLEN_y. ans += ' ' * ( MAXLEN_y - len ( ans )) questions . append ( query ) expected . append ( ans ) print ( 'Total addition questions:{}' . format ( len ( questions ))) The number of characters in dictionary: 17 Generating data... 0 samples are generated... 10000 samples are generated... 20000 samples are generated... 30000 samples are generated... 40000 samples are generated... 50000 samples are generated... 60000 samples are generated... 70000 samples are generated... 80000 samples are generated... 90000 samples are generated... Total addition questions:100000 Example data strings look like: In [15]: for q , e in zip ( questions , expected )[: 20 ]: print ( \"{} = {}\" . format ( q , e )) 226-13 = 213 961+019 = 980 093+400 = 493 173*247 = 42731 480+793 = 1273 461/562 = 0 83 *502 = 41666 62/446 = 0 3 0-647 = -617 671/ 57 = 11 248/121 = 2 359*081 = 29079 335*979 = 327965 333-869 = -536 639/666 = 0 34/310 = 0 868-910 = -42 134+761 = 895 337/686 = 0 447+009 = 456 One-hot encoding and split between training and testing. In [16]: x , y = one_hot_encoder ( expected , questions , MAXLEN_x , MAXLEN_y , chars2 , ctable2 ) ( x_train , x_val ),( y_train , y_val ) = split_train_test ( x , y ) Vectorization... Define Model This time, I consider LSTM with more nodes and more layers for encoders! Note we also consider the encoder structure with bottole neck encoder dimention to be 1. This model was converging prohibitably slow: e.g., at epoch 500, it was still about validation accuracy was still as low as 0.6. Due to the computational resource, we consider complex model that converges faster. In [17]: set_seed () model2 , encoder2 = define_model ( MAXLEN_x , MAXLEN_y , chars2 , HIDDEN_ENCODER = [ 256 ], HIDDEN_DECODER = [ 256 ], RNN = layers . LSTM ) model2 . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= Input (InputLayer) (None, 7, 17) 0 _________________________________________________________________ encoder1 (LSTM) (None, 256) 280576 _________________________________________________________________ repeat_vector (RepeatVector) (None, 6, 256) 0 _________________________________________________________________ decoder1 (LSTM) (None, 6, 256) 525312 _________________________________________________________________ time_distributed_dense (Time (None, 6, 17) 4369 ================================================================= Total params: 810,257 Trainable params: 810,257 Non-trainable params: 0 _________________________________________________________________ Training In [18]: set_seed () history2 = train ( model2 , ctable2 , x_train , y_train , x_val , y_val , nb_epochs = 100 , #2000 print_every = 10 , BATCH_SIZE = 64 ) -------------------------------------------------- Iteration 10 acc:0.799 loss:0.504 val_acc:0.801 val_loss:0.498 Q:405*934 T:378270 Model:359600 â˜’ Q:799+472 T:1271 Model:1271 â˜‘ Q:33 *952 T:31416 Model:30966 â˜’ Q:283+926 T:1209 Model:1211 â˜’ Q:232+59 T:291 Model:399 â˜’ Q: 88/154 T:0 Model:0 â˜‘ Q:412*280 T:115360 Model:110880 â˜’ Q:787+074 T:861 Model:861 â˜‘ Q:2 3+ 39 T:62 Model:50 â˜’ Q:1 7/397 T:0 Model:0 â˜‘ -------------------------------------------------- Iteration 20 acc:0.904 loss:0.256 val_acc:0.886 val_loss:0.296 Q:077/558 T:0 Model:0 â˜‘ Q:765* 60 T:45900 Model:46300 â˜’ Q:037+3 8 T:75 Model:75 â˜‘ Q: 54-721 T:-667 Model:-667 â˜‘ Q:308-666 T:-358 Model:-358 â˜‘ Q:976/619 T:1 Model:1 â˜‘ Q: 19/127 T:0 Model:0 â˜‘ Q:982+899 T:1881 Model:1881 â˜‘ Q:426/669 T:0 Model:0 â˜‘ Q:8 -5 2 T:-44 Model:-44 â˜‘ -------------------------------------------------- Iteration 30 acc:0.929 loss:0.197 val_acc:0.894 val_loss:0.293 Q:236*520 T:122720 Model:123720 â˜’ Q:525/402 T:1 Model:1 â˜‘ Q:012-869 T:-857 Model:-857 â˜‘ Q:115*615 T:70725 Model:66925 â˜’ Q:419-66 T:353 Model:353 â˜‘ Q:838+4 3 T:881 Model:881 â˜‘ Q:932*006 T:5592 Model:6632 â˜’ Q: 58*619 T:35902 Model:35742 â˜’ Q:144/353 T:0 Model:0 â˜‘ Q:898* 04 T:3592 Model:3992 â˜’ -------------------------------------------------- Iteration 40 acc:0.952 loss:0.139 val_acc:0.889 val_loss:0.359 Q:923*709 T:654407 Model:655407 â˜’ Q:709- 43 T:666 Model:666 â˜‘ Q:64 /269 T:0 Model:0 â˜‘ Q:172+618 T:790 Model:790 â˜‘ Q:234- 08 T:226 Model:226 â˜‘ Q:152-313 T:-161 Model:-161 â˜‘ Q:62 *513 T:31806 Model:32346 â˜’ Q:354-1 7 T:337 Model:337 â˜‘ Q:535+604 T:1139 Model:1139 â˜‘ Q:934*7 2 T:67248 Model:68768 â˜’ -------------------------------------------------- Iteration 50 acc:0.968 loss:0.099 val_acc:0.889 val_loss:0.424 Q:3 2/525 T:0 Model:0 â˜‘ Q:592/864 T:0 Model:0 â˜‘ Q:616*016 T:9856 Model:9034 â˜’ Q:53 -08 T:45 Model:55 â˜’ Q:92 +145 T:237 Model:237 â˜‘ Q:488*419 T:204472 Model:205772 â˜’ Q:020/312 T:0 Model:0 â˜‘ Q:151*607 T:91657 Model:96957 â˜’ Q:507-537 T:-30 Model:-30 â˜‘ Q:2 8*649 T:18172 Model:17812 â˜’ -------------------------------------------------- Iteration 60 acc:0.983 loss:0.058 val_acc:0.889 val_loss:0.505 Q:192*998 T:191616 Model:188778 â˜’ Q:290+24 T:314 Model:314 â˜‘ Q:540*034 T:18360 Model:18360 â˜‘ Q:368*910 T:334880 Model:330080 â˜’ Q: 07-417 T:-410 Model:-410 â˜‘ Q:0 1*642 T:642 Model:642 â˜‘ Q: 99/219 T:0 Model:0 â˜‘ Q:01 *872 T:872 Model:872 â˜‘ Q:8 0*394 T:31520 Model:31780 â˜’ Q:178/654 T:0 Model:0 â˜‘ -------------------------------------------------- Iteration 70 acc:0.989 loss:0.039 val_acc:0.883 val_loss:0.623 Q: 22*841 T:18502 Model:17422 â˜’ Q:298-465 T:-167 Model:-167 â˜‘ Q: 94-619 T:-525 Model:-525 â˜‘ Q:3 4+035 T:69 Model:69 â˜‘ Q:367*213 T:78171 Model:77231 â˜’ Q:6 +0 8 T:14 Model:14 â˜‘ Q:215+211 T:426 Model:426 â˜‘ Q:035+695 T:730 Model:730 â˜‘ Q:3 3*615 T:20295 Model:20765 â˜’ Q:845+416 T:1261 Model:1261 â˜‘ -------------------------------------------------- Iteration 80 acc:0.990 loss:0.037 val_acc:0.884 val_loss:0.670 Q:121+944 T:1065 Model:1065 â˜‘ Q: 08+933 T:941 Model:941 â˜‘ Q:368+626 T:994 Model:994 â˜‘ Q:751-895 T:-144 Model:-144 â˜‘ Q:913/495 T:1 Model:1 â˜‘ Q:305/381 T:0 Model:0 â˜‘ Q:511+ 7 T:518 Model:528 â˜’ Q: 19/187 T:0 Model:0 â˜‘ Q:293/ 11 T:26 Model:26 â˜‘ Q:040-889 T:-849 Model:-849 â˜‘ -------------------------------------------------- Iteration 90 acc:0.993 loss:0.027 val_acc:0.884 val_loss:0.710 Q:731/15 T:48 Model:48 â˜‘ Q:013*567 T:7371 Model:7471 â˜’ Q:008*013 T:104 Model:144 â˜’ Q:424*260 T:110240 Model:100460 â˜’ Q: 95*529 T:50255 Model:58135 â˜’ Q: 67*294 T:19698 Model:20058 â˜’ Q:33 *952 T:31416 Model:32756 â˜’ Q:142+201 T:343 Model:343 â˜‘ Q:806/559 T:1 Model:1 â˜‘ Q:648-055 T:593 Model:593 â˜‘ Plot validation and accuracy over epochs The validation loss starts increasing after 30 epochs indicating that the model overfits. On the other hand, the overfitting behavior cannot be seen from the validation accuracy (it remains the same (and not decreasing) after the 40 epochs.) This is probably because the accuracy is a more \"corse\" measure than the \"softmax-categorical_crossentropy\" which was used as our loss. I would increase training sample size or change the model structure to improve the model performance. In [19]: plot_loss_acc ( history2 ) Model performance The code block below shows the randomly selected 100 validation samples. It is clear that the model performance is reasonable in many problems but the performance becomes poor when the solution requires more than 4 digits. In [30]: set_seed ( 1 ) random_index = np . random . choice ( x_val . shape [ 0 ], 100 , replace = False ) print_predicted ( x_val [ random_index ], y_val [ random_index ], model2 , ctable2 , colors ) Q:478+629 T:1107 Model:1107 â˜‘ Q:383*249 T:95367 Model:944977 â˜’ Q:231-94 T:137 Model:137 â˜‘ Q:841/232 T:3 Model:3 â˜‘ Q:151+448 T:599 Model:597 â˜’ Q:539-661 T:-122 Model:-122 â˜‘ Q:9 5*474 T:45030 Model:44070 â˜’ Q:801+180 T:981 Model:981 â˜‘ Q:767/260 T:2 Model:2 â˜‘ Q:079/983 T:0 Model:0 â˜‘ Q:180+926 T:1106 Model:1106 â˜‘ Q:411+515 T:926 Model:926 â˜‘ Q: 78/983 T:0 Model:0 â˜‘ Q:067+029 T:96 Model:96 â˜‘ Q:595+ 76 T:671 Model:671 â˜‘ Q:182/691 T:0 Model:0 â˜‘ Q: 5+800 T:805 Model:805 â˜‘ Q:238-617 T:-379 Model:-379 â˜‘ Q: 05- 64 T:-59 Model:-59 â˜‘ Q:734-59 T:675 Model:675 â˜‘ Q:53 +037 T:90 Model:80 â˜’ Q:326-157 T:169 Model:169 â˜‘ Q:681+695 T:1376 Model:1376 â˜‘ Q:1 0/6 9 T:0 Model:0 â˜‘ Q:825/518 T:1 Model:1 â˜‘ Q:3 4/809 T:0 Model:0 â˜‘ Q:097+312 T:409 Model:409 â˜‘ Q:435*892 T:388020 Model:388180 â˜’ Q:699+420 T:1119 Model:1109 â˜’ Q:2 8/985 T:0 Model:0 â˜‘ Q:574+725 T:1299 Model:1299 â˜‘ Q:35 -581 T:-546 Model:-546 â˜‘ Q: 98/171 T:0 Model:0 â˜‘ Q:679-237 T:442 Model:442 â˜‘ Q:088+401 T:489 Model:489 â˜‘ Q:390/007 T:55 Model:52 â˜’ Q:577/091 T:6 Model:6 â˜‘ Q:887*909 T:806283 Model:811773 â˜’ Q:225+324 T:549 Model:549 â˜‘ Q:165/0 0 T:NaN Model:NaN â˜‘ Q:551- 1 T:550 Model:550 â˜‘ Q:15 -747 T:-732 Model:-732 â˜‘ Q: 63+520 T:583 Model:583 â˜‘ Q:251+158 T:409 Model:309 â˜’ Q:698-518 T:180 Model:180 â˜‘ Q:3 1+980 T:1011 Model:1011 â˜‘ Q:970/902 T:1 Model:1 â˜‘ Q:183*53 T:9699 Model:1669 â˜’ Q:294-213 T:81 Model:71 â˜’ Q:707+400 T:1107 Model:1107 â˜‘ Q:402+846 T:1248 Model:1248 â˜‘ Q:533/971 T:0 Model:0 â˜‘ Q:2 7+549 T:576 Model:576 â˜‘ Q:654-805 T:-151 Model:-151 â˜‘ Q:974/609 T:1 Model:1 â˜‘ Q:783+540 T:1323 Model:1323 â˜‘ Q:01 *835 T:835 Model:835 â˜‘ Q:068*650 T:44200 Model:44400 â˜’ Q:071-566 T:-495 Model:-495 â˜‘ Q:390/821 T:0 Model:0 â˜‘ Q:794+954 T:1748 Model:1748 â˜‘ Q: 17-323 T:-306 Model:-306 â˜‘ Q:386-042 T:344 Model:344 â˜‘ Q:797*354 T:282138 Model:270618 â˜’ Q:1 /44 T:0 Model:0 â˜‘ Q:531+299 T:830 Model:820 â˜’ Q:357*6 8 T:24276 Model:22716 â˜’ Q:862*608 T:524096 Model:528016 â˜’ Q:7 5*36 T:2700 Model:2600 â˜’ Q:326/ 13 T:25 Model:20 â˜’ Q:538+214 T:752 Model:752 â˜‘ Q:713/ 54 T:13 Model:13 â˜‘ Q:126+013 T:139 Model:139 â˜‘ Q:906* 2 T:1812 Model:1972 â˜’ Q:39 /0 7 T:5 Model:5 â˜‘ Q:664*5 7 T:37848 Model:37348 â˜’ Q:790+914 T:1704 Model:1704 â˜‘ Q:909*1 6 T:14544 Model:14564 â˜’ Q:49 *529 T:25921 Model:25981 â˜’ Q:299+308 T:607 Model:607 â˜‘ Q:040+503 T:543 Model:543 â˜‘ Q:47 -088 T:-41 Model:-41 â˜‘ Q:525/3 1 T:16 Model:17 â˜’ Q:479+981 T:1460 Model:1460 â˜‘ Q:731-358 T:373 Model:373 â˜‘ Q:96 *856 T:82176 Model:81536 â˜’ Q: 65-541 T:-476 Model:-476 â˜‘ Q:806+323 T:1129 Model:1129 â˜‘ Q:099* 73 T:7227 Model:7287 â˜’ Q:481-06 T:475 Model:475 â˜‘ Q: 89/6 4 T:1 Model:1 â˜‘ Q:080/919 T:0 Model:0 â˜‘ Q:917-741 T:176 Model:176 â˜‘ Q:449-565 T:-116 Model:-116 â˜‘ Q:851-67 T:784 Model:794 â˜’ Q:427/083 T:5 Model:5 â˜‘ Q:76 *00 T:0 Model:0 â˜‘ Q:101+30 T:131 Model:131 â˜‘ Q:961*179 T:172019 Model:168879 â˜’ Q:6 2+ 5 T:67 Model:67 â˜‘ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Create deep learning calculators based on Encoder-Decoder RNN using Keras"},{"url":"Assess-the-robustness-of-CapsNet.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In the Understanding and Experimenting Capsule Networks , I experimented Hinton's Capsule Network . Dynamic Routing Between Capsules discusses the robustness of the Capsule Networks to affine transformations: \"Experiments show that each DigitCaps capsule learns a more robust representation for each class than a traditional convolutional network. Because there is natural variance in skew, rotation, style, etc in hand written digits, the trained CapsNet is moderately robust to small affine transformations of the training data (Section 5.2, page 6).\" The authors used affNIST for the robustness valdiation. In this blog, I evaluate the robustness of this network. I will create affine transformed MNIST test data by myself and use my version of CapsNet and my version of the standard CNN model introduced in prvious blog posts for predicting the correct class of digits in this new data. my version of CapsNet and my version of the standard CNN model respectively yield validation losses of 0.42% and 0.50% on MNIST with no affine transformation. As the models are trained only on 2-pixel shifted MNIST, the performance of models would deteriorate. Nevertheless, I expect that robust model would have lower performance deterioration. Reference Dynamic Routing Between Capsules Understanding and Experimenting Capsule Networks In [1]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import sys print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )) print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"2\" #### 2 GPU1 #### 0 GPU3 #### 4 GPU4 #### 3 GPU2 set_session ( tf . Session ( config = config )) Using TensorFlow backend. python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.0.6 tensorflow version 1.2.1 Define class objects for CapsNet. The codes here are created by modifing Kevin Mader's ipython notebook script in Kaggle competition , which, in turn are written by adapting Xifeng Guo's script in Github . In [2]: import keras.backend as K from keras import initializers , layers from keras import models class Length ( layers . Layer ): \"\"\" Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss inputs: shape=[dim_1, ..., dim_{n-1}, dim_n] output: shape=[dim_1, ..., dim_{n-1}] \"\"\" def call ( self , inputs , ** kwargs ): return K . sqrt ( K . sum ( K . square ( inputs ), - 1 )) def compute_output_shape ( self , input_shape ): return input_shape [: - 1 ] class Mask ( layers . Layer ): \"\"\" Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1. Output shape: [None, d2] This class is used to reduce the dimention of the (Nsample, n_class, dim_vector) --> (Nsample, dim_vector) For training: only keep the activity vector (v in paper) of true class for testing only keep the activity vector with the largest norm (length in vector) \"\"\" def call ( self , inputs , ** kwargs ): # use true label to select target capsule, shape=[batch_size, num_capsule] if type ( inputs ) is list : # true label is provided with shape = [batch_size, n_classes], i.e. one-hot code. assert len ( inputs ) == 2 inputs , mask = inputs else : # if no true label, mask by the max length of vectors of capsules x = inputs # Enlarge the range of values in x to make max(new_x)=1 and others < 0 x = ( x - K . max ( x , 1 , True )) / K . epsilon () + 1 mask = K . clip ( x , 0 , 1 ) # the max value in x clipped to 1 and other to 0 # masked inputs, shape = [batch_size, dim_vector] inputs_masked = K . batch_dot ( inputs , mask , [ 1 , 1 ]) return inputs_masked def compute_output_shape ( self , input_shape ): if type ( input_shape [ 0 ]) is tuple : # true label provided return tuple ([ None , input_shape [ 0 ][ - 1 ]]) else : return tuple ([ None , input_shape [ - 1 ]]) def squash ( vectors , axis =- 1 ): \"\"\" The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0 :param vectors: some vectors to be squashed, N-dim tensor :param axis: the axis to squash :return: a Tensor with same shape as input vectors \"\"\" s_squared_norm = K . sum ( K . square ( vectors ), axis , keepdims = True ) scale = s_squared_norm / ( 1 + s_squared_norm ) / K . sqrt ( s_squared_norm ) return scale * vectors class CapsuleLayer ( layers . Layer ): \"\"\" The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\ [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1. :param num_capsule: number of capsules in this layer :param dim_vector: dimension of the output vectors of the capsules in this layer :param num_routings: number of iterations for the routing algorithm \"\"\" def __init__ ( self , num_capsule , dim_vector , num_routing = 3 , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , ** kwargs ): super ( CapsuleLayer , self ) . __init__ ( ** kwargs ) self . num_capsule = num_capsule self . dim_vector = dim_vector self . num_routing = num_routing self . kernel_initializer = initializers . get ( kernel_initializer ) self . bias_initializer = initializers . get ( bias_initializer ) def build ( self , input_shape ): assert len ( input_shape ) >= 3 , \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\" self . input_num_capsule = input_shape [ 1 ] self . input_dim_vector = input_shape [ 2 ] # Transform matrix self . W = self . add_weight ( shape = [ self . input_num_capsule , self . num_capsule , self . input_dim_vector , self . dim_vector ], initializer = self . kernel_initializer , name = 'W' ) # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation. self . bias = self . add_weight ( shape = [ 1 , self . input_num_capsule , self . num_capsule , 1 , 1 ], initializer = self . bias_initializer , name = 'bias' , trainable = False ) self . built = True def call ( self , inputs , training = None ): # inputs.shape=[None, input_num_capsule, input_dim_vector] # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector] inputs_expand = K . expand_dims ( K . expand_dims ( inputs , 2 ), 2 ) # Replicate num_capsule dimension to prepare being multiplied by W # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector] inputs_tiled = K . tile ( inputs_expand , [ 1 , 1 , self . num_capsule , 1 , 1 ]) \"\"\" # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size. # Now W has shape = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector] w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1]) # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector] inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3]) \"\"\" # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow. # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector] inputs_hat = tf . scan ( lambda ac , x : K . batch_dot ( x , self . W , [ 3 , 2 ]), elems = inputs_tiled , initializer = K . zeros ([ self . input_num_capsule , self . num_capsule , 1 , self . dim_vector ])) \"\"\" # Routing algorithm V1. Use tf.while_loop in a dynamic way. def body(i, b, outputs): c = tf.nn.softmax(self.bias, dim=2) # dim=2 is the num_capsule dimension outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True)) b = b + K.sum(inputs_hat * outputs, -1, keepdims=True) return [i-1, b, outputs] cond = lambda i, b, inputs_hat: i > 0 loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)] _, _, outputs = tf.while_loop(cond, body, loop_vars) \"\"\" # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance assert self . num_routing > 0 , 'The num_routing should be > 0.' for i in range ( self . num_routing ): c = tf . nn . softmax ( self . bias , dim = 2 ) # dim=2 is the num_capsule dimension # outputs.shape=[None, 1, num_capsule, 1, dim_vector] outputs = squash ( K . sum ( c * inputs_hat , 1 , keepdims = True )) # last iteration needs not compute bias which will not be passed to the graph any more anyway. if i != self . num_routing - 1 : # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True)) self . bias += K . sum ( inputs_hat * outputs , - 1 , keepdims = True ) # tf.summary.histogram('BigBee', self.bias) # for debugging return K . reshape ( outputs , [ - 1 , self . num_capsule , self . dim_vector ]) def compute_output_shape ( self , input_shape ): return tuple ([ None , self . num_capsule , self . dim_vector ]) def PrimaryCap ( inputs , dim_vector , n_channels , kernel_size , strides , padding ): \"\"\" Apply Conv2D `n_channels` times and concatenate all capsules :param inputs: 4D tensor, shape=[None, width, height, channels] :param dim_vector: the dim of the output vector of capsule :param n_channels: the number of types of capsules :return: output tensor, shape=[None, num_capsule, dim_vector] \"\"\" output = layers . Conv2D ( filters = dim_vector * n_channels , ## 8 x 32 kernel_size = kernel_size , ## 9x9 strides = strides , ## 2 padding = padding , name = \"PrimaryCap_conv2d\" )( inputs ) outputs = layers . Reshape ( target_shape = [ - 1 , dim_vector ], name = \"PrimaryCap_reshape\" )( output ) return layers . Lambda ( squash , name = \"PrimaryCap_squash\" )( outputs ) def NetworkInputToDigitCap ( input_shape , n_class , num_routing ): x = layers . Input ( shape = input_shape ) # Layer 1: Just a conventional Conv2D layer conv1 = layers . Conv2D ( filters = 256 , kernel_size = 9 , strides = 1 , padding = 'valid' , activation = 'relu' , name = 'Conv1' )( x ) # Layer 2: Conv2D layer with `squash` activation, # then reshape to [None, num_capsule, dim_vector] primarycaps = PrimaryCap ( conv1 , dim_vector = 8 , n_channels = 32 , kernel_size = 9 , strides = 2 , padding = 'valid' ) # Layer 3: Capsule layer. Routing algorithm works here. digitcaps = CapsuleLayer ( num_capsule = n_class , dim_vector = 16 , num_routing = num_routing , name = 'DigitCaps' )( primarycaps ) return ( x , digitcaps ) CapsNet functions In [3]: from keras import backend as K def CapsNet ( input_shape , n_class , num_routing ): \"\"\" A Capsule Network on MNIST. :param input_shape: data shape, 4d, [None, width, height, channels] :param n_class: number of classes :param num_routing: number of routing iterations :return: A Keras Model with 2 inputs and 2 outputs \"\"\" x , digitcaps = NetworkInputToDigitCap ( input_shape , n_class , num_routing ) # Layer 4: This is an auxiliary layer to replace each capsule with its length. # Just to match the true label's shape. # If using tensorflow, this will not be necessary. :) out_caps = Length ( name = 'out_caps' )( digitcaps ) ##||v|| margin_loss # Decoder network. y = layers . Input ( shape = ( n_class ,), name = \"true_label\" ) masked = Mask ( name = \"activity_vec\" )([ digitcaps , ## digitcaps.shape = (Nsample, n_class, dim_vector ) y ]) # The true label is used to mask the output of capsule layer. x_recon = Decoder ( masked ) # two-input-two-output keras Model m1 = models . Model ([ x , y ], [ out_caps , x_recon ]) m2 = models . Model ([ x ],[ digitcaps ]) ## YUMI added this line return m1 , m2 def Decoder ( masked ): ## Yumi refactored x_recon = layers . Dense ( 512 , activation = 'relu' , name = \"FC1\" )( masked ) x_recon = layers . Dense ( 1024 , activation = 'relu' , name = \"FC2\" )( x_recon ) x_recon = layers . Dense ( 784 , activation = 'sigmoid' , name = \"FC3\" )( x_recon ) ## mse to ensure that the reconstructed images are close to the original image. x_recon = layers . Reshape ( target_shape = [ 28 , 28 , 1 ], name = 'out_recon' )( x_recon ) return ( x_recon ) Build a CapsNet model and load the weights In [4]: import keras.backend as K from keras import initializers , layers from keras import models CapsNet , model_wo_decoder = CapsNet ( input_shape = [ 28 , 28 , 1 ], n_class = 10 , num_routing = 3 ) CapsNet . load_weights ( \"CapsNet_weights.h5\" ) Build a standard CNN model and load the weights In [5]: from keras.models import model_from_json def load_model ( name ): model = model_from_json ( open ( name + '_architecture.json' ) . read ()) model . load_weights ( name + '_weights.h5' ) return ( model ) standard = load_model ( \"standard\" ) Use Keras's ImageDataGenerator to modify the original data, and create transforemd data. In [6]: from keras.preprocessing.image import ImageDataGenerator from numpy.random import seed from tensorflow import set_random_seed def generate_once ( x , y , shift_fraction = 0.1 , shear_range = 0.2 , zoom_range = 0.2 , rotation_range = 40 ): ''' modify each image in x to generate new data return x, y of the same shape seed is set for reproducable results ''' seed ( 1 ) set_random_seed ( 1 ) batch_size = x . shape [ 0 ] train_datagen = ImageDataGenerator ( width_shift_range = shift_fraction , height_shift_range = shift_fraction , shear_range = shear_range , rotation_range = rotation_range , zoom_range = zoom_range ) # shift up to 2 pixel for MNIST generator = train_datagen . flow ( x , y , batch_size = batch_size ) x , y = generator . next () return ([ x , y ]) def plot_example_images ( x_tra , title , n = 10 ): ''' plot first n images in x_tra ''' const = 0.9 fig = plt . figure ( figsize = ( n * const , 1 * const )) fig . subplots_adjust ( hspace = 0.01 , wspace = 0.01 , left = 0 , right = 1 , bottom = 0 , top = 1 ) for i in range ( x_tra . shape [ 0 ]): ax = fig . add_subplot ( 1 , n , i + 1 , xticks = [], yticks = []) ax . imshow ( x_tra [ i ] . reshape ( 28 , 28 )) if n < ( i + 2 ): break plt . suptitle ( \"shift_fraction:{}, shear_range:{}, zoom_range:{}, {}\" . format ( shift_fraction , shear_range , zoom_range , title ), y = 1.5 ) plt . show () def get_error ( y_tra , y_pr ): ''' Return error y_tr : np.array() of shape (Nsample, Ncat), containing 0 or 1 y_pr : np.array() of shape (Nsample, Ncat), containing probabilities ''' pred_class = y_pr . argmax ( axis = 1 ) acc = np . mean ( y_tra [ range ( len ( pred_class )), pred_class ] == 1 ) * 100 return ( 100 - acc ) Load MNIST test data Here, I only load test data because I will use the test for evaluation only. In [7]: from mnist import MNIST import numpy as np from keras.utils import to_categorical mndata = MNIST ( './MNIST_data/' ) images , labels = mndata . load_testing () x_test , y_test = np . array ( images ), np . array ( labels ) x_test = x_test . reshape ( - 1 , 28 , 28 , 1 ) . astype ( 'float32' ) / 255. y_test = to_categorical ( y_test . astype ( 'float32' )) print ( x_test . shape , y_test . shape ) ((10000, 28, 28, 1), (10000, 10)) The validation error reported in my previous blog article about CapsNet and my previous blog article about standard CNN model blog are: - CapsNet: 0.42% - Standard: 0.54% We reproduce these results first. In [8]: y_pred_Cap , _ = CapsNet . predict ([ x_test , y_test ]) y_pred_sta = standard . predict ( x_test ) acc_Cap = get_error ( y_test , y_pred_Cap ) acc_sta = get_error ( y_test , y_pred_sta ) print ( \"Error CapsNet: {:5.3}%, Standard: {:5.3}%\" . format ( acc_Cap , acc_sta )) Error CapsNet: 0.42%, Standard: 0.54% Model performance assessments with affine transfomation. I consider four types of affine transfomation: shifting shearing zooming rotation For rotation, I always consider rotating images for at most 40 degrees. For other types of affine transfomation, I consider 3 different degree, and created 27 different generated data. For each generated data, I will classify the digits using the two models. Let's see which model is more robust to the changes. In [9]: shift_fractions = [ 0.1 , 0.15 , 0.2 ] shear_ranges = [ 0.1 , 0.15 , 0.2 ] zoom_ranges = [ 0.05 , 0.1 , 0.15 ] N = len ( shift_fractions ) result_Cap = np . zeros (( N , N , N )) result_sta = np . zeros (( N , N , N )) for ishift , shift_fraction in enumerate ( shift_fractions ): for ishear , shear_range in enumerate ( shear_ranges ): for izoom , zoom_range in enumerate ( zoom_ranges ): x_tra , y_tra = generate_once ( x_test , y_test , shift_fraction , shear_range , zoom_range ) ### Prediction by CapsNet and the standard CNN y_pred_Cap , _ = CapsNet . predict ([ x_tra , y_tra ]) y_pred_sta = standard . predict ( x_tra ) ### Prediction error acc_Cap = get_error ( y_tra , y_pred_Cap ) acc_sta = get_error ( y_tra , y_pred_sta ) result_Cap [ ishift , ishear , izoom ] = acc_Cap result_sta [ ishift , ishear , izoom ] = acc_sta title = \"Error CapsNet: {:5.3}%, Standard: {:5.3}%\" . format ( acc_Cap , acc_sta ) plot_example_images ( x_tra , title ) Evaluation results for the two model's prediction performance on various augmented data. Both CapsNet and standard CNN deteriorates the performance when more severe transfomation is performed. CapsNet does not always yield the smaller error than the standard CNN for considered set of transformed data. When zooming proportion is low (<=0.1), CapsNet always performs better than the standard CNN no matter what other affine transfomation is applyed to the data. In [10]: const = 3 N = result_Cap . shape [ 0 ] fig = plt . figure ( figsize = ( N * const , N * const )) fig . subplots_adjust ( hspace = 0.1 , wspace = 0.1 , left = 0 , right = 1 , bottom = 0 , top = 1 ) count = 1 for ishift in range ( N ): for ishear in range ( N ): if ( ishear == 0 ) and ( ishift == N - 1 ): ax = fig . add_subplot ( N , N , count ) elif ishear == 0 : ax = fig . add_subplot ( N , N , count , xticks = [] ) elif ishift == N - 1 : ax = fig . add_subplot ( N , N , count , yticks = []) else : ax = fig . add_subplot ( N , N , count , xticks = [], yticks = []) count += 1 cvec = result_Cap [ ishift , ishear ,:] svec = result_sta [ ishift , ishear ,:] ax . plot ( zoom_ranges , cvec , label = \"CapsNet\" ) ax . plot ( zoom_ranges , svec , label = \"standard\" ) vec = list ( result_Cap [ ishift ,:,:]) + list ( result_sta [ ishift ,:,:]) ax . set_ylim ( np . min ( vec ), np . max ( vec )) ax . set_title ( \"shift%={}, shear={}\" . format ( shift_fractions [ ishift ], shear_ranges [ ishear ])) ax . legend () ax = fig . add_subplot ( 1 , 1 , 1 , frameon = False , xticks = [], yticks = []) ax . set_xlabel ( \"zoom ranges\" , labelpad = 30 ) ax . set_ylabel ( \"error (%)\" , labelpad = 50 ) plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Assess the robustness of CapsNet"},{"url":"Understanding-and-Experimenting-Capsule-Networks.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } This blog is inspired by Dynamic Routing Between Capsules and aims to understand Capsule Networks with hands-on coding. I use Keras with tensorflow backend. The codes here are created by modifing Kevin Mader's ipython notebook script in Kaggle competition , which, in turn are written by adapting Xifeng Guo's script in Github . CapsNet is cool in many ways but I was particularly impressed with the following claims that the authors made: CapsNet requires less parameters to achieve lower validation error (0.25%) the state of art (0.39%). This CapsNet model \"only\" has 8M parameters while the baseline (current state of art) requires 35M parameters. CapsNet is interpretable. CapsNet is more robust than state of art. CapsNet needs less images for training. In this blog, I examine the points 1 and 2, and would like to discuss the points 3 and 4 in Assess the robustness of CapsNet . Findings: I was able to use Capsule Network to return validation error of 0.42%, which is less than my baseline (0.54%). Intepretation of the capsules is possible but it requires some efforts. Reference: Kaggle competition Siraj Raval's YouTube video Jonathan Hui's blog <- This blog helped me a lot to understand the techinical details. So thank you very much Jonathan! Max Pechyonkin's blog My previous blog Importing necessary modules In [1]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import sys print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )) print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"0\" #### 2 GPU1 #### 0 GPU3 #### 4 GPU4 #### 3 GPU2 set_session ( tf . Session ( config = config )) Using TensorFlow backend. python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.0.6 tensorflow version 1.2.1 Core functions for DigitCaps and PrimaryCaps In [2]: import keras.backend as K from keras import initializers , layers from keras import models class Length ( layers . Layer ): \"\"\" Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss inputs: shape=[dim_1, ..., dim_{n-1}, dim_n] output: shape=[dim_1, ..., dim_{n-1}] \"\"\" def call ( self , inputs , ** kwargs ): return K . sqrt ( K . sum ( K . square ( inputs ), - 1 )) def compute_output_shape ( self , input_shape ): return input_shape [: - 1 ] class Mask ( layers . Layer ): \"\"\" Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1. Output shape: [None, d2] This class is used to reduce the dimention of the (Nsample, n_class, dim_vector) --> (Nsample, dim_vector) For training: only keep the activity vector (v in paper) of true class for testing only keep the activity vector with the largest norm (length in vector) \"\"\" def call ( self , inputs , ** kwargs ): # use true label to select target capsule, shape=[batch_size, num_capsule] if type ( inputs ) is list : # true label is provided with shape = [batch_size, n_classes], i.e. one-hot code. assert len ( inputs ) == 2 inputs , mask = inputs else : # if no true label, mask by the max length of vectors of capsules x = inputs # Enlarge the range of values in x to make max(new_x)=1 and others < 0 x = ( x - K . max ( x , 1 , True )) / K . epsilon () + 1 mask = K . clip ( x , 0 , 1 ) # the max value in x clipped to 1 and other to 0 # masked inputs, shape = [batch_size, dim_vector] inputs_masked = K . batch_dot ( inputs , mask , [ 1 , 1 ]) return inputs_masked def compute_output_shape ( self , input_shape ): if type ( input_shape [ 0 ]) is tuple : # true label provided return tuple ([ None , input_shape [ 0 ][ - 1 ]]) else : return tuple ([ None , input_shape [ - 1 ]]) def squash ( vectors , axis =- 1 ): \"\"\" The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0 :param vectors: some vectors to be squashed, N-dim tensor :param axis: the axis to squash :return: a Tensor with same shape as input vectors \"\"\" s_squared_norm = K . sum ( K . square ( vectors ), axis , keepdims = True ) scale = s_squared_norm / ( 1 + s_squared_norm ) / K . sqrt ( s_squared_norm ) return scale * vectors class CapsuleLayer ( layers . Layer ): \"\"\" The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\ [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1. :param num_capsule: number of capsules in this layer :param dim_vector: dimension of the output vectors of the capsules in this layer :param num_routings: number of iterations for the routing algorithm \"\"\" def __init__ ( self , num_capsule , dim_vector , num_routing = 3 , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , ** kwargs ): super ( CapsuleLayer , self ) . __init__ ( ** kwargs ) self . num_capsule = num_capsule self . dim_vector = dim_vector self . num_routing = num_routing self . kernel_initializer = initializers . get ( kernel_initializer ) self . bias_initializer = initializers . get ( bias_initializer ) def build ( self , input_shape ): assert len ( input_shape ) >= 3 , \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\" self . input_num_capsule = input_shape [ 1 ] self . input_dim_vector = input_shape [ 2 ] # Transform matrix self . W = self . add_weight ( shape = [ self . input_num_capsule , self . num_capsule , self . input_dim_vector , self . dim_vector ], initializer = self . kernel_initializer , name = 'W' ) # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation. self . bias = self . add_weight ( shape = [ 1 , self . input_num_capsule , self . num_capsule , 1 , 1 ], initializer = self . bias_initializer , name = 'bias' , trainable = False ) self . built = True def call ( self , inputs , training = None ): # inputs.shape=[None, input_num_capsule, input_dim_vector] # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector] inputs_expand = K . expand_dims ( K . expand_dims ( inputs , 2 ), 2 ) # Replicate num_capsule dimension to prepare being multiplied by W # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector] inputs_tiled = K . tile ( inputs_expand , [ 1 , 1 , self . num_capsule , 1 , 1 ]) \"\"\" # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size. # Now W has shape = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector] w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1]) # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector] inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3]) \"\"\" # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow. # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector] inputs_hat = tf . scan ( lambda ac , x : K . batch_dot ( x , self . W , [ 3 , 2 ]), elems = inputs_tiled , initializer = K . zeros ([ self . input_num_capsule , self . num_capsule , 1 , self . dim_vector ])) \"\"\" # Routing algorithm V1. Use tf.while_loop in a dynamic way. def body(i, b, outputs): c = tf.nn.softmax(self.bias, dim=2) # dim=2 is the num_capsule dimension outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True)) b = b + K.sum(inputs_hat * outputs, -1, keepdims=True) return [i-1, b, outputs] cond = lambda i, b, inputs_hat: i > 0 loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)] _, _, outputs = tf.while_loop(cond, body, loop_vars) \"\"\" # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance assert self . num_routing > 0 , 'The num_routing should be > 0.' for i in range ( self . num_routing ): c = tf . nn . softmax ( self . bias , dim = 2 ) # dim=2 is the num_capsule dimension # outputs.shape=[None, 1, num_capsule, 1, dim_vector] outputs = squash ( K . sum ( c * inputs_hat , 1 , keepdims = True )) # last iteration needs not compute bias which will not be passed to the graph any more anyway. if i != self . num_routing - 1 : # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True)) self . bias += K . sum ( inputs_hat * outputs , - 1 , keepdims = True ) # tf.summary.histogram('BigBee', self.bias) # for debugging return K . reshape ( outputs , [ - 1 , self . num_capsule , self . dim_vector ]) def compute_output_shape ( self , input_shape ): return tuple ([ None , self . num_capsule , self . dim_vector ]) def PrimaryCap ( inputs , dim_vector , n_channels , kernel_size , strides , padding ): \"\"\" Apply Conv2D `n_channels` times and concatenate all capsules :param inputs: 4D tensor, shape=[None, width, height, channels] :param dim_vector: the dim of the output vector of capsule :param n_channels: the number of types of capsules :return: output tensor, shape=[None, num_capsule, dim_vector] \"\"\" output = layers . Conv2D ( filters = dim_vector * n_channels , ## 8 x 32 kernel_size = kernel_size , ## 9x9 strides = strides , ## 2 padding = padding , name = \"PrimaryCap_conv2d\" )( inputs ) outputs = layers . Reshape ( target_shape = [ - 1 , dim_vector ], name = \"PrimaryCap_reshape\" )( output ) return layers . Lambda ( squash , name = \"PrimaryCap_squash\" )( outputs ) def NetworkInputToDigitCap ( input_shape , n_class , num_routing ): x = layers . Input ( shape = input_shape ) # Layer 1: Just a conventional Conv2D layer conv1 = layers . Conv2D ( filters = 256 , kernel_size = 9 , strides = 1 , padding = 'valid' , activation = 'relu' , name = 'Conv1' )( x ) # Layer 2: Conv2D layer with `squash` activation, # then reshape to [None, num_capsule, dim_vector] primarycaps = PrimaryCap ( conv1 , dim_vector = 8 , n_channels = 32 , kernel_size = 9 , strides = 2 , padding = 'valid' ) # Layer 3: Capsule layer. Routing algorithm works here. digitcaps = CapsuleLayer ( num_capsule = n_class , dim_vector = 16 , num_routing = num_routing , name = 'DigitCaps' )( primarycaps ) return ( x , digitcaps ) CapsNet function In [3]: from keras import backend as K def CapsNet ( input_shape , n_class , num_routing ): \"\"\" A Capsule Network on MNIST. :param input_shape: data shape, 4d, [None, width, height, channels] :param n_class: number of classes :param num_routing: number of routing iterations :return: A Keras Model with 2 inputs and 2 outputs \"\"\" x , digitcaps = NetworkInputToDigitCap ( input_shape , n_class , num_routing ) # Layer 4: This is an auxiliary layer to replace each capsule with its length. # Just to match the true label's shape. # If using tensorflow, this will not be necessary. :) out_caps = Length ( name = 'out_caps' )( digitcaps ) ##||v|| margin_loss # Decoder network. y = layers . Input ( shape = ( n_class ,), name = \"true_label\" ) masked = Mask ( name = \"activity_vec\" )([ digitcaps , ## digitcaps.shape = (Nsample, n_class, dim_vector ) y ]) # The true label is used to mask the output of capsule layer. x_recon = Decoder ( masked ) # two-input-two-output keras Model m1 = models . Model ([ x , y ], [ out_caps , x_recon ]) m2 = models . Model ([ x ],[ digitcaps ]) ## YUMI added this line return m1 , m2 def Decoder ( masked ): ## Yumi refactored x_recon = layers . Dense ( 512 , activation = 'relu' , name = \"FC1\" )( masked ) x_recon = layers . Dense ( 1024 , activation = 'relu' , name = \"FC2\" )( x_recon ) x_recon = layers . Dense ( 784 , activation = 'sigmoid' , name = \"FC3\" )( x_recon ) ## mse to ensure that the reconstructed images are close to the original image. x_recon = layers . Reshape ( target_shape = [ 28 , 28 , 1 ], name = 'out_recon' )( x_recon ) return ( x_recon ) def margin_loss ( y_true , y_pred ): \"\"\" Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it. :param y_true: [None, n_classes] :param y_pred: [None, num_capsule] :return: a scalar loss value. \"\"\" L = y_true * K . square ( K . maximum ( 0. , 0.9 - y_pred )) + \\ 0.5 * ( 1 - y_true ) * K . square ( K . maximum ( 0. , y_pred - 0.1 )) return K . mean ( K . sum ( L , 1 )) CapsNet outputs: model: Keras's model object for entire CapsNet model_wo_decoder: Keras's model object without encoder parts CapsNet has decoder subnetwork which reconstruct the image from the 16-dimensional activity vector. The model without the decoder subnetwork only contains the single ouputs which are the output vectors of capsule. I create a model without the decoder subnetwork so that I can easily access the output vectors of capsules. These two model objects share weights. I will train using \"model\" this will automatically update the digitcaps's weights. CapsNet architecture: The summary shows CapsNet architecture. It is always good to check the understanding of model by checking the number of parameters produced by Keras model and your own calculation! input_1 : 28 x 28 frame image Conv1 : 256, 9 x 9 convolution kernels with a stride of 1 and Relu activation Param N : 20,992 = 256 x (1 x (9 x 9) + 1), +1 for bias The 1st and 2nd dim of output is 20 = (28 - 9 + 1)/1 PrimaryCamspules layer : convolusional capsule layer with 32 channels of convolutional 8D capsules (each primary capsule contains 8 convolutional units with a 9 x 9 kernel and a stride of 2). Param N : 5,308,672 = (32 x 8) x (256 x (9 x 9 ) + 1) The input of Primary Camspules is 256 20 x 20 feature maps. For each feature map, 9 x 9 kernel with stride 2 and no padding is applied 32 x 8 times. Each input feature map is reduced to 6 x 6 feature maps (6 = floor[ (20 - 9)/2 ] + 1) and there are 32 x 8 of such feature maps. Therefore, the output dimention is 6 x 6 x (32 x 8). Each group of 8 feature maps is considered as a capsule. This layer returns: $\\boldsymbol{u}_i \\in R&#94;8, i=1,..., 6 * 6 * 32$ DigiCaps layer : Param N : 1,486,080 = 16 x 8 x (6 x 6 x 32) x 10 + 6 x 6 x 32 x 10 This is the core layer for Capsule Network. The index $j$ represents the index for output class $j=1,...,10$ and let $c_{i,j} \\in R&#94;1,\\boldsymbol{W}_{i,j} \\in R&#94;{16 x 8} $ and $\\boldsymbol{v}_j \\in R&#94;{16}$. $$ \\widehat{\\boldsymbol{u}}_{j|i} = \\boldsymbol{W}_{i,j} \\boldsymbol{u}_i\\\\ \\boldsymbol{s}_j = \\sum_{i=1}&#94;{6 x 6 x32} c_{i,j} \\widehat{\\boldsymbol{u}}_{j|i}\\\\ \\boldsymbol{v}_j = \\frac{ ||\\boldsymbol{s}_j||&#94;2 }{ 1 + ||\\boldsymbol{s}_j||&#94;2 } \\frac{ \\boldsymbol{s}_j }{ ||\\boldsymbol{s}_j|| } $$ DigiCaps layer contains 2 types of unknown weights: $c_{i,j}$ and $\\boldsymbol{W}_{i,j}$. As $c_{i,j} \\in R&#94;1$, we have 1 x (6 x 6 x 32) x 10 parameters. As $\\boldsymbol{W}_{i,j} \\in R&#94;{16 x 8} $, we have 1,474,560 = (16 x 8 x (6 x 6 x 32) x 10) parameters for the affine transfomation matrix The output is 10 x 16. In [4]: def myprint (): print ( \"~~~\" * 10 ) ## define model n_class = 10 model , model_wo_decoder = CapsNet ( input_shape = [ 28 , 28 , 1 ], n_class = n_class , num_routing = 3 ) myprint () print ( \"Capsule Net (all)\" ) myprint () model . summary () myprint () print ( \"Capsule Net (without the decoder)\" ) myprint () model_wo_decoder . summary () ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Capsule Net (all) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ____________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ==================================================================================================== input_1 (InputLayer) (None, 28, 28, 1) 0 ____________________________________________________________________________________________________ Conv1 (Conv2D) (None, 20, 20, 256) 20992 input_1[0][0] ____________________________________________________________________________________________________ PrimaryCap_conv2d (Conv2D) (None, 6, 6, 256) 5308672 Conv1[0][0] ____________________________________________________________________________________________________ PrimaryCap_reshape (Reshape) (None, 1152, 8) 0 PrimaryCap_conv2d[0][0] ____________________________________________________________________________________________________ PrimaryCap_squash (Lambda) (None, 1152, 8) 0 PrimaryCap_reshape[0][0] ____________________________________________________________________________________________________ DigitCaps (CapsuleLayer) (None, 10, 16) 1486080 PrimaryCap_squash[0][0] ____________________________________________________________________________________________________ true_label (InputLayer) (None, 10) 0 ____________________________________________________________________________________________________ activity_vec (Mask) (None, 16) 0 DigitCaps[0][0] true_label[0][0] ____________________________________________________________________________________________________ FC1 (Dense) (None, 512) 8704 activity_vec[0][0] ____________________________________________________________________________________________________ FC2 (Dense) (None, 1024) 525312 FC1[0][0] ____________________________________________________________________________________________________ FC3 (Dense) (None, 784) 803600 FC2[0][0] ____________________________________________________________________________________________________ out_caps (Length) (None, 10) 0 DigitCaps[0][0] ____________________________________________________________________________________________________ out_recon (Reshape) (None, 28, 28, 1) 0 FC3[0][0] ==================================================================================================== Total params: 8,153,360 Trainable params: 8,141,840 Non-trainable params: 11,520 ____________________________________________________________________________________________________ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Capsule Net (without the decoder) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 28, 28, 1) 0 _________________________________________________________________ Conv1 (Conv2D) (None, 20, 20, 256) 20992 _________________________________________________________________ PrimaryCap_conv2d (Conv2D) (None, 6, 6, 256) 5308672 _________________________________________________________________ PrimaryCap_reshape (Reshape) (None, 1152, 8) 0 _________________________________________________________________ PrimaryCap_squash (Lambda) (None, 1152, 8) 0 _________________________________________________________________ DigitCaps (CapsuleLayer) (None, 10, 16) 1486080 ================================================================= Total params: 6,815,744 Trainable params: 6,804,224 Non-trainable params: 11,520 _________________________________________________________________ Load data Keras has nice API to extract MNIST data. Using keras.datasets, we can readily load data. from keras.datasets import mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() However, I have a connection issue in accessing MNIST data via this API. So instead, I use python-mnist for loading data. Split between training and testing. Following paper, 60K and 10K images are used for training and testing respectively. I reshape the data. In [5]: from mnist import MNIST import numpy as np from keras.utils import to_categorical mndata = MNIST ( './MNIST_data/' ) images , labels = mndata . load_training () x_train , y_train = np . array ( images ), np . array ( labels ) images , labels = mndata . load_testing () x_test , y_test = np . array ( images ), np . array ( labels ) x_train = x_train . reshape ( - 1 , 28 , 28 , 1 ) . astype ( 'float32' ) / 255. x_test = x_test . reshape ( - 1 , 28 , 28 , 1 ) . astype ( 'float32' ) / 255. y_train = to_categorical ( y_train . astype ( 'float32' )) y_test = to_categorical ( y_test . astype ( 'float32' )) print ( x_train . shape , x_test . shape , y_train . shape , y_test . shape ) ((60000, 28, 28, 1), (10000, 28, 28, 1), (60000, 10), (10000, 10)) Train CapsNet As in paper, training is performed on 28 x 28 MNIST images that have been shifted by up to 2 pixels in each direction with zero padding. This is done using Keras's ImageDataGenerator API. See my prebious blog that discusses the useage of this API. In [6]: import pandas as pd from keras.preprocessing.image import ImageDataGenerator from keras import callbacks from keras.utils.vis_utils import plot_model from datetime import datetime from numpy.random import seed from tensorflow import set_random_seed def train ( model , data , epochs , epoch_size_frac = 1.0 ): \"\"\" Training a CapsuleNet :param model: the CapsuleNet model :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))` :param args: arguments :return: The trained model \"\"\" start = datetime . today () model . compile ( optimizer = 'adam' , loss = [ margin_loss , 'mse' ], loss_weights = [ 1. , 0.0005 ], metrics = { 'out_caps' : 'accuracy' }) ( x_train , y_train ), ( x_test , y_test ) = data # callbacks log = callbacks . CSVLogger ( 'log.csv' ) checkpoint = callbacks . ModelCheckpoint ( 'weights-{epoch:02d}.h5' , save_best_only = True , save_weights_only = True , verbose = False ) lr_decay = callbacks . LearningRateScheduler ( schedule = lambda epoch : 0.001 * np . exp ( - epoch / 10. )) # compile the model model . compile ( optimizer = 'adam' , loss = [ margin_loss , 'mse' ], loss_weights = [ 1. , 0.0005 ], metrics = { 'out_caps' : 'accuracy' }) \"\"\" # Training without data augmentation: model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs, validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint]) \"\"\" # -----------------------------------Begin: Training with data augmentation -----------------------------------# def train_generator ( x , y , batch_size , shift_fraction = 0. ): train_datagen = ImageDataGenerator ( width_shift_range = shift_fraction , height_shift_range = shift_fraction ) # shift up to 2 pixel for MNIST generator = train_datagen . flow ( x , y , batch_size = batch_size ) while 1 : x_batch , y_batch = generator . next () yield ([ x_batch , y_batch ], [ y_batch , x_batch ]) # Training with data augmentation. If shift_fraction=0., also no augmentation. hist = model . fit_generator ( generator = train_generator ( x_train , y_train , 64 , 0.1 ), steps_per_epoch = int ( epoch_size_frac * y_train . shape [ 0 ] / 64 ), epochs = epochs , verbose = 2 , validation_data = [[ x_test , y_test ], [ y_test , x_test ]], callbacks = [ log , checkpoint , lr_decay ]) # -----------------------------------End: Training with data augmentation -----------------------------------# end = datetime . today () print ( \"_\" * 10 ) print ( \"Time Took: {}\" . format ( end - start )) return hist The paper did not say exact number of epochs. Here, I try 100 epochs. In [7]: seed ( 1 ) set_random_seed ( 1 ) hist = train ( model = model , epochs = 100 , #50, data = (( x_train , y_train ), ( x_test , y_test )), epoch_size_frac = 1 ) Epoch 1/100 382s - loss: 0.0840 - out_caps_loss: 0.0839 - out_recon_loss: 0.0818 - out_caps_acc: 0.9204 - val_loss: 0.0248 - val_out_caps_loss: 0.0248 - val_out_recon_loss: 0.0585 - val_out_caps_acc: 0.9862 Epoch 2/100 388s - loss: 0.0255 - out_caps_loss: 0.0255 - out_recon_loss: 0.0658 - out_caps_acc: 0.9814 - val_loss: 0.0146 - val_out_caps_loss: 0.0146 - val_out_recon_loss: 0.0562 - val_out_caps_acc: 0.9898 Epoch 3/100 390s - loss: 0.0193 - out_caps_loss: 0.0193 - out_recon_loss: 0.0632 - out_caps_acc: 0.9838 - val_loss: 0.0122 - val_out_caps_loss: 0.0122 - val_out_recon_loss: 0.0553 - val_out_caps_acc: 0.9905 Epoch 4/100 391s - loss: 0.0151 - out_caps_loss: 0.0151 - out_recon_loss: 0.0620 - out_caps_acc: 0.9880 - val_loss: 0.0113 - val_out_caps_loss: 0.0112 - val_out_recon_loss: 0.0553 - val_out_caps_acc: 0.9910 Epoch 5/100 391s - loss: 0.0136 - out_caps_loss: 0.0136 - out_recon_loss: 0.0612 - out_caps_acc: 0.9884 - val_loss: 0.0096 - val_out_caps_loss: 0.0096 - val_out_recon_loss: 0.0545 - val_out_caps_acc: 0.9916 Epoch 6/100 391s - loss: 0.0118 - out_caps_loss: 0.0118 - out_recon_loss: 0.0607 - out_caps_acc: 0.9902 - val_loss: 0.0093 - val_out_caps_loss: 0.0093 - val_out_recon_loss: 0.0542 - val_out_caps_acc: 0.9919 Epoch 7/100 390s - loss: 0.0109 - out_caps_loss: 0.0109 - out_recon_loss: 0.0602 - out_caps_acc: 0.9908 - val_loss: 0.0085 - val_out_caps_loss: 0.0085 - val_out_recon_loss: 0.0539 - val_out_caps_acc: 0.9926 Epoch 8/100 386s - loss: 0.0092 - out_caps_loss: 0.0092 - out_recon_loss: 0.0601 - out_caps_acc: 0.9928 - val_loss: 0.0106 - val_out_caps_loss: 0.0106 - val_out_recon_loss: 0.0540 - val_out_caps_acc: 0.9911 Epoch 9/100 384s - loss: 0.0088 - out_caps_loss: 0.0088 - out_recon_loss: 0.0598 - out_caps_acc: 0.9926 - val_loss: 0.0075 - val_out_caps_loss: 0.0075 - val_out_recon_loss: 0.0533 - val_out_caps_acc: 0.9934 Epoch 10/100 384s - loss: 0.0078 - out_caps_loss: 0.0077 - out_recon_loss: 0.0596 - out_caps_acc: 0.9937 - val_loss: 0.0069 - val_out_caps_loss: 0.0068 - val_out_recon_loss: 0.0533 - val_out_caps_acc: 0.9931 Epoch 11/100 383s - loss: 0.0071 - out_caps_loss: 0.0071 - out_recon_loss: 0.0595 - out_caps_acc: 0.9940 - val_loss: 0.0072 - val_out_caps_loss: 0.0071 - val_out_recon_loss: 0.0536 - val_out_caps_acc: 0.9936 Epoch 12/100 382s - loss: 0.0063 - out_caps_loss: 0.0063 - out_recon_loss: 0.0594 - out_caps_acc: 0.9951 - val_loss: 0.0062 - val_out_caps_loss: 0.0062 - val_out_recon_loss: 0.0534 - val_out_caps_acc: 0.9948 Epoch 13/100 382s - loss: 0.0059 - out_caps_loss: 0.0058 - out_recon_loss: 0.0594 - out_caps_acc: 0.9952 - val_loss: 0.0066 - val_out_caps_loss: 0.0065 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9939 Epoch 14/100 381s - loss: 0.0055 - out_caps_loss: 0.0055 - out_recon_loss: 0.0593 - out_caps_acc: 0.9957 - val_loss: 0.0056 - val_out_caps_loss: 0.0055 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9949 Epoch 15/100 381s - loss: 0.0049 - out_caps_loss: 0.0049 - out_recon_loss: 0.0592 - out_caps_acc: 0.9961 - val_loss: 0.0057 - val_out_caps_loss: 0.0057 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9950 Epoch 16/100 381s - loss: 0.0046 - out_caps_loss: 0.0045 - out_recon_loss: 0.0592 - out_caps_acc: 0.9965 - val_loss: 0.0052 - val_out_caps_loss: 0.0052 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9950 Epoch 17/100 383s - loss: 0.0042 - out_caps_loss: 0.0042 - out_recon_loss: 0.0591 - out_caps_acc: 0.9969 - val_loss: 0.0055 - val_out_caps_loss: 0.0054 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9941 Epoch 18/100 384s - loss: 0.0039 - out_caps_loss: 0.0039 - out_recon_loss: 0.0591 - out_caps_acc: 0.9969 - val_loss: 0.0054 - val_out_caps_loss: 0.0054 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9952 Epoch 19/100 384s - loss: 0.0035 - out_caps_loss: 0.0034 - out_recon_loss: 0.0590 - out_caps_acc: 0.9973 - val_loss: 0.0055 - val_out_caps_loss: 0.0054 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9957 Epoch 20/100 384s - loss: 0.0033 - out_caps_loss: 0.0032 - out_recon_loss: 0.0590 - out_caps_acc: 0.9977 - val_loss: 0.0053 - val_out_caps_loss: 0.0053 - val_out_recon_loss: 0.0530 - val_out_caps_acc: 0.9948 Epoch 21/100 383s - loss: 0.0030 - out_caps_loss: 0.0030 - out_recon_loss: 0.0589 - out_caps_acc: 0.9978 - val_loss: 0.0053 - val_out_caps_loss: 0.0053 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9954 Epoch 22/100 383s - loss: 0.0030 - out_caps_loss: 0.0030 - out_recon_loss: 0.0589 - out_caps_acc: 0.9978 - val_loss: 0.0048 - val_out_caps_loss: 0.0048 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9948 Epoch 23/100 383s - loss: 0.0025 - out_caps_loss: 0.0025 - out_recon_loss: 0.0590 - out_caps_acc: 0.9983 - val_loss: 0.0043 - val_out_caps_loss: 0.0043 - val_out_recon_loss: 0.0533 - val_out_caps_acc: 0.9960 Epoch 24/100 383s - loss: 0.0026 - out_caps_loss: 0.0026 - out_recon_loss: 0.0590 - out_caps_acc: 0.9981 - val_loss: 0.0048 - val_out_caps_loss: 0.0048 - val_out_recon_loss: 0.0533 - val_out_caps_acc: 0.9957 Epoch 25/100 383s - loss: 0.0023 - out_caps_loss: 0.0022 - out_recon_loss: 0.0588 - out_caps_acc: 0.9984 - val_loss: 0.0046 - val_out_caps_loss: 0.0045 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9957 Epoch 26/100 383s - loss: 0.0023 - out_caps_loss: 0.0023 - out_recon_loss: 0.0588 - out_caps_acc: 0.9984 - val_loss: 0.0047 - val_out_caps_loss: 0.0046 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9958 Epoch 27/100 383s - loss: 0.0020 - out_caps_loss: 0.0020 - out_recon_loss: 0.0588 - out_caps_acc: 0.9987 - val_loss: 0.0048 - val_out_caps_loss: 0.0047 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9956 Epoch 28/100 383s - loss: 0.0020 - out_caps_loss: 0.0019 - out_recon_loss: 0.0588 - out_caps_acc: 0.9986 - val_loss: 0.0048 - val_out_caps_loss: 0.0048 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9953 Epoch 29/100 383s - loss: 0.0020 - out_caps_loss: 0.0019 - out_recon_loss: 0.0587 - out_caps_acc: 0.9987 - val_loss: 0.0043 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 30/100 380s - loss: 0.0016 - out_caps_loss: 0.0016 - out_recon_loss: 0.0587 - out_caps_acc: 0.9991 - val_loss: 0.0044 - val_out_caps_loss: 0.0044 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9961 Epoch 31/100 380s - loss: 0.0017 - out_caps_loss: 0.0017 - out_recon_loss: 0.0587 - out_caps_acc: 0.9989 - val_loss: 0.0045 - val_out_caps_loss: 0.0044 - val_out_recon_loss: 0.0533 - val_out_caps_acc: 0.9956 Epoch 32/100 380s - loss: 0.0017 - out_caps_loss: 0.0016 - out_recon_loss: 0.0588 - out_caps_acc: 0.9990 - val_loss: 0.0044 - val_out_caps_loss: 0.0044 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9956 Epoch 33/100 380s - loss: 0.0015 - out_caps_loss: 0.0015 - out_recon_loss: 0.0587 - out_caps_acc: 0.9990 - val_loss: 0.0044 - val_out_caps_loss: 0.0044 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 34/100 383s - loss: 0.0015 - out_caps_loss: 0.0014 - out_recon_loss: 0.0588 - out_caps_acc: 0.9992 - val_loss: 0.0045 - val_out_caps_loss: 0.0045 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9953 Epoch 35/100 383s - loss: 0.0014 - out_caps_loss: 0.0014 - out_recon_loss: 0.0588 - out_caps_acc: 0.9992 - val_loss: 0.0045 - val_out_caps_loss: 0.0045 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 36/100 381s - loss: 0.0014 - out_caps_loss: 0.0014 - out_recon_loss: 0.0587 - out_caps_acc: 0.9992 - val_loss: 0.0043 - val_out_caps_loss: 0.0043 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9953 Epoch 37/100 380s - loss: 0.0013 - out_caps_loss: 0.0013 - out_recon_loss: 0.0588 - out_caps_acc: 0.9993 - val_loss: 0.0043 - val_out_caps_loss: 0.0043 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9959 Epoch 38/100 380s - loss: 0.0013 - out_caps_loss: 0.0013 - out_recon_loss: 0.0588 - out_caps_acc: 0.9991 - val_loss: 0.0043 - val_out_caps_loss: 0.0043 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 39/100 379s - loss: 0.0013 - out_caps_loss: 0.0013 - out_recon_loss: 0.0588 - out_caps_acc: 0.9993 - val_loss: 0.0043 - val_out_caps_loss: 0.0043 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 40/100 380s - loss: 0.0012 - out_caps_loss: 0.0012 - out_recon_loss: 0.0588 - out_caps_acc: 0.9993 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9960 Epoch 41/100 380s - loss: 0.0012 - out_caps_loss: 0.0012 - out_recon_loss: 0.0587 - out_caps_acc: 0.9992 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9959 Epoch 42/100 381s - loss: 0.0011 - out_caps_loss: 0.0011 - out_recon_loss: 0.0587 - out_caps_acc: 0.9993 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0531 - val_out_caps_acc: 0.9960 Epoch 43/100 382s - loss: 0.0011 - out_caps_loss: 0.0011 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 44/100 382s - loss: 0.0012 - out_caps_loss: 0.0011 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0043 - val_out_caps_loss: 0.0043 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9960 Epoch 45/100 382s - loss: 0.0010 - out_caps_loss: 9.8727e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0043 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9959 Epoch 46/100 382s - loss: 0.0011 - out_caps_loss: 0.0011 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9960 Epoch 47/100 382s - loss: 0.0010 - out_caps_loss: 9.8633e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0043 - val_out_caps_loss: 0.0043 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 48/100 381s - loss: 0.0012 - out_caps_loss: 0.0011 - out_recon_loss: 0.0587 - out_caps_acc: 0.9993 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9956 Epoch 49/100 380s - loss: 0.0010 - out_caps_loss: 0.0010 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0043 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 50/100 379s - loss: 0.0010 - out_caps_loss: 9.9851e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9955 Epoch 51/100 380s - loss: 0.0011 - out_caps_loss: 0.0011 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 52/100 379s - loss: 0.0010 - out_caps_loss: 9.7172e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9960 Epoch 53/100 380s - loss: 9.6384e-04 - out_caps_loss: 9.3450e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9960 Epoch 54/100 379s - loss: 9.5086e-04 - out_caps_loss: 9.2148e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0041 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 55/100 380s - loss: 9.2795e-04 - out_caps_loss: 8.9861e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0041 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 56/100 379s - loss: 0.0010 - out_caps_loss: 9.9036e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 57/100 380s - loss: 9.6289e-04 - out_caps_loss: 9.3352e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0041 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9960 Epoch 58/100 380s - loss: 9.2247e-04 - out_caps_loss: 8.9310e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9996 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 59/100 379s - loss: 9.4866e-04 - out_caps_loss: 9.1929e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9956 Epoch 60/100 380s - loss: 9.7747e-04 - out_caps_loss: 9.4812e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9956 Epoch 61/100 381s - loss: 0.0010 - out_caps_loss: 9.8595e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 62/100 379s - loss: 9.6557e-04 - out_caps_loss: 9.3622e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0041 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9959 Epoch 63/100 379s - loss: 9.8001e-04 - out_caps_loss: 9.5062e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 64/100 379s - loss: 9.3814e-04 - out_caps_loss: 9.0879e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9996 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 65/100 381s - loss: 9.6921e-04 - out_caps_loss: 9.3984e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9956 Epoch 66/100 381s - loss: 9.7658e-04 - out_caps_loss: 9.4722e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9959 Epoch 67/100 381s - loss: 9.5560e-04 - out_caps_loss: 9.2624e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9960 Epoch 68/100 381s - loss: 9.9863e-04 - out_caps_loss: 9.6929e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 69/100 381s - loss: 9.9393e-04 - out_caps_loss: 9.6456e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 70/100 381s - loss: 9.1131e-04 - out_caps_loss: 8.8195e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9996 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 71/100 381s - loss: 9.9856e-04 - out_caps_loss: 9.6921e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9960 Epoch 72/100 380s - loss: 9.1848e-04 - out_caps_loss: 8.8910e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 73/100 380s - loss: 9.3726e-04 - out_caps_loss: 9.0791e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9959 Epoch 74/100 379s - loss: 0.0010 - out_caps_loss: 9.9644e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9993 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 75/100 379s - loss: 0.0010 - out_caps_loss: 9.8316e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 76/100 379s - loss: 9.9465e-04 - out_caps_loss: 9.6529e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9993 - val_loss: 0.0042 - val_out_caps_loss: 0.0042 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 77/100 379s - loss: 0.0010 - out_caps_loss: 9.8456e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 78/100 379s - loss: 9.6042e-04 - out_caps_loss: 9.3105e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 79/100 379s - loss: 9.4492e-04 - out_caps_loss: 9.1553e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 80/100 379s - loss: 9.0155e-04 - out_caps_loss: 8.7220e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 81/100 379s - loss: 9.3271e-04 - out_caps_loss: 9.0336e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9957 Epoch 82/100 379s - loss: 9.8777e-04 - out_caps_loss: 9.5841e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9993 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 83/100 379s - loss: 9.4561e-04 - out_caps_loss: 9.1624e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 84/100 379s - loss: 0.0010 - out_caps_loss: 9.7501e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 85/100 379s - loss: 9.1841e-04 - out_caps_loss: 8.8903e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 86/100 379s - loss: 9.4855e-04 - out_caps_loss: 9.1920e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 87/100 379s - loss: 9.4658e-04 - out_caps_loss: 9.1723e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 88/100 379s - loss: 8.9132e-04 - out_caps_loss: 8.6196e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 89/100 379s - loss: 9.4895e-04 - out_caps_loss: 9.1962e-04 - out_recon_loss: 0.0586 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 90/100 379s - loss: 9.2920e-04 - out_caps_loss: 8.9982e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 91/100 380s - loss: 9.9252e-04 - out_caps_loss: 9.6317e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 92/100 379s - loss: 9.4020e-04 - out_caps_loss: 9.1085e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 93/100 379s - loss: 9.8118e-04 - out_caps_loss: 9.5181e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9993 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 94/100 380s - loss: 9.4192e-04 - out_caps_loss: 9.1257e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 95/100 379s - loss: 9.0891e-04 - out_caps_loss: 8.7955e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 96/100 380s - loss: 9.5846e-04 - out_caps_loss: 9.2910e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 97/100 380s - loss: 9.3610e-04 - out_caps_loss: 9.0674e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 98/100 380s - loss: 8.9387e-04 - out_caps_loss: 8.6452e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9995 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 99/100 380s - loss: 9.2877e-04 - out_caps_loss: 8.9941e-04 - out_recon_loss: 0.0587 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 Epoch 100/100 380s - loss: 9.8709e-04 - out_caps_loss: 9.5768e-04 - out_recon_loss: 0.0588 - out_caps_acc: 0.9994 - val_loss: 0.0042 - val_out_caps_loss: 0.0041 - val_out_recon_loss: 0.0532 - val_out_caps_acc: 0.9958 __________ Time Took: 10:36:17.718351 Save model architecutres and weights into current directory In [8]: def save_model ( model , name ): ''' save model architecture and model weights ''' json_string = model . to_json () open ( name + '_architecture.json' , 'w' ) . write ( json_string ) model . save_weights ( name + '_weights.h5' ) save_model ( model , \"CapsNet\" ) The size of the final model There are 8.2M parameters in this model. What is the size? In [9]: import os file_paths = [ \"CapsNet_weights.h5\" , \"CapsNet_architecture.json\" ] for file_path in file_paths : w = os . stat ( file_path ) . st_size print ( \"{:30} {:4.2f} MB\" . format ( file_path , w / ( 1024.0 ** 2 ))) CapsNet_weights.h5 31.13 MB CapsNet_architecture.json 0.01 MB Plotting accuracy and validation accuracy over epochs The validation accuracy (and loss, plot not shown) seems to be stabilized after 40 epochs. 100 epochs are more than enough for the convergence of the algorithm. In [10]: for label in [ \"out_caps_acc\" , \"val_out_caps_acc\" ]: plt . plot ( hist . history [ label ], label = label ) plt . title ( \"Final validation accuracy: {}\" . format ( hist . history [ \"val_out_caps_acc\" ][ - 1 ])) plt . legend () plt . ylim ( 0.98 , 1 ) plt . show () In [11]: def combine_images ( generated_images ): num = generated_images . shape [ 0 ] width = int ( np . sqrt ( num )) height = int ( np . ceil ( float ( num ) / width )) shape = generated_images . shape [ 1 : 3 ] image = np . zeros (( height * shape [ 0 ], width * shape [ 1 ]), dtype = generated_images . dtype ) for index , img in enumerate ( generated_images ): i = int ( index / width ) j = index % width image [ i * shape [ 0 ]:( i + 1 ) * shape [ 0 ], j * shape [ 1 ]:( j + 1 ) * shape [ 1 ]] = img [:, :, 0 ] return image def test ( model , data ): x_test , y_test = data y_pred , x_recon = model . predict ([ x_test , y_test ], batch_size = 100 ) print ( '-' * 50 ) class_pred = np . argmax ( y_pred , 1 ) class_test = np . argmax ( y_test , 1 ) acc = np . sum ( class_pred == class_test ) / float ( y_test . shape [ 0 ]) * 100 print ( 'Test accuracy: {:5.3f}%' . format ( acc )) print ( 'Test error : {:5.3f}%' . format (( 100 - acc ))) from PIL import Image img = combine_images ( np . concatenate ([ x_test [: 50 ], x_recon [: 50 ]])) image = img * 255 print ( '-' * 50 ) plt . figure ( figsize = ( 10 , 10 )) plt . imshow ( image ) plt . show () Test set results Testing error of paper was 0.25% with standard deviation of 0.005 after 3 trials. My error was 0.420%, not quite as low; I did not reproduce the result entirely. This is probablly because I only consider one testing set and not three, and some of the parameter values are not the same as paper , e.g., number of epochs or parameters of Adam's optimizer. (The paper use Tensorflow for implementation.) Unfortunatelly, the error of my CapsNet was even higher than the error produced by the baseline (0.39%). However, when I try fitting this baseline model to the MNIST data in my prebious blog , the validation loss of my baseline model was 0.5%. At least, the validation loss of my CapsNet model is less than the validation loss of the my baseline. This means that my CapsNet model classified additional 12 (= 10000x(0.540 - 0.420)/100) images correctly in comparisons to my baseline model. Correct classification of additional 8 images sounds low, and the statistical significance of improvment from the CapsNet is questionable for MNIST-like data. paper validates CapsNet's performance on other data such as CIFAR10 or smallNORB. I should also consider applying CapsNet for more complex image data. In [12]: ntest = x_test . shape [ 0 ] test ( model = model , data = ( x_test [: ntest ], y_test [: ntest ])) -------------------------------------------------- Test accuracy: 99.580% Test error : 0.420% -------------------------------------------------- Interprete Capsule Networks Since the Capsule Network passing the acitivity vector to encoding of only one digit and zeroing out other digits (Mask method is doing this in the code), the activity vector of the correct class $\\boldsymbol{v}_{\\textrm{correct}}$ should be able to reconstruct the image well while the activity vector of the incorrect $\\boldsymbol{v}_{\\textrm{not correct}}$ cannot reconstruct sensible image. Let's see if this is the case. In order to visualize the reconstructed images from every activity vector, we create a decoder model that takes activity vectors as input and output the reconstructed images. In [13]: dim_vector = 16 activity_vec = layers . Input ( shape = ( dim_vector ,), name = \"activity_vec\" ) decoder_out = Decoder ( activity_vec ) decoder_model = models . Model ( activity_vec , decoder_out ) decoder_model . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= activity_vec (InputLayer) (None, 16) 0 _________________________________________________________________ FC1 (Dense) (None, 512) 8704 _________________________________________________________________ FC2 (Dense) (None, 1024) 525312 _________________________________________________________________ FC3 (Dense) (None, 784) 803600 _________________________________________________________________ out_recon (Reshape) (None, 28, 28, 1) 0 ================================================================= Total params: 1,337,616 Trainable params: 1,337,616 Non-trainable params: 0 _________________________________________________________________ Transfer the weights trained Capsule Network to the newly created decoder model. In [14]: for layer_CapsNet in model . layers : for layer_decoder in decoder_model . layers : if layer_decoder . name == layer_CapsNet . name : weight = layer_CapsNet . get_weights () if len ( weight ) != 0 : layer_decoder . set_weights ( weight ) Example reconstructed images from activity vector of each capsule in DigitCaps layer Observations Expectedly, the reconstructed image from the activity vector of the correct class reproduces the better image. We also see that when the model is not confident about the prediction ($|| \\boldsymbol{v}_j ||$ is high for multiple $j$), all reconstructed images of clases with high predicted probabilities are also well represented. See the example image for digit 2. The model thinks that the digit could be either digit 2 or 7. I am slighly surprized to see that many lengths of output vector of the capsule are almost zero, i.e., the probability that the image belongs to wrong digit class is almost zero, correctly. As the margin loss function was defined as: $$ L_k = T_k \\max(0, m&#94;+ - || \\boldsymbol{v}_k ||)&#94;2 + (1-T_k) \\max(0, || \\boldsymbol{v}_k || - m&#94;-)&#94;2 $$ where $T_k=1$ iff a digit of class $k$ is present, and $m&#94;+=0.9$ and $m&#94;-=0.1$. I interpret this loss as \"avoid model being over confident. The model is good enough if the probability estimate is < 0.1 when the class is incorrect and > 0.9 if the probability estimate is > 0.9 when the class is correct.\" So the probability estimate of 0.1 is just as confident as 0 that the class is incorrect, and 0.9 is just as confident as 0.95 that the class is correct. However, the model seems to return probability estimates of 0 pretty often when the class is incorrect. In [37]: #Npic = 5 #random_pick_pics = np.random.choice(x_test.shape[1],Npic,replace=False) random_pick_pics = [ 10 , ## 0 1097 , ## 1 924 , ## 2 looks like 7 912 , ## 3 5923 , ## 4 1948 , ## 5 3923 , ## 6 124 , ## 7 800 , ## 8 7 ## 9 ] Npic = len ( random_pick_pics ) count = 1 fig = plt . figure ( figsize = ( 20 , 2 * Npic )) for image_index in random_pick_pics : orig_image = x_test [[ image_index ]] V = model_wo_decoder . predict ( orig_image ) orig_image = orig_image . reshape ( 28 , 28 ) ax = fig . add_subplot ( Npic , 11 , count , xticks = [], yticks = []) ax . imshow ( orig_image ) ax . set_title ( \"original\" ) count += 1 for myclass , pred_class in enumerate ( range ( V . shape [ 1 ])): Vpred_class = V [ 0 , pred_class ,:] reconst_image = decoder_model . predict ( Vpred_class . reshape ( 1 , 16 )) ax = fig . add_subplot ( Npic , 11 , count , xticks = [], yticks = []) ax . imshow ( reconst_image . reshape ( 28 , 28 )) ax . set_title ( \"{}: ||v||={:3.2f}\" . format ( myclass , np . sum ( Vpred_class ** 2 ))) count += 1 plt . show () Evaluate the values of activity vectors In [16]: activity_vecs = model_wo_decoder . predict ( x_test ) print ( activity_vecs . shape ) (10000, 10, 16) Dimension perturbations One thing that really fascinates me about the CapsNet was the interpretability of the model. When working with deep learning models for customer projects, I often have difficulty in explaining why the model work (or not work) to non-exparts. Many researchers put efforts to visualize and interpret the model and I also discuss this in the previous post . It would be great if CapsNet can help me interpret the model! Hinton discusses the interpretability and says: \" ... dimensions of a digit capsule should learn to span the space of variations in the way digits of that class are instantiated. These variations include stroke thickness, skew and width. They also include digit-specific variations such as the length of the tail of a 2. We can see what the individual dimensions represent by making use of the decoder network. \" Each dimention of the acivity vector could represent stroke, thickness etc?! That is great! The intepretability of the model was explained with Figure 4 of page 6 in paper , which shows the reconstruction when one of the 16 dimensions in the DigitCaps representation is tweaked by intervals of 0.05 in the range [-0.25,0.25]. Figure 4 explains what the individual dimensions of a capsule represent. I try to reproduce images similar to Figure 4. Although the range of the dimensions are set to [-0.25,0.25], I rather want to make the range to span the entire possible values of the activity vector. Therefore, first, evaluate the range of each element of the activity vector of a capsule. Remind you that the activity vector $\\boldsymbol{v}_j$ is defined as: $$ \\boldsymbol{v}_j = \\frac{ ||\\boldsymbol{s}_j||&#94;2 }{ 1 + ||\\boldsymbol{s}_j||&#94;2 } \\frac{ \\boldsymbol{s}_j }{ ||\\boldsymbol{s}_j|| } $$ As it is defined to be scaled normalized vector with a scale ranging between 0 and 1, each element of the activity vector ranges between -1 and 1. The following code shows that the range is rather narrower. In [17]: min_avec = activity_vecs . min () max_avec = activity_vecs . max () print ( \"The range of each element of the output vector:\" ) print ( \"Min: {:4.2f}\" . format ( min_avec )) print ( \"Max: {:4.2f}\" . format ( max_avec )) avec_range = np . arange ( min_avec , max_avec ,( max_avec - min_avec ) / 5 ) plt . hist ( activity_vecs . flatten ()) plt . title ( \"The distribution of elements of the activity vector\" ) plt . show () The range of each element of the output vector: Min: -0.44 Max: 0.40 Observations Paper says \"We found that one dimension (out of 16) of the capsule almost always represents the width of the digit\". In my particular experiment, dimension 1 (and maybe 3 and 12) seems to measure the width of the digit. The thickness of the stroke is represented by dimension 8. For digit 6, the size of the loop relative to the size of the tail is measured by dimension 10. This seems to be the case for digit 4, and digit 9. In general, the intepretation is not super straightfoward, and you have to sit down and stare at the image a while. But it seems to be not impossible to interpret each dimension of the activity vector. In [18]: from copy import copy def plot_dimension_perturbation ( x_test1 , y_test1 , avec_range ): ''' x_test1 : np.array() of 1 x 28 x 28, containing image y_test1 : np.array() of length 10, binary array indicating class avec_range : the value of activity vectors to replace the original ''' y_test1 = y_test1 . flatten () true_class = np . where ( y_test1 == 1 )[ 0 ] V = model_wo_decoder . predict ( x_test1 ) const = 0.5 fig = plt . figure ( figsize = ( 16 * const , len ( avec_range ) * const )) fig . subplots_adjust ( hspace = 0.05 , wspace = 0.0001 , left = 0 , right = 1 , bottom = 0 , top = 1 ) count = 1 for value in avec_range : for di in range ( V . shape [ 2 ]): activity_vec = ( copy ( V [ 0 , true_class ,:])) . flatten () activity_vec [ di ] = value reconst_image = decoder_model . predict ( activity_vec . reshape ( 1 , 16 )) ax = fig . add_subplot ( len ( avec_range ), 16 , count , xticks = [], yticks = []) ax . imshow ( reconst_image . reshape ( 28 , 28 ), cmap = \"gray\" ) if value == avec_range [ 0 ]: ax . set_title ( \"V[{}]\" . format ( di )) if di == 0 : ax . set_ylabel ( \"V[i]={:3.2f}\" . format ( value ), rotation = 0 , fontsize = 10 , labelpad = 30 ) count += 1 plt . show () for image_index in [ 296 , ## 0 920 , ## 1 421 , ## 2 369 , ## 3 399 , ## 4 502 , ## 5 123 , ## 6 2123 , ## 7 998 , ## 8 1013 ## 9 ]: plot_dimension_perturbation ( x_test [[ image_index ]], y_test [[ image_index ]], avec_range ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Understanding and Experimenting Capsule Networks"},{"url":"CNN-modeling-with-image-translations-using-MNIST-data.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In this blog, I train a standard CNN model on the MNIST data and assess its performance. In [1]: import matplotlib.pyplot as plt import tensorflow as tf from keras.backend.tensorflow_backend import set_session import keras import sys print ( \"python {}\" . format ( sys . version )) print ( \"keras version {}\" . format ( keras . __version__ )) print ( \"tensorflow version {}\" . format ( tf . __version__ )) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"3\" #### 2 GPU1 #### 3 GPU2 #### 0 GPU3 #### 4 GPU4 set_session ( tf . Session ( config = config )) Using TensorFlow backend. python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] keras version 2.0.6 tensorflow version 1.2.1 In [2]: from mnist import MNIST import numpy as np from keras.utils import to_categorical mndata = MNIST ( './MNIST_data/' ) images , labels = mndata . load_training () x_train , y_train = np . array ( images ), np . array ( labels ) images , labels = mndata . load_testing () x_test , y_test = np . array ( images ), np . array ( labels ) x_train = x_train . reshape ( - 1 , 28 , 28 , 1 ) . astype ( 'float32' ) / 255. x_test = x_test . reshape ( - 1 , 28 , 28 , 1 ) . astype ( 'float32' ) / 255. y_train = to_categorical ( y_train . astype ( 'float32' )) y_test = to_categorical ( y_test . astype ( 'float32' )) print ( x_train . shape , x_test . shape , y_train . shape , y_test . shape ) ((60000, 28, 28, 1), (10000, 28, 28, 1), (60000, 10), (10000, 10)) CNN model I define a standard CNN with three convolutional layers of 256, 256, 128 channels. Each has 5x5 kernels and stride of 1. The last convolutional layers are followed by two fully connected layers of size 328, 192. The last fully connected layer is connected with dropout to a 10 class softmax layer with cross entropy loss. According to Recent Hinton's paper , \"[This model] is designed to achieve the best performance on MNIST\" when model complexity is at this level. This model contains 35M parameters. In [3]: from keras.layers import Conv2D , MaxPooling2D , Flatten , Dropout , Activation , Dense from keras.models import Sequential from keras import optimizers from numpy.random import seed from tensorflow import set_random_seed def StandardCNN ( withDropout = False , n_class = 10 , input_shape = ( 96 , 96 , 1 )): ''' WithDropout: If True, then dropout regularlization is added. This feature is experimented later. ''' model = Sequential () model . add ( Conv2D ( 256 ,( 5 , 5 ), strides = 1 , activation = \"relu\" , padding = \"same\" , input_shape = input_shape )) ## 96 - 3 + 2 #model.add(MaxPooling2D(pool_size = (2,2))) ## 96 - (3-1)*2 model . add ( Conv2D ( 256 ,( 5 , 5 ), strides = 1 , activation = \"relu\" , padding = \"same\" )) #model.add(MaxPooling2D(pool_size = (2,2))) model . add ( Conv2D ( 128 ,( 5 , 5 ), strides = 1 , activation = \"relu\" , padding = \"same\" )) #model.add(MaxPooling2D(pool_size=(2,2))) model . add ( Flatten ()) model . add ( Dense ( 328 , activation = 'relu' )) model . add ( Dense ( 192 , activation = 'relu' )) ## 1024, 2048, 4096,8192 model . add ( Dropout ( 0.5 )) model . add ( Dense ( n_class )) model . add ( Activation ( 'softmax' , name = \"output\" )) model . compile ( loss = \"categorical_crossentropy\" , optimizer = optimizers . Adam ( lr = 0.0005 ), metrics = { 'output' : 'accuracy' }) return ( model ) model = StandardCNN ( input_shape = ( 28 , 28 , 1 )) model . summary () _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 28, 28, 256) 6656 _________________________________________________________________ conv2d_2 (Conv2D) (None, 28, 28, 256) 1638656 _________________________________________________________________ conv2d_3 (Conv2D) (None, 28, 28, 128) 819328 _________________________________________________________________ flatten_1 (Flatten) (None, 100352) 0 _________________________________________________________________ dense_1 (Dense) (None, 328) 32915784 _________________________________________________________________ dense_2 (Dense) (None, 192) 63168 _________________________________________________________________ dropout_1 (Dropout) (None, 192) 0 _________________________________________________________________ dense_3 (Dense) (None, 10) 1930 _________________________________________________________________ output (Activation) (None, 10) 0 ================================================================= Total params: 35,445,522 Trainable params: 35,445,522 Non-trainable params: 0 _________________________________________________________________ Training We set the number of epochs to be 20. In [4]: %% time seed(1) set_random_seed(1) hist1 = model.fit(x_train, y_train, batch_size=64, nb_epoch=20, validation_data=[x_test, y_test],verbose=2) /home/fairy/anaconda2/lib/python2.7/site-packages/keras/models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`. warnings.warn('The `nb_epoch` argument in `fit` ' Train on 60000 samples, validate on 10000 samples Epoch 1/20 52s - loss: 0.1395 - acc: 0.9581 - val_loss: 0.0349 - val_acc: 0.9884 Epoch 2/20 51s - loss: 0.0480 - acc: 0.9862 - val_loss: 0.0340 - val_acc: 0.9889 Epoch 3/20 51s - loss: 0.0308 - acc: 0.9907 - val_loss: 0.0261 - val_acc: 0.9924 Epoch 4/20 51s - loss: 0.0235 - acc: 0.9933 - val_loss: 0.0336 - val_acc: 0.9912 Epoch 5/20 51s - loss: 0.0183 - acc: 0.9948 - val_loss: 0.0345 - val_acc: 0.9911 Epoch 6/20 50s - loss: 0.0138 - acc: 0.9958 - val_loss: 0.0326 - val_acc: 0.9916 Epoch 7/20 50s - loss: 0.0109 - acc: 0.9969 - val_loss: 0.0280 - val_acc: 0.9918 Epoch 8/20 50s - loss: 0.0097 - acc: 0.9974 - val_loss: 0.0373 - val_acc: 0.9915 Epoch 9/20 50s - loss: 0.0082 - acc: 0.9978 - val_loss: 0.0449 - val_acc: 0.9911 Epoch 10/20 50s - loss: 0.0075 - acc: 0.9977 - val_loss: 0.0413 - val_acc: 0.9908 Epoch 11/20 50s - loss: 0.0065 - acc: 0.9984 - val_loss: 0.0324 - val_acc: 0.9914 Epoch 12/20 50s - loss: 0.0081 - acc: 0.9976 - val_loss: 0.0345 - val_acc: 0.9922 Epoch 13/20 50s - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0456 - val_acc: 0.9906 Epoch 14/20 51s - loss: 0.0052 - acc: 0.9986 - val_loss: 0.0525 - val_acc: 0.9892 Epoch 15/20 51s - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0429 - val_acc: 0.9922 Epoch 16/20 50s - loss: 0.0028 - acc: 0.9992 - val_loss: 0.0453 - val_acc: 0.9925 Epoch 17/20 50s - loss: 0.0061 - acc: 0.9985 - val_loss: 0.0521 - val_acc: 0.9908 Epoch 18/20 50s - loss: 0.0065 - acc: 0.9985 - val_loss: 0.0413 - val_acc: 0.9931 Epoch 19/20 50s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0392 - val_acc: 0.9929 Epoch 20/20 50s - loss: 0.0055 - acc: 0.9988 - val_loss: 0.0516 - val_acc: 0.9917 CPU times: user 13min 8s, sys: 1min 56s, total: 15min 5s Wall time: 16min 57s The back propagation algorithm seems to converge. In [5]: for label in [ \"val_acc\" , \"acc\" ]: plt . plot ( hist1 . history [ label ], label = label ) plt . xlabel ( \"epoch\" ) plt . ylabel ( \"accuracy\" ) plt . title ( \"validation accuracy, model1:{:6.5f}\" . format ( hist1 . history [ \"val_acc\" ][ - 1 ])) plt . legend () plt . ylim ( 0.95 , 1.005 ) plt . show () Train the model with data augmentation I now train the same model with image translation. \"translation of image\" means shifting image by costant value to right, left, up or down, or some combinations of these. Keras provides the ImageDataGenerator class that defines the configuration for image data preparation and augmentation. Rather than performing the operations on your entire image dataset in memory, the ImageDataGenerator API is designed to be iterated by the deep learning model fitting process, creating augmented image data for you just-in-time. This reduces your memory overhead, but adds some additional time cost during model training. I discussed the useage of ImageDataGenerator in previous blog post . I also found Jason Brownlee's blog is very useful for learning ImageDataGenerator. train_generator First, define train_generator method which infenitely return training and testing batch. batch_size determines the batch size. generator.next() randomly select batch_size many samples from x_train, then the random transformation is applied to the selected images. If the x_train.shape[0] < batch_size, the returned batch_size is x_train.shape[0]. In [6]: from keras.preprocessing.image import ImageDataGenerator def train_generator ( x , y , batch_size , shift_fraction = 0. ): train_datagen = ImageDataGenerator ( width_shift_range = shift_fraction , height_shift_range = shift_fraction ) # shift up to 2 pixel for MNIST generator = train_datagen . flow ( x , y , batch_size = batch_size ) while 1 : x_batch , y_batch = generator . next () ## x_batch.shape = (bsize,28,28,1) ## where bsize = np.min(batch_size,x.shape[0]) yield ([ x_batch , y_batch ]) The following code shows the images generated by train_generator when only 5 images are fed into the train_generator. Although the batch_size is set to 1,000, every batch only yields 5 transformed images, and original image is transoformed only once. In [7]: def plot_sample ( x_train , y_train , batch_size , shift_fraction = 0.4 ): count = 0 for generator in train_generator ( x_train , y_train , batch_size , shift_fraction ): x_tra , y_tra = generator count += 1 if count > 0 : break print ( x_tra . shape , y_tra . shape ) n = 5 fig = plt . figure ( figsize = ( 10 , 10 )) for i in range ( x_tra . shape [ 0 ]): ax = fig . add_subplot ( n , n , i + 1 ) ax . imshow ( x_tra [ i ] . reshape ( 28 , 28 )) if ( n * n ) < ( i + 2 ): break plt . show () plot_sample ( x_train [: 5 ], y_train [: 5 ], batch_size = 1000 , shift_fraction = 0.4 ) plot_sample ( x_train [: 5 ], y_train [: 5 ], batch_size = 1000 , shift_fraction = 0.4 ) ((5, 28, 28, 1), (5, 10)) ((5, 28, 28, 1), (5, 10)) Model training I will set the shift_fraction = 0.1. This means that each image is shifted at most 3 pixels (28x0.1 = 2.8). I noticed that fit_generator causes error messages when steps_per_epoch is NOT specified. This is \"Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch.\" In [8]: %% time seed(1) set_random_seed(1) batch_size = 64 model2 = StandardCNN(input_shape = (28, 28, 1)) hist2 = model2.fit_generator( generator=train_generator(x_train, y_train, batch_size, 0.1), epochs=20, verbose=2, steps_per_epoch=int(y_train.shape[0] / batch_size), validation_data=[x_test, y_test])#,callbacks=[log, checkpoint, lr_decay]) Epoch 1/20 51s - loss: 0.2261 - acc: 0.9295 - val_loss: 0.0298 - val_acc: 0.9899 Epoch 2/20 50s - loss: 0.0744 - acc: 0.9790 - val_loss: 0.0329 - val_acc: 0.9884 Epoch 3/20 50s - loss: 0.0512 - acc: 0.9858 - val_loss: 0.0330 - val_acc: 0.9900 Epoch 4/20 50s - loss: 0.0418 - acc: 0.9879 - val_loss: 0.0280 - val_acc: 0.9911 Epoch 5/20 50s - loss: 0.0380 - acc: 0.9890 - val_loss: 0.0191 - val_acc: 0.9939 Epoch 6/20 50s - loss: 0.0332 - acc: 0.9908 - val_loss: 0.0206 - val_acc: 0.9934 Epoch 7/20 50s - loss: 0.0292 - acc: 0.9913 - val_loss: 0.0276 - val_acc: 0.9924 Epoch 8/20 50s - loss: 0.0267 - acc: 0.9927 - val_loss: 0.0195 - val_acc: 0.9936 Epoch 9/20 50s - loss: 0.0247 - acc: 0.9928 - val_loss: 0.0166 - val_acc: 0.9949 Epoch 10/20 50s - loss: 0.0212 - acc: 0.9938 - val_loss: 0.0176 - val_acc: 0.9949 Epoch 11/20 50s - loss: 0.0208 - acc: 0.9941 - val_loss: 0.0197 - val_acc: 0.9945 Epoch 12/20 50s - loss: 0.0178 - acc: 0.9945 - val_loss: 0.0172 - val_acc: 0.9949 Epoch 13/20 50s - loss: 0.0185 - acc: 0.9950 - val_loss: 0.0233 - val_acc: 0.9946 Epoch 14/20 50s - loss: 0.0189 - acc: 0.9947 - val_loss: 0.0182 - val_acc: 0.9942 Epoch 15/20 50s - loss: 0.0145 - acc: 0.9959 - val_loss: 0.0305 - val_acc: 0.9928 Epoch 16/20 50s - loss: 0.0168 - acc: 0.9953 - val_loss: 0.0185 - val_acc: 0.9945 Epoch 17/20 50s - loss: 0.0139 - acc: 0.9960 - val_loss: 0.0227 - val_acc: 0.9942 Epoch 18/20 50s - loss: 0.0145 - acc: 0.9961 - val_loss: 0.0176 - val_acc: 0.9951 Epoch 19/20 50s - loss: 0.0124 - acc: 0.9963 - val_loss: 0.0284 - val_acc: 0.9936 Epoch 20/20 50s - loss: 0.0115 - acc: 0.9969 - val_loss: 0.0210 - val_acc: 0.9946 CPU times: user 17min 4s, sys: 1min 56s, total: 19min Wall time: 16min 53s Result The final validation accuracy is 0.995 (the validation error is 0.540%). The data augmentation approach improved the validation accuracy by 0.0029. As the validation data contains 10K images, this means that additional 29 (=10000x0.0029) images are correctly classified. According to Recent Hinton's paper , this model could return the validation error of 0.39%. My final model is not as good. I probablly need to tweek the backpropagation parameters and dropout rate. Some comments on algorithm's termination criteria In this excercise, I did not formaly split the data into train, validation and test sets. Instead, I treated testing data also as a validation data. This is OK if testing data is not used for deciding the model parameter updates. However, it is NOT OK if the testing data is used for this purpose, e.g., deciding when to terminate the algorithm. I selected the model updated at the final epoch as the final model; my algorithm termination criteria is simply \"stop at 50th epoch\". This simple approach may cause issues, because the model may be overfitted or underfitted at the pre-specified epoch size. It is wiser to decide the final model by introducing validation set and considering the early stopping when the validation loss is minimized. In [23]: m1_err = hist1 . history [ \"val_acc\" ][ - 1 ] m2_err = hist2 . history [ \"val_acc\" ][ - 1 ] for label in [ \"val_acc\" , \"acc\" ]: plt . plot ( hist1 . history [ label ], \"--\" , label = \"model1 \" + label ) plt . plot ( hist2 . history [ label ], label = \"model2 \" + label ) plt . xlabel ( \"epoch\" ) plt . ylabel ( \"accuracy\" ) plt . title ( \"validation accuracy, model1: {:6.5f}, model2: {:6.5f}\" . format ( m1_err , m2_err )) plt . legend () plt . ylim ( 0.95 , 1.005 ) plt . show () print ( \"validation error: \\n model1: {:5.3f}%, model2: {:5.3f}%\" . format ( ( 1 - m1_err ) * 100 ,( 1 - m2_err ) * 100 )) print ( \"validation accuracy improvement: {}\" . format ( m2_err - m1_err )) validation error: model1: 0.830%, model2: 0.540% validation accuracy improvement: 0.0029 Save model In [10]: from keras.models import model_from_json def save_model ( model , name ): ''' save model architecture and model weights ''' json_string = model . to_json () open ( name + '_architecture.json' , 'w' ) . write ( json_string ) model . save_weights ( name + '_weights.h5' ) save_model ( model2 , name = \"standard\" ) Due to the large number of parameters (35M), the model weight object is hudge! In [11]: import os file_paths = [ \"standard_weights.h5\" , \"standard_architecture.json\" ] for file_path in file_paths : w = os . stat ( file_path ) . st_size print ( \"{:30} {:4.2f} MB\" . format ( file_path , w / ( 1024.0 ** 2 ))) standard_weights.h5 135.24 MB standard_architecture.json 0.00 MB if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"CNN modeling with image translations using MNIST data"},{"url":"Learn-the-breed-of-a-dog-using-deep-learning.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } My friend asked me if I can figure out the breed of his dog, Loki. As I am not a dog expart, I will ask opinions from deep learning. Here, I use VGG16 trained on ImageNet dataset. What is VGG16 and ImageNet? According to Wikipedia , \"The ImageNet project is a large visual database designed for use in visual object recognition software research...Since 2010, the annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is a competition where research teams evaluate their algorithms on the given data set, and compete to achieve higher accuracy on several visual recognition tasks.\" The good think about this dataset is that it contains 120 categories of dog breeds. So the fine tuned model in this data should be able to classify Loki to the right breed! VGG16 (also called OxfordNet) is a convolutional neural network architecture named after the Visual Geometry Group from Oxford, who developed it. It was used to win the ILSVR (ImageNet) competition in 2014. In [1]: import os import matplotlib.pyplot as plt import numpy as np In [2]: import tensorflow as tf from keras.backend.tensorflow_backend import set_session print ( tf . __version__ ) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"2\" #### 1 GPU1 #### 2 GPU2 #### 0 GPU3 #### 4 GPU4 set_session ( tf . Session ( config = config )) Using TensorFlow backend. 1.2.1 My data folder contains 6 images of Loki. Load them into python and save them as a numpy array. The image is converted into 3 dimentional numpy array by the method \"img_to_array\" In [4]: ls data / IMG-20180108-WA0004.jpg * WP_20150225_15_18_47_Pro.jpg * IMG-20180122-WA0003.jpg * WP_20171222_09_09_22_Pro.jpg * WP_20150225_14_59_57_Pro.jpg * WP_20180120_12_00_15_Pro.jpg * In [5]: from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array pic_nms = os . listdir ( \"data/\" ) fig = plt . figure ( figsize = ( 10 , 10 )) count = 1 myimages = [] for pic_nm in pic_nms : axs = fig . add_subplot ( 3 , len ( pic_nms ) / 2 , count ) image = load_img ( 'data/' + pic_nm , target_size = ( 224 , 224 , 3 )) axs . imshow ( image ) axs . set_title ( count ) image = img_to_array ( image ) myimages . append ( image ) count += 1 plt . show () The picture 1 needs to be rotated by 90 degree. For rotating picture, I use opencv. Nicely, pip was available for installation: pip install opencv - python In [6]: import cv2 print ( cv2 . __version__ ) 3.4.0 In [7]: def rotateImage ( image , angle ): image_center = tuple ( np . array ( image . shape [ 1 :: - 1 ]) / 2 ) rot_mat = cv2 . getRotationMatrix2D ( image_center , angle , 1.0 ) result = cv2 . warpAffine ( image , rot_mat , image . shape [ 1 :: - 1 ], flags = cv2 . INTER_LINEAR ) return result image = rotateImage ( myimages [ 0 ], 270 ) Let's check if the image is correctly rotated. Here we have to be careful. The image array converted from the img_to_array has a data type float which is not accepted by plt.imshow. I need to convert its type to unit8. For this casting, I use: np . unit8 ( image ) Please see the discussion here In [8]: print ( image . dtype ) pimage = np . uint8 ( image ) print ( pimage . dtype ) plt . imshow ( pimage ) plt . show () myimages [ 0 ] = image float32 uint8 In [9]: os . chdir ( \"../FacialKeypoint/\" ) Load VGG16 Images are loaded into python as correct formats. Now, we import the VGG16 model. I assume that the followings are available in the current directory. vgg16_weights_tf_dim_ordering_tf_kernels.h5 imagenet_class_index.json See my past blog post which discusses where they are downloaded. In [10]: from keras.applications import VGG16 model = VGG16 ( include_top = True , weights = None ) ## load the locally saved weights model . load_weights ( \"vgg16_weights_tf_dim_ordering_tf_kernels.h5\" ) Create an array containing all the 1000 class labels of ImageNet In [11]: import json CLASS_INDEX = json . load ( open ( \"imagenet_class_index.json\" )) classlabel = [] for i in range ( 1000 ): classlabel . append ( CLASS_INDEX [ str ( i )][ 1 ]) classlabel = np . array ( classlabel ) print ( len ( classlabel )) 1000 Classification The following script predict the class label for each image. One note: RGB to BGR According to this , there are differences in pixel ordering in various modules in python e.g., OpenCV and Matplotlib. Here is a note: OpenCV: BGR order matplotlib: RGB order Keras's img_to_array: RGB order VGG16: BGR order (because it was created using Caffee which uses OpenCV) So I need to re order my image array before prediction. In [12]: ## top 5 selected classes from copy import copy top = 5 prob_string = [] y_preds = [] for image in myimages : image = copy ( image [:,:,:: - 1 ]) ## change order to BGR y_pred = model . predict ( image . reshape ( 1 , image . shape [ 0 ], image . shape [ 1 ], image . shape [ 2 ])) . flatten () chosen_classes = classlabel [ np . argsort ( y_pred )[:: - 1 ][: top ] ] y_preds . append ( y_pred ) mystrs = \"\" for myclass in chosen_classes : myprob = y_pred [ classlabel == myclass ][ 0 ] mystr = \"{:10} p={:5.3}% \\n \" . format ( myclass , myprob * 100 ) mystrs += \" \" + mystr #print(mystr) prob_string . append ( mystrs ) Plot the images together with the top 5 likely class labels It seems like VGG16 is doing good jobs for classifing Loki except the second image: In all but the second image, Loki is classified into some kind of dog class. Loki is not classified into a reasonable dog class in the second picture, probablly because the ImageNet data does not have many data with a dog surrounded by this much snow. In [13]: fig = plt . figure ( figsize = ( 15 , 15 )) count = 1 for image , pstr in zip ( myimages , prob_string ): axs = fig . add_subplot ( 3 , len ( myimages ) / 2 , count ) axs . imshow ( np . uint8 ( image )) axs . set_title ( count ) axs . text ( 80 , 220 , pstr , bbox = { 'facecolor' : 'white' , 'alpha' : 0.95 , 'pad' : 10 }) axs . set_xticks ([]) axs . set_yticks ([]) count += 1 plt . show () So what is the Loki's breed? The predicted classes are somewhat different across images. To give an overall answer, I will aggregate the probability estimates obtained from all pictures. In [14]: y_preds = np . array ( y_preds ) y_pred_all = np . mean ( y_preds , axis = 0 ) print ( \"Make sure that the probability sums up to 1: {}\" . format ( np . sum ( y_pred_all ))) Make sure that the probability sums up to 1: 1.0 Plot below shows the top 30 overall probabilities for Loki's class. It seems that Loki is most likely black and tan coonhound. Loki may be a weevil with 10% chance. In [15]: top = 30 yorder = np . argsort ( y_pred_all )[:: - 1 ] classes_reorder = classlabel [ yorder ][: top ] y_pred_reorder = y_pred_all [ yorder ][: top ] mystrs = \"\" xaxis = range ( len ( y_pred_reorder )) plt . figure ( figsize = ( 10 , 3 )) plt . bar ( xaxis , y_pred_reorder ) plt . xticks ( xaxis , classes_reorder , rotation = 90 ) plt . title ( \"Top {} probailities for Loki's class label\" . format ( top )) plt . show () print ( \"Loki is:\" ) for p , cl in zip ( y_pred_reorder , classes_reorder ): print ( \"{:5.3}% {}\" . format ( p * 100 , cl )) Loki is: 24.8% black-and-tan_coonhound 18.6% Rottweiler 16.0% Doberman 9.74% weevil 6.63% Great_Dane 3.99% kelpie 2.18% Chesapeake_Bay_retriever 1.63% tick 1.4% Weimaraner 1.05% slug 0.917% miniature_pinscher 0.884% lacewing 0.858% bluetick 0.736% Labrador_retriever 0.73% bloodhound 0.714% redbone 0.618% fountain 0.53% curly-coated_retriever 0.514% leafhopper 0.482% schipperke 0.443% cockroach 0.327% German_short-haired_pointer 0.313% Gordon_setter 0.303% flat-coated_retriever 0.282% Airedale 0.278% giant_schnauzer 0.179% German_shepherd 0.172% Greater_Swiss_Mountain_dog 0.165% ant 0.156% ladybug if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Learn the breed of a dog using deep learning"},{"url":"The-first-deep-learning-model-for-NLP-Let-AI-tweet-like-President-Trump.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } The goal of this blog is to learn the functionalities of Keras for language processing applications. In the first section, I create a very simple single-word-in single-word-out model based on a single sentence. With this application, I make sure that the model works in this simple possible scenario and it can correctly predict the next word given the current word correctly for this trainning sentence. In the second section, we will use a similar model with more nodes to more than one sentence. Here I try to create AI that tweets like President Trump. I will use the President Trump's latest ~3,000 tweets to train the model. The data extraction procedure using tweepy is previously discussed . How to Develop Word-Based Neural Language Models in Python with Keras Using pre-trained word embeddings in a Keras model Text Generation With LSTM Recurrent Neural Networks in Python with Keras In [1]: import sys print ( sys . version ) import keras print ( \"keras {}\" . format ( keras . __version__ )) import tensorflow as tf print ( \"tensorflow {}\" . format ( tf . __version__ )) import numpy as np print ( \"numpy {}\" . format ( np . __version__ )) import matplotlib.pyplot as plt 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] Using TensorFlow backend. keras 2.0.6 tensorflow 1.2.1 numpy 1.11.3 In [2]: from keras.backend.tensorflow_backend import set_session print ( tf . __version__ ) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"0\" #### 1 GPU1 #### 2 GPU2 #### 0 GPU3 #### 4 GPU4 set_session ( tf . Session ( config = config )) 1.2.1 Very very simple example: One-word-in one-word-out model The first training data is a single sentence with 11 words followed by three exclamation marks. I will create a simple LSTM model using this single sentence. The model should be able to overfit and reproduce this sentence! In [3]: # source text data = \"\"\"I want my deep learning model to guess this sentence perfectly ... YES!\"\"\" First create Tokenizer object. Tokenizer creates a mapping between word and idnex. The mapping is recorded in dictionary: key = words, value = index. The dictionary can be accessed via \"tokenizer.word_index\". Observation The word_index removes special character e.g., ... or ! All the words are converted to lower case The indexing starts from '1' NOT zero! In [4]: from keras.preprocessing.text import Tokenizer tokenizer = Tokenizer () tokenizer . fit_on_texts ([ data ]) for key , value in tokenizer . word_index . items (): print ( \"key:{:10} value:{:4}\" . format ( key , value )) key:this value: 9 key:guess value: 8 key:want value: 2 key:sentence value: 10 key:i value: 1 key:deep value: 4 key:to value: 7 key:learning value: 5 key:model value: 6 key:perfectly value: 11 key:my value: 3 key:yes value: 12 tokenizer.texts_to_sequences converts string to a list of index. The index is from tokenizer.word_index. You see that the index is in the order of the words appearing in the sentence. In [5]: encoded = tokenizer . texts_to_sequences ([ data ])[ 0 ] ## [0] to extract the first sentence print ( encoded ) print ( \"The sentence has {} words\" . format ( len ( encoded ))) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] The sentence has 12 words Define the vocabulary size to the number of unique words + 1. This vocab_size is used for defining the number of classes to predict. Plus 1 is neccesary to include class \"0\". tokenizer.word_index.values() has no word defined with 0. So we can potentially use this class 0 for a place holder class (i.e., padding class). In [6]: ## + 1 for potential padding vocab_size = len ( tokenizer . word_index ) + 1 print ( \"The sentence has {} unique words\" . format ( len ( np . unique ( tokenizer . word_index . keys ())))) print ( \"> vocab_size={}\" . format ( vocab_size )) The sentence has 12 unique words > vocab_size=13 Prepare training data X, y. As we are creating one-word-in one-word out model, X.shape = (N words in sentence - 1, 1) y.shape = (N words in sentence - 1, 1) In [7]: sequences = [] n_lookback = 1 for i in range ( n_lookback , len ( encoded )): sequences . append ( encoded [( i - n_lookback ): i + 1 ]) # split into X and y elements sequences = np . array ( sequences ) X , y = sequences [:, 0 ], sequences [:, 1 ] print ( X . shape , y . shape ) ((11,), (11,)) One hot encoding for outputs. Notice that num_class is set to vocab_size which is N of unique words + 1. The printing of y shows that there is no row that has index in the 0th and 1st columns. This is because: there is no word belonging to class 0 in our original sentence \"data\". the word indexed with 1 appears at the start of the sentence hence it does not appear in the target y. In [8]: from keras.utils import to_categorical y = to_categorical ( y , num_classes = vocab_size ) print ( y . shape ) print ( y ) (11, 13) [[ 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] Define model and train data The model is simple; one embedding layer followed by one LSTM layer and then feed foward layer. \"Word embeddings\" are a family of natural language processing techniques aiming at mapping semantic meaning of each word into a geometric space. Parameter Embedding layer: for each word, create a continuous vector of length 10 to represent itself 130 parameters = \"vocab_size\" x 10 LSTM layers: 10 hidden units each has 4 gates 840 parameters = 10 hidden LSTM untis 4 (3 gates and 1 state) ((10 input + 1 bias) + 10 hidden LSTM untis) Feed forward layer: 143 parameters = (10 hidden LSTM units + 1 bias) x 13 class In [9]: from keras.models import Model from keras.layers import Input , Dense , Activation , Embedding , LSTM def define_model ( vocab_size , input_length = 1 , dim_dense_embedding = 10 , hidden_unit_LSTM = 5 ): main_input = Input ( shape = ( input_length ,), dtype = 'int32' , name = 'main_input' ) embedding = Embedding ( vocab_size , dim_dense_embedding , input_length = input_length )( main_input ) x = LSTM ( hidden_unit_LSTM )( embedding ) main_output = Dense ( vocab_size , activation = 'softmax' )( x ) model = Model ( inputs = [ main_input ], output = [ main_output ]) print ( model . summary ()) return ( model ) model = define_model ( vocab_size , input_length = 1 , dim_dense_embedding = 10 , hidden_unit_LSTM = 10 ) # compile network model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) # fit network hist = model . fit ( X , y , epochs = 500 , verbose = False ) /home/fairy/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:16: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)` _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= main_input (InputLayer) (None, 1) 0 _________________________________________________________________ embedding_1 (Embedding) (None, 1, 10) 130 _________________________________________________________________ lstm_1 (LSTM) (None, 10) 840 _________________________________________________________________ dense_1 (Dense) (None, 13) 143 ================================================================= Total params: 1,113 Trainable params: 1,113 Non-trainable params: 0 _________________________________________________________________ None The traiing accuracy is perfect, indicating that the model can predict perfectly for training sentence. In [10]: plt . plot ( hist . history [ \"acc\" ]) plt . xlabel ( \"epoch\" ) plt . ylabel ( \"accuracy\" ) plt . show () Now check if my model can correctly generate the trained sentence. Generate a 13 words sentence starting with 'I'. Successfully it generates the original sentence. The original sentence has 12 words so the 13th word predicted after \"yes\" could be anything. In this case the word after yes was predicted to be \"to\". But this value changes if you train with different initial values. In [11]: def predict_sentence ( model , tokenizer , in_text , n_words ): index_word = { v : k for k , v in tokenizer . word_index . items ()} encoded = tokenizer . texts_to_sequences ([ in_text ])[ 0 ] encoded = np . array ( encoded ) words = [ in_text ] for _ in range ( n_words - 1 ): ## encoded is index probs = model . predict ( encoded , verbose = 0 ) encoded = np . argmax ( probs , axis = 1 )[ 0 ] word = index_word [ encoded ] encoded = np . array ([ encoded ]) words . append ( word ) return ( words ) n_words = 13 in_text = 'I' pred_sentence = predict_sentence ( model , tokenizer , in_text , n_words ) print ( \"Predicted sentence with {} words:\" . format ( n_words )) for k in pred_sentence : print ( k ), Predicted sentence with 13 words: I want my deep learning model to guess this sentence perfectly yes to Look at the probability distribution of the word given the previous word. In [12]: # words is a list of vocabularly words such that words[word_index.values()[i]] = word_index.keys()[i] words = tokenizer . word_index . keys () words = np . array ( words ) words = list ( words [ np . argsort ( tokenizer . word_index . values ())]) words = [ \"padding\" ] + words print ( words ) ['padding', 'i', 'want', 'my', 'deep', 'learning', 'model', 'to', 'guess', 'this', 'sentence', 'perfectly', 'yes'] The extimated probability distribution of all words except 'yes' have large peak and flat elsewhere. The peak is located at the next word. For example, the peak of the probability distribution of the word after 'deep' is at 'learning'. However, the probability distribution of the word after 'yes' is rather flat. In [13]: choice_words = [ \"yes\" , 'deep' , 'my' ] for choice_word in choice_words : encoded = tokenizer . texts_to_sequences ([ choice_word ])[ 0 ] encoded = np . array ([ encoded ]) probs = model . predict ( encoded ) . flatten () y_pos = range ( len ( probs )) plt . figure ( figsize = ( 14 , 1 )) plt . bar ( y_pos , probs ) plt . xticks ( y_pos , words ) plt . title ( \"The probability distribution: word after '{}'\" . format ( choice_word )) plt . show () Train a NLP model with President Trump's Tweets Now let's move on to more complex application. In the previous example, we only have a single sentence to train the model. I will now use ~3,000 tweets from President Trump to train a deep learning model. In the previous post , I presented how to extract ~3,000 President trump's latest tweets using tweepy. I use this data to create a simple deep learning model that tweets like he does! In [14]: import pandas as pd data = pd . read_csv ( \"data/realDonaldTrump_tweets.csv\" ) data . head ( 5 ) Out[14]: id created_at favorite_count retweet_count text 0 952540700683497472 2018-01-14 13:59:35 63773 14402 ...big unnecessary regulation cuts made it all... 1 952538350333939713 2018-01-14 13:50:14 81577 18816 \"President Trump is not getting the credit he ... 2 952530515894169601 2018-01-14 13:19:06 112532 28970 I, as President, want people coming into our C... 3 952528011869478912 2018-01-14 13:09:09 91606 21864 DACA is probably dead because the Democrats do... 4 952526145064505345 2018-01-14 13:01:44 66552 13551 ...and they knew exactly what I said and meant... Let's look at randomly selected 10 tweets in my dataframe. It shows that the tweets contains lots of terms that only appears single times or the terms that are not interesting for predictions. So I will first clean the texts. In [15]: random_index = np . random . choice ( data . shape [ 0 ], 10 ) for index , k in enumerate ( data [ \"text\" ] . iloc [ random_index ]): print ( \"\" ) print ( \"irow={}\" . format ( random_index [ index ])) print ( k ) irow=1846 Getting ready to meet President al-Sisi of Egypt. On behalf of the United States, I look forward to a long and wonderful relationship. irow=2153 Christians in the Middle-East have been executed in large numbers. We cannot allow this horror to continue! irow=962 Thank you, our great honor! https://t.co/StrciEwuWs irow=394 Will be leaving the Philippines tomorrow after many days of constant mtgs & work in order to #MAGA! My promises are rapidly being fulfilled. irow=488 It is finally happening for our great clean coal miners! https://t.co/suAnjs6Ccz irow=611 Great news on the 2018 budget @SenateMajLdr McConnell - first step toward delivering MASSIVE tax cuts for the American people! #TaxReform https://t.co/aBzQR7KR0c irow=1276 My son Donald openly gave his e-mails to the media & authorities whereas Crooked Hillary Clinton deleted (& acid washed) her 33,000 e-mails! irow=246 MAKE AMERICA GREAT AGAIN! irow=457 Getting ready to land in Hawaii. Looking so much forward to meeting with our great Military/Veterans at Pearl Harbor! irow=2238 .@FoxNews \"Outgoing CIA Chief, John Brennan, blasts Pres-Elect Trump on Russia threat. Does not fully understand.\" Oh really, couldn't do... Tweet cleaning stranteges: remove quotes Ideally I want to treat each of '`' and '\"' as a single word. However, I found that Tokenizer is not always treating these as single words. For example, if there is a sentence \"I say so \". \"I is treated as a single word while the ending quote \" is treated as a single word. For simplicity, I will ignore quotes in this blog. remove URL. # and @. Most of these appear only once. Therefore including URL decreases the model performance on valdiation set substantially. In [16]: import re texts = [] for text in data [ \"text\" ] . values : text = text . replace ( '\"' , \"\" ) text = text . replace ( '\"' , \"\" ) text = text . replace ( '\"' , \"\" ) ## remove link text = re . sub ( r 'https?://.*' , '' , text , flags = re . MULTILINE ) ## remove hashtag text = re . sub ( r '#.*' , '' , text , flags = re . MULTILINE ) ## remove text = re . sub ( r '@.*' , '' , text , flags = re . MULTILINE ) texts . append ( text ) data [ \"text\" ] = texts I found that these cleaning is very important to create a meaningful model. Without the cleaning, the model's training accuracy did not increase more than 0.05. I tried to solve this problem by substantially increasing the complexity of the model but I was not very successful. It seems that removing infrequently appearing words is very useful approach. This makes sense because the removals of these infrequently appearing words reduce the size of Tokenizer.word_index by more than 20% times (1 - 5689/7300). Now, we create a mapping between a word and an index. Tokenizer nicely filters special characters. In [17]: tokenizer = Tokenizer () tokenizer . fit_on_texts ( data [ \"text\" ] . values ) vocab_size = len ( tokenizer . word_index ) + 1 index_word = { v : k for k , v in tokenizer . word_index . items ()} print ( \"The sentence has {} unique words\" . format ( len ( np . unique ( tokenizer . word_index . keys ())))) print ( \"> vocab_size={}\" . format ( vocab_size )) The sentence has 5688 unique words > vocab_size=5689 Using the Tokenizer's word index dictionary, represents each sentence just by the word indecies. Let's see how the sentence is represented with word indecies. In [18]: def print_text ( ks ): for k in ks : print ( \"{}({})\" . format ( index_word [ k ], k )), print ( \"\" ) for irow , line in enumerate ( data [ \"text\" ] . iloc [ random_index ]): encoded = tokenizer . texts_to_sequences ([ line ])[ 0 ] print ( \"irow={}\" . format ( random_index [ irow ])) print_text ( encoded ) print ( \"\" ) irow=1846 getting(182) ready(321) to(2) meet(561) president(54) al(1042) sisi(2117) of(4) egypt(1158) on(12) behalf(489) of(4) the(1) united(95) states(91) i(10) look(180) forward(236) to(2) a(6) long(159) and(3) wonderful(171) relationship(674) irow=2153 christians(5059) in(5) the(1) middle(348) east(595) have(22) been(87) executed(5060) in(5) large(741) numbers(282) we(15) cannot(469) allow(568) this(28) horror(1582) to(2) continue(456) irow=962 thank(30) you(20) our(14) great(11) honor(106) irow=394 will(9) be(13) leaving(359) the(1) philippines(1590) tomorrow(150) after(97) many(62) days(305) of(4) constant(3653) mtgs(2491) amp(18) work(177) in(5) order(184) to(2) irow=488 it(21) is(7) finally(355) happening(501) for(8) our(14) great(11) clean(1941) coal(1609) miners(2532) irow=611 great(11) news(36) on(12) the(1) 2018(676) budget(589) irow=1276 my(26) son(666) donald(319) openly(2811) gave(364) his(109) e(565) mails(935) to(2) the(1) media(69) amp(18) authorities(1828) whereas(4405) crooked(141) hillary(77) clinton(81) deleted(826) amp(18) acid(2393) washed(2394) her(272) 33(762) 000(157) e(565) mails(935) irow=246 make(66) america(42) great(11) again(63) irow=457 getting(182) ready(321) to(2) land(1447) in(5) hawaii(2517) looking(181) so(34) much(72) forward(236) to(2) meeting(151) with(16) our(14) great(11) military(117) veterans(432) at(23) pearl(1139) harbor(1330) irow=2238 Restructure the sentence data Currently each row is a sentence We will change it so that every row corresponds to a single word for prediction For example, if there are two sentences \"Make America Great Again\" and \"Thanks United States\" this will create 5 rows such that: \"- - Make America\", \"- Make America Great\", \"Make America Great Again\", \"- - Thanks United\", \"- Thanks United States\". Devide the sentences into taining and testing dataseat. I make sure that any sub sentences from the same single original sentence go to the same dataset. In [19]: N = data . shape [ 0 ] prop_train = 0.8 Ntrain = int ( N * prop_train ) Ntest = N - Ntrain sequences , index_train , index_test = [], [], [] count = 0 for irow , line in enumerate ( data [ \"text\" ]): encoded = tokenizer . texts_to_sequences ([ line ])[ 0 ] for i in range ( 1 , len ( encoded )): sequence = encoded [: i + 1 ] sequences . append ( sequence ) if irow < Ntrain : index_train . append ( count ) else : index_test . append ( count ) count += 1 print ( 'Total Sequences: %d ' % ( len ( sequences ))) Total Sequences: 50854 The sequence lengths differ in the data. We add \"0\" to fill make each sentence the same. See pad_sequence . Transform the target variables into one-hot encoded vectors. In [20]: from keras.preprocessing.sequence import pad_sequences max_length = max ([ len ( seq ) for seq in sequences ]) sequences = pad_sequences ( sequences , maxlen = max_length , padding = 'pre' ) print ( 'Max Sequence Length: %d ' % max_length ) # split into input and output elements sequences = np . array ( sequences ) X , y = sequences [:,: - 1 ], sequences [:, - 1 ] y = to_categorical ( y , num_classes = vocab_size ) X_train , y_train , X_test , y_test = X [ index_train ], y [ index_train ], X [ index_test ], y [ index_test ] Max Sequence Length: 55 Model training starts here! I made the model more complex than the previous example by increasing the dimention of the dense embedding vector, and increasing the number of hidden units in LSTM. The training accuracy keeps increasing but the validation accuracies do not increase as much. This is reasonable considering the small training data size; the model is overfitting. In [21]: model = define_model ( vocab_size , input_length = X . shape [ 1 ], dim_dense_embedding = 30 , hidden_unit_LSTM = 64 ) # compile network model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) # fit network hist = model . fit ( X_train , y_train , validation_data = ( X_test , y_test ), epochs = 50 , verbose = 2 , batch_size = 128 ) /home/fairy/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:16: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)` _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= main_input (InputLayer) (None, 54) 0 _________________________________________________________________ embedding_2 (Embedding) (None, 54, 30) 170670 _________________________________________________________________ lstm_2 (LSTM) (None, 64) 24320 _________________________________________________________________ dense_2 (Dense) (None, 5689) 369785 ================================================================= Total params: 564,775 Trainable params: 564,775 Non-trainable params: 0 _________________________________________________________________ None Train on 42933 samples, validate on 7921 samples Epoch 1/50 42s - loss: 7.0381 - acc: 0.0457 - val_loss: 7.1778 - val_acc: 0.0376 Epoch 2/50 43s - loss: 6.7187 - acc: 0.0468 - val_loss: 7.1333 - val_acc: 0.0489 Epoch 3/50 42s - loss: 6.5754 - acc: 0.0561 - val_loss: 7.1526 - val_acc: 0.0518 Epoch 4/50 42s - loss: 6.4653 - acc: 0.0620 - val_loss: 7.1248 - val_acc: 0.0542 Epoch 5/50 42s - loss: 6.3519 - acc: 0.0712 - val_loss: 7.0637 - val_acc: 0.0636 Epoch 6/50 41s - loss: 6.2376 - acc: 0.0831 - val_loss: 7.0578 - val_acc: 0.0659 Epoch 7/50 42s - loss: 6.1370 - acc: 0.0893 - val_loss: 7.0022 - val_acc: 0.0819 Epoch 8/50 42s - loss: 6.0357 - acc: 0.0985 - val_loss: 6.9901 - val_acc: 0.0889 Epoch 9/50 42s - loss: 5.9305 - acc: 0.1113 - val_loss: 6.9461 - val_acc: 0.0971 Epoch 10/50 42s - loss: 5.8292 - acc: 0.1210 - val_loss: 6.9221 - val_acc: 0.1023 Epoch 11/50 41s - loss: 5.7368 - acc: 0.1259 - val_loss: 6.9087 - val_acc: 0.1050 Epoch 12/50 41s - loss: 5.6531 - acc: 0.1313 - val_loss: 6.8999 - val_acc: 0.1057 Epoch 13/50 41s - loss: 5.5744 - acc: 0.1361 - val_loss: 6.8956 - val_acc: 0.1066 Epoch 14/50 41s - loss: 5.4992 - acc: 0.1419 - val_loss: 6.8832 - val_acc: 0.1117 Epoch 15/50 40s - loss: 5.4262 - acc: 0.1450 - val_loss: 6.8832 - val_acc: 0.1146 Epoch 16/50 41s - loss: 5.3554 - acc: 0.1498 - val_loss: 6.8813 - val_acc: 0.1197 Epoch 17/50 42s - loss: 5.2873 - acc: 0.1535 - val_loss: 6.8838 - val_acc: 0.1179 Epoch 18/50 41s - loss: 5.2193 - acc: 0.1569 - val_loss: 6.8880 - val_acc: 0.1199 Epoch 19/50 41s - loss: 5.1511 - acc: 0.1615 - val_loss: 6.8848 - val_acc: 0.1225 Epoch 20/50 41s - loss: 5.0858 - acc: 0.1661 - val_loss: 6.8957 - val_acc: 0.1240 Epoch 21/50 41s - loss: 5.0204 - acc: 0.1709 - val_loss: 6.9047 - val_acc: 0.1278 Epoch 22/50 41s - loss: 4.9554 - acc: 0.1759 - val_loss: 6.9067 - val_acc: 0.1260 Epoch 23/50 40s - loss: 4.8918 - acc: 0.1811 - val_loss: 6.9166 - val_acc: 0.1283 Epoch 24/50 40s - loss: 4.8282 - acc: 0.1861 - val_loss: 6.9308 - val_acc: 0.1299 Epoch 25/50 40s - loss: 4.7659 - acc: 0.1903 - val_loss: 6.9423 - val_acc: 0.1321 Epoch 26/50 39s - loss: 4.7034 - acc: 0.1951 - val_loss: 6.9518 - val_acc: 0.1327 Epoch 27/50 38s - loss: 4.6431 - acc: 0.1998 - val_loss: 6.9729 - val_acc: 0.1312 Epoch 28/50 38s - loss: 4.5823 - acc: 0.2061 - val_loss: 6.9828 - val_acc: 0.1324 Epoch 29/50 38s - loss: 4.5236 - acc: 0.2094 - val_loss: 7.0053 - val_acc: 0.1329 Epoch 30/50 37s - loss: 4.4663 - acc: 0.2153 - val_loss: 7.0225 - val_acc: 0.1328 Epoch 31/50 38s - loss: 4.4097 - acc: 0.2198 - val_loss: 7.0357 - val_acc: 0.1357 Epoch 32/50 40s - loss: 4.3553 - acc: 0.2236 - val_loss: 7.0553 - val_acc: 0.1348 Epoch 33/50 39s - loss: 4.3003 - acc: 0.2296 - val_loss: 7.0785 - val_acc: 0.1360 Epoch 34/50 40s - loss: 4.2490 - acc: 0.2343 - val_loss: 7.0932 - val_acc: 0.1396 Epoch 35/50 40s - loss: 4.1977 - acc: 0.2382 - val_loss: 7.1158 - val_acc: 0.1384 Epoch 36/50 39s - loss: 4.1474 - acc: 0.2444 - val_loss: 7.1318 - val_acc: 0.1385 Epoch 37/50 40s - loss: 4.0999 - acc: 0.2499 - val_loss: 7.1532 - val_acc: 0.1381 Epoch 38/50 40s - loss: 4.0533 - acc: 0.2543 - val_loss: 7.1659 - val_acc: 0.1409 Epoch 39/50 40s - loss: 4.0083 - acc: 0.2604 - val_loss: 7.1852 - val_acc: 0.1389 Epoch 40/50 37s - loss: 3.9632 - acc: 0.2663 - val_loss: 7.2059 - val_acc: 0.1393 Epoch 41/50 41s - loss: 3.9196 - acc: 0.2719 - val_loss: 7.2269 - val_acc: 0.1386 Epoch 42/50 40s - loss: 3.8774 - acc: 0.2764 - val_loss: 7.2385 - val_acc: 0.1386 Epoch 43/50 41s - loss: 3.8377 - acc: 0.2825 - val_loss: 7.2597 - val_acc: 0.1408 Epoch 44/50 41s - loss: 3.7978 - acc: 0.2874 - val_loss: 7.2728 - val_acc: 0.1391 Epoch 45/50 41s - loss: 3.7604 - acc: 0.2923 - val_loss: 7.2909 - val_acc: 0.1372 Epoch 46/50 41s - loss: 3.7229 - acc: 0.2977 - val_loss: 7.3075 - val_acc: 0.1387 Epoch 47/50 41s - loss: 3.6861 - acc: 0.3024 - val_loss: 7.3216 - val_acc: 0.1403 Epoch 48/50 39s - loss: 3.6515 - acc: 0.3062 - val_loss: 7.3379 - val_acc: 0.1384 Epoch 49/50 40s - loss: 3.6179 - acc: 0.3112 - val_loss: 7.3497 - val_acc: 0.1401 Epoch 50/50 42s - loss: 3.5841 - acc: 0.3150 - val_loss: 7.3721 - val_acc: 0.1372 Plot validation accuracy and training accuracy In [34]: plt . figure ( figsize = ( 8 , 8 )) plt . plot ( hist . history [ \"val_acc\" ], label = \"val_acc\" ) plt . plot ( hist . history [ \"acc\" ], label = \"acc\" ) plt . legend () plt . show () Check the dimentions of available weights In [22]: count_layer = 0 for layer in model . layers : ws = layer . get_weights () c = 0 for w in ws : print ( \"layer{}_{} {:20} {}\" . format ( count_layer , c , layer . name , w . shape )) c += 1 count_layer += 1 layer1_0 embedding_2 (5689, 30) layer2_0 lstm_2 (30, 256) layer2_1 lstm_2 (64, 256) layer2_2 lstm_2 (256,) layer3_0 dense_2 (64, 5689) layer3_1 dense_2 (5689,) Reduce the dimention of the word vectors using PCA and visualize their distirubtions in 2d. In [23]: weight = model . layers [ 1 ] . get_weights ()[ 0 ] print ( weight . shape ) from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) y_embed_pca = pca . fit_transform ( weight ) print ( y_embed_pca . shape ) (5689, 30) (5689, 2) Observations Seemingly similar words are clustered in the same area: kim, saudi, radical, differently north, south estimates, statistically knives, dialogs independent, united thank, honor In [27]: fig , ax = plt . subplots ( figsize = ( 25 , 25 )) ax . scatter ( y_embed_pca [:, 0 ], y_embed_pca [:, 1 ], c = \"white\" ) for txt , irow in tokenizer . word_index . items (): try : ax . annotate ( txt , ( y_embed_pca [ irow , 0 ], y_embed_pca [ irow , 1 ])) except : pass ax . set_xlabel ( \"pca embedding 1\" ) ax . set_ylabel ( \"pca embedding 2\" ) plt . show () Let President Trump's AI talk about some topics I feed the first few words, and let's see what opnion of President Trump's AI is!! I randomly sample the next word according to the estimated probability distribution. This way, I can ensure that the outputs are different every time I feed the same initial words. Hummm the predicted tweets makes sense, kind of? Observations When \"Make America\" is provided as the first 2 words, the AI almost always predicts \"great again\" as the next words. When \"North\" is provided, the next word is almost always \"Korea\" followed often by some negative sentence. The sentence starting with \"Omaga is\" tends to have negative meaning. In [37]: def sample ( probs ): return ( np . random . choice ( range ( len ( probs )), p = probs )) def predict_sentence ( in_text , n_words , tokenizer , model , max_length ): words = [] for _ in range ( n_words ): # encode the text as integer enc = tokenizer . texts_to_sequences ([ in_text ])[ 0 ] # pre-pad sequences to a fixed length enc_padding = pad_sequences ([ enc ], maxlen = max_length - 1 , padding = 'pre' ) probs = model . predict ( enc_padding , verbose = 0 ) . flatten () index = sample ( probs ) word = index_word [ index ] in_text += ' ' + word return ( in_text ) print ( predict_sentence ( \"North\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"America\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"I'm\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"I won't\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"MAKE AMERICA\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"american jobs\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"Obama is\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"Universal healthcare\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"H1B visa\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"Women\" , n_words , tokenizer , model , max_length )) print ( predict_sentence ( \"fat\" , n_words , tokenizer , model , max_length )) North korea is so out with my the story on crooked hillary investigation as America is time blames came played many agencies before he voted out this is I'm so is dead they got china failing big crowd amp yates of historic I won't been would be a great nation the u s made what a grateful MAKE AMERICA great again to have forced to be a last legs in alabama uttered american jobs growth and now elections great healthcare with mine that is many fake news Obama is also way china media about representatives if it is a great days or Universal healthcare and now we will the u s demand manchester should do real james H1B visa person jobs in louvre powell to stand for what on the roosevelt room Women individuals our great healthcare amp tax cuts is approved hopefully terminate bonuses in fat other in china like killed presidential support that the rulers of she which Next Steps Try transfer learning Increase more tweets Prediction of the number of retweets/likes given tweets if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"The first deep learning model for NLP - Let AI tweet like President Trump -"},{"url":"extract-someones-tweet-using-tweepy.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } This blog post is to remind myself the simple useage of the tweepy. I will extract someone's past tweets using tweepy and create .csv file that can be used to train machine learning models. I created the scripts by referencing the following seminal blog posts: api.user_timeline not grabbing full tweet yanofsky/tweet_dumper.py tweepy Importing necessary python scripts. In [28]: ## credentials contain: # customer_key = \"XXX\" # customer_secret = \"XXX\" # access_token = \"XXX\" # access_token_secret = \"XXX\" from credentials import * import tweepy print ( tweepy . __version__ ) 3.5.0 Select the userID. In [2]: userID = \"realDonaldTrump\" Step 1: extract the latest 200 tweets using api.user_timeline In [31]: # Authorize our Twitter credentials auth = tweepy . OAuthHandler ( consumer_key , consumer_secret ) auth . set_access_token ( access_token , access_token_secret ) api = tweepy . API ( auth ) tweets = api . user_timeline ( screen_name = userID , # 200 is the maximum allowed count count = 200 , include_rts = False , # Necessary to keep full_text # otherwise only the first 140 words are extracted tweet_mode = 'extended' ) Show the extracted 3 latest tweets info.id is larger for the later tweets In [32]: for info in tweets [: 3 ]: print ( \"ID: {}\" . format ( info . id )) print ( info . created_at ) print ( info . full_text ) print ( \" \\n \" ) ID: 952540700683497472 2018-01-14 13:59:35 ...big unnecessary regulation cuts made it all possible\" (among many other things). \"President Trump reversed the policies of President Obama, and reversed our economic decline.\" Thank you Stuart Varney. @foxandfriends ID: 952538350333939713 2018-01-14 13:50:14 \"President Trump is not getting the credit he deserves for the economy. Tax Cut bonuses to more than 2,000,000 workers. Most explosive Stock Market rally that we've seen in modern times. 18,000 to 26,000 from Election, and grounded in profitability and growth. All Trump, not 0... ID: 952530515894169601 2018-01-14 13:19:06 I, as President, want people coming into our Country who are going to help us become strong and great again, people coming in through a system based on MERIT. No more Lotteries! #AMERICA FIRST Step 2: Extract as many past tweets as possible. I was able to extract In [33]: all_tweets = [] all_tweets . extend ( tweets ) oldest_id = tweets [ - 1 ] . id while True : tweets = api . user_timeline ( screen_name = userID , # 200 is the maximum allowed count count = 200 , include_rts = False , max_id = oldest_id - 1 , # Necessary to keep full_text # otherwise only the first 140 words are extracted tweet_mode = 'extended' ) if len ( tweets ) == 0 : break oldest_id = tweets [ - 1 ] . id all_tweets . extend ( tweets ) print ( 'N of tweets downloaded till now {}' . format ( len ( all_tweets ))) N of tweets downloaded till now 357 N of tweets downloaded till now 548 N of tweets downloaded till now 728 N of tweets downloaded till now 889 N of tweets downloaded till now 1043 N of tweets downloaded till now 1194 N of tweets downloaded till now 1381 N of tweets downloaded till now 1555 N of tweets downloaded till now 1732 N of tweets downloaded till now 1918 N of tweets downloaded till now 2107 N of tweets downloaded till now 2302 N of tweets downloaded till now 2489 N of tweets downloaded till now 2677 N of tweets downloaded till now 2863 N of tweets downloaded till now 2893 The total N of tweets: 2893 Step 3: Save the tweets into csv In [18]: tweet . full_text . encode ( \"utf-8\" ) --------------------------------------------------------------------------- NameError Traceback (most recent call last) in () ----> 1 tweet . full_text . encode ( \"utf-8\" ) NameError : name 'tweet' is not defined In [34]: #transform the tweepy tweets into a 2D array that will populate the csv from pandas import DataFrame outtweets = [[ tweet . id_str , tweet . created_at , tweet . favorite_count , tweet . retweet_count , tweet . full_text . encode ( \"utf-8\" ) . decode ( \"utf-8\" )] for idx , tweet in enumerate ( all_tweets )] df = DataFrame ( outtweets , columns = [ \"id\" , \"created_at\" , \"favorite_count\" , \"retweet_count\" , \"text\" ]) df . to_csv ( ' %s _tweets.csv' % userID , index = False ) df . head ( 3 ) Out[34]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } id created_at favorite_count retweet_count text 0 952540700683497472 2018-01-14 13:59:35 64325 14528 ...big unnecessary regulation cuts made it all... 1 952538350333939713 2018-01-14 13:50:14 82267 18998 \"President Trump is not getting the credit he ... 2 952530515894169601 2018-01-14 13:19:06 113506 29228 I, as President, want people coming into our C... The data is saved at current working directory as: In [23]: ls *. csv realDonaldTrump_tweets.csv In [24]: cat *. csv | head - 4 ,id,created_at,favorite_count,retweet_count,text 0,952540700683497472,2018-01-14 13:59:35,63773,14402,\"...big unnecessary regulation cuts made it all possible\" (among many other things). \"President Trump reversed the policies of President Obama, and reversed our economic decline.\" Thank you Stuart Varney. @foxandfriends\" 1,952538350333939713,2018-01-14 13:50:14,81577,18816,\"\"President Trump is not getting the credit he deserves for the economy. Tax Cut bonuses to more than 2,000,000 workers. Most explosive Stock Market rally that we've seen in modern times. 18,000 to 26,000 from Election, and grounded in profitability and growth. All Trump, not 0...\" 2,952530515894169601,2018-01-14 13:19:06,112532,28970,\"I, as President, want people coming into our Country who are going to help us become strong and great again, people coming in through a system based on MERIT. No more Lotteries! #AMERICA FIRST\" cat: stdout: Broken pipe Preliminary analysis of President Trump's tweets Let's look at how the favorite counts and retweet counts change over time. There are some extraordinary popular tweets. It shows that we extracted the tweets since 2016-10. In [25]: import matplotlib.pyplot as plt ylabels = [ \"favorite_count\" , \"retweet_count\" ] fig = plt . figure ( figsize = ( 13 , 3 )) fig . subplots_adjust ( hspace = 0.01 , wspace = 0.01 ) n_row = len ( ylabels ) n_col = 1 for count , ylabel in enumerate ( ylabels ): ax = fig . add_subplot ( n_row , n_col , count + 1 ) ax . plot ( df [ \"created_at\" ], df [ ylabel ]) ax . set_ylabel ( ylabel ) plt . show () Let's look at the actual most popular tweets. Here, most popular tweets are defined as favorite_count > 400,000 and retweet_count > 200,000. The 1st peak: The tweets that President Trump made when he was selected to President. The 2nd peak: President Trump's response to CNN. This tweet includes a youtube video where President Trump body slams a man whose face is covered with the text \"CNN\". The 3rd peak: President Trump's response to Kim Jong-un. In [26]: df_sub = df . loc [( df [ \"favorite_count\" ] > 400000 ) & ( df [ \"retweet_count\" ] > 200000 ),:] for irow in range ( df_sub . shape [ 0 ]): df_row = df_sub . iloc [ irow ,:] print ( df_row [ \"created_at\" ]) print ( \"favorite_count={:6} retweet_count={:6}\" . format ( df_row [ \"favorite_count\" ], df_row [ \"retweet_count\" ])) print ( df_row [ \"text\" ]) print ( \" \\n \" ) 2017-11-12 00:48:01 favorite_count=617512 retweet_count=271595 Why would Kim Jong-un insult me by calling me \"old,\" when I would NEVER call him \"short and fat?\" Oh well, I try so hard to be his friend - and maybe someday that will happen! 2017-07-02 13:21:42 favorite_count=586558 retweet_count=361672 #FraudNewsCNN #FNN https://t.co/WYUnHjjUjg 2016-11-09 11:36:58 favorite_count=613262 retweet_count=211250 Such a beautiful and important evening! The forgotten man and woman will never be forgotten again. We will all come together as never before 2016-11-08 11:43:14 favorite_count=557330 retweet_count=331050 TODAY WE MAKE AMERICA GREAT AGAIN! if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Extract someone's tweet using tweepy"},{"url":"Visualization of Filters with Keras.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } The goal of this blog post is to understand \"what my CNN model is looking at\". People call this visualization of the filters. But more precisely, what I will do here is to visualize the input images that maximizes (sum of the) activation map (or feature map) of the filters. I will visualize the filters of deep learning models for two different applications: Facial landmark detection Classification For the facial landmark detection, I will visualize the filters of the model that was trained and described in my previous post Achieving Top 23% in Kaggle's Facial Keypoints Detection with Keras + Tensorflow . For the classification, I will use the VGG16. Once again, I will follow the two great blog posts: Shinya's Kerasã§å­¦ã¶è»¢ç§»å­¦ç¿’ and Keras's official blog . In [3]: import os import matplotlib.pyplot as plt import numpy as np from pandas.io.parsers import read_csv from sklearn.utils import shuffle ## These files must be downloaded from Keras website and saved under data folder Use a single GPU In [4]: import tensorflow as tf from keras.backend.tensorflow_backend import set_session print ( tf . __version__ ) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"2\" #### 1 GPU1 #### 2 GPU2 #### 0 GPU3 #### 4 GPU4 set_session ( tf . Session ( config = config )) 1.2.1 Load the previously trained model Model 4 was the best among all considered single models in previous analysis. I will load Model 4. Create alias \"input_img\". This is the 96 pixcel x 96 pixcel image input for the deep learning model. \"layer_names\" is a list of the names of layers to visualize. \"layer_dict\" contains model layers model.summary() shows the deep learning architecture. In [5]: from keras.models import model_from_json def load_model ( name ): model = model_from_json ( open ( name + '_architecture.json' ) . read ()) model . load_weights ( name + '_weights.h5' ) return ( model ) model = load_model ( \"model4\" ) model . summary () input_img = model . layers [ 0 ] . input layer_names = [ \"conv2d_22\" , \"conv2d_23\" , \"conv2d_24\" ] layer_dict = dict ([( layer . name , layer ) for layer in model . layers ]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_22 (Conv2D) (None, 94, 94, 32) 320 _________________________________________________________________ activation_37 (Activation) (None, 94, 94, 32) 0 _________________________________________________________________ max_pooling2d_22 (MaxPooling (None, 47, 47, 32) 0 _________________________________________________________________ conv2d_23 (Conv2D) (None, 46, 46, 64) 8256 _________________________________________________________________ activation_38 (Activation) (None, 46, 46, 64) 0 _________________________________________________________________ max_pooling2d_23 (MaxPooling (None, 23, 23, 64) 0 _________________________________________________________________ conv2d_24 (Conv2D) (None, 22, 22, 128) 32896 _________________________________________________________________ activation_39 (Activation) (None, 22, 22, 128) 0 _________________________________________________________________ max_pooling2d_24 (MaxPooling (None, 11, 11, 128) 0 _________________________________________________________________ flatten_8 (Flatten) (None, 15488) 0 _________________________________________________________________ dense_31 (Dense) (None, 500) 7744500 _________________________________________________________________ activation_40 (Activation) (None, 500) 0 _________________________________________________________________ dense_32 (Dense) (None, 500) 250500 _________________________________________________________________ activation_41 (Activation) (None, 500) 0 _________________________________________________________________ dense_33 (Dense) (None, 30) 15030 ================================================================= Total params: 8,051,502 Trainable params: 8,051,502 Non-trainable params: 0 _________________________________________________________________ Functions to maximize the activation layer In [121]: import numpy as np from keras import backend as K class VisualizeImageMaximizeFmap ( object ): def __init__ ( self , pic_shape ): ''' pic_shape : a dimention of a single picture e.g., (96,96,1) ''' self . pic_shape = pic_shape def find_n_feature_map ( self , layer_name , max_nfmap ): ''' shows the number of feature maps for this layer only works if the layer is CNN ''' n_fmap = None for layer in model . layers : if layer . name == layer_name : weights = layer . get_weights () n_fmap = weights [ 1 ] . shape [ 0 ] if n_fmap is None : print ( layer_name + \" is not one of the layer names..\" ) n_fmap = 1 n_fmap = np . min ([ max_nfmap , n_fmap ]) return ( int ( n_fmap )) def find_image_maximizing_activation ( self , iterate , input_img_data , picorig = False , n_iter = 30 ): ''' The input image is scaled to range between 0 and 1 picorig : True if the picture image for input is original scale ranging between 0 and 225 False if the picture image for input is ranging [0,1] ''' input_img_data = np . random . random (( 1 , self . pic_shape [ 0 ], self . pic_shape [ 1 ], self . pic_shape [ 2 ])) if picorig : ## if the original picture is unscaled and ranging between (0,225), ## then the image values are centered around 123 with STD=25 input_img_data = input_img_data * 25 + 123 ## I played with this step value but the final image looks to be robust step = 500 # gradient ascent loss_values = [] for i in range ( n_iter ): loss_value , grads_value = iterate ([ input_img_data , 0 ]) input_img_data += grads_value * step loss_values . append ( loss_value ) return ( input_img_data , loss_values ) def create_iterate ( self , input_img , layer_output , filter_index ): ''' layer_output[:,:,:,0] is (Nsample, 94, 94) tensor contains: W0&#94;T [f(image)]_{i,j}], i = 1,..., 94, j = 1,..., 94 layer_output[:,:,:,1] contains: W1&#94;T [f(image)]_{i,j}], i = 1,..., 94, j = 1,..., 94 W0 and W1 are different kernel! ''' ## loss is a scalar if len ( layer_output . shape ) == 4 : ## conv layer loss = K . mean ( layer_output [:, :, :, filter_index ]) elif len ( layer_output . shape ) == 2 : ## fully connected layer loss = K . mean ( layer_output [:, filter_index ]) # calculate the gradient of the loss evaluated at the provided image grads = K . gradients ( loss , input_img )[ 0 ] # normalize the gradients grads /= ( K . sqrt ( K . mean ( K . square ( grads ))) + 1e-5 ) # iterate is a function taking (input_img, scalar) and output [loss_value, gradient_value] iterate = K . function ([ input_img , K . learning_phase ()], [ loss , grads ]) return ( iterate ) def deprocess_image ( self , x ): # standardize to have a mean 0 and std 0.1 x -= x . mean () x /= ( x . std () + 1e-5 ) x *= 0.1 # Shift x to have a mean 0.5 and std 0.1 # This means 95% of the x should be in between 0 and 1 # if x is normal x += 0.5 x = np . clip ( x , 0 , 1 ) # resclar the values to range between 0 and 255 x *= 255 x = np . clip ( x , 0 , 255 ) . astype ( 'uint8' ) return x def find_images ( self , input_img , layer_names , layer_dict , max_nfmap , picorig = False , n_iter = 30 ): ''' Input : input_img : the alias of the input layer from the deep learning model layer_names : list containing the name of the layers whose feature maps to be used layer_dict : symbolic outputs of each \"key\" layer (we gave them unique names). max_nfmap : the maximum number of feature map to be used for each layer. pic_shape : For example pic_shape = (96,96,1) Output : dictionary key = layer name value = a list containing the tuple of (images, list of loss_values) that maximize each feature map ''' argimage = {} ## Look for the image for each feature map of each layer one by one for layer_name in layer_names : ## the layer to visualize n_fmap = self . find_n_feature_map ( layer_name , max_nfmap ) layer_output = layer_dict [ layer_name ] . output result = self . find_images_for_layer ( input_img , layer_output , range ( n_fma ), picorig = picorig , n_iter = n_iter ) argimage [ layer_name ] = result return ( argimage ) def find_images_for_layer ( self , input_img , layer_output , indecies , picorig = False , n_iter = 30 ): ''' indecies : list containing index of --> filtermaps of CNN or --> nodes of fully-connected layer Output a list containing the tuple of (images, list of loss_values) that maximize each feature map ''' result_temp = [] for filter_index in indecies : # filtermap to visualize iterate = self . create_iterate ( input_img , layer_output , filter_index ) input_img_data , loss_values = self . find_image_maximizing_activation ( iterate , input_img , picorig = picorig , n_iter = n_iter ) result_temp . append (( input_img_data , loss_values )) return ( result_temp ) def plot_images_wrapper ( self , argimage , n_row = 8 , scale = 1 ): ''' scale : scale up or down the plot size ''' pic_shape = self . pic_shape if pic_shape [ 2 ] == 1 : pic_shape = self . pic_shape [: 2 ] layer_names = np . sort ( argimage . keys ()) for layer_name in layer_names : n_fmap = len ( argimage [ layer_name ]) n_col = np . ceil ( n_fmap / float ( n_row )) fig = plt . figure ( figsize = ( n_col * scale , n_row * scale )) fig . subplots_adjust ( hspace = 0.001 , wspace = 0.001 ) plt . title ( layer_name + \" n_featuremap=\" + str ( n_fmap )) count = 1 for value in argimage [ layer_name ]: input_img_data = value [ 0 ][ 0 ] img = self . deprocess_image ( input_img_data ) ax = fig . add_subplot ( n_row , n_col , count , xticks = [], yticks = []) ax . imshow ( img . reshape ( * pic_shape ), cmap = \"gray\" ) count += 1 plt . show () For each feature map from each CNN layer, we look for the image that maximizes the sum of the feature maps. Look for the 96 pixcel x 96 pixcel image that maximize: $ \\textrm{argmax}_{image} \\sum_{\\textrm{kernel}} \\boldsymbol{W}&#94;T \\left[ f(image) \\right]_{\\textrm{kernel}} $ $\\boldsymbol{W}$ is a \"filter\", vector of length kernel_size[0] * kernel_size[1]. For example, for conv1, kernel_size=(4,4). $\\sum_{\\textrm{kernel}}$ The sum goes over 94 windows, running over the picture for conv1. max_nfmap determines the number of feature maps from each layer to use for analysis. Observations First layer distinguish colours. Some of the first images seem to have duplicated infomation (same colour). So maybe we can reduce the number of featuremap while keeping the same model performance. The 2nd and the 3rd layer get excited with more complex images. In [8]: max_nfmap = np . Inf ## print ALL the images visualizer = VisualizeImageMaximizeFmap ( pic_shape = ( 96 , 96 , 1 )) print ( \"find images that maximize feature maps\" ) argimage = visualizer . find_images ( input_img , layer_names , layer_dict , max_nfmap ) print ( \"plot them...\" ) visualizer . plot_images_wrapper ( argimage , n_row = 8 , scale = 1 ) find images that maximize feature maps plot them... Did the gradient ascent converge? In Kaggle's official blog , the number of iterations for the gradient ascent is set to as low as 20. Do I really find the best image that maximizes the sum of the feature map? To answer to this question, I plotted the sum of the feature map over iterations. Not suprisingly, the gradient ascent did not converge at all!! I run the codes several times and look at different best images found by the gradient ascent (i.e., run the codes in the previous cell several times). It seems that the best image obtained and plotted above are virtually the same even when the algorithm did not coverge. I guess there are various numerical solutions to this optimization but they are vertually the same image after rescaling. In [9]: const = 1 n_row = 8 for layer_name in layer_names : n_fmap = len ( argimage [ layer_name ]) n_col = np . ceil ( n_fmap / float ( n_row )) fig = plt . figure ( figsize = ( n_col * const , n_row * const )) fig . subplots_adjust ( hspace = 0.001 , wspace = 0.001 ) plt . title ( layer_name + \" n_featuremap=\" + str ( n_fmap )) for count , value in enumerate ( argimage [ layer_name ]): objective = value [ 1 ] ax = fig . add_subplot ( n_row , n_col , count + 1 , xticks = [], yticks = []) ax . plot ( objective ) plt . show () Visualizing VGG16 Let's learn to visualize layers of deep learning model for classification problem. Here I use VGG model following the discussion of Keras's official blog . VGG16 (also called OxfordNet) is a convolutional neural network architecture named after the Visual Geometry Group from Oxford, who developed it. It was used to win the ILSVR (ImageNet) competition in 2014. Due to the proxy problem I cannot download and bulid model using keras.applications.VGG16 so here I take manual approach. Step 1: Downloading data from Github . This is a massive .h5 file (57MB). Step 2: The source code of keras.applications.VGG16 is available. It seems that we can manually set the weights to be the one locally available. In [10]: ls \"vgg16\" * vgg16_weights_tf_dim_ordering_tf_kernels.h5 * vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 * Some observations: Input shape is (None, None, None, 3). This (probablly) means that the sample size and the frame sizes can be specified later. But the channel (RGB) is specified at the 3rd dimention and it must be 3? Keras's official blog sets the 1st dimention for the RGB specification i.e., (None,3,None,None). According to the Keras's documentation about VGG16 , you are allowed to chose the channel to come to the last dimention or the first dimention. But where can I set the dimention? I found my answer in K.image_data_format(). It shows that the default setting is 'channels_last'. Therefore, it makes sense that the VGG16 has 3 as the last dimention. In [11]: import keras.backend as K K . image_data_format () Out[11]: 'channels_last' In [12]: from keras.applications import VGG16 model = VGG16 ( include_top = False , weights = None ) ## load the locally saved weights model . load_weights ( \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\" ) ## show the deep learning model model . summary () input_img = model . layers [ 0 ] . input # get the symbolic outputs of each \"key\" layer (we gave them unique names). layer_dict = dict ([( layer . name , layer ) for layer in model . layers ]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, None, None, 3) 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, None, None, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, None, None, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, None, None, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, None, None, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, None, None, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, None, None, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, None, None, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, None, None, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, None, None, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, None, None, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, None, None, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, None, None, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, None, None, 512) 0 ================================================================= Total params: 14,714,688 Trainable params: 14,714,688 Non-trainable params: 0 _________________________________________________________________ Here the max_nfmap is set to a low number because the number of feature maps is as large as 512 for block5, and this is too many to plot. Notice that although the frame size is set to (96, 96, 3), the frame size can be different: as long as the width and height are larger than 48. Great flexibility of deep learning model :D. This flexilibity is also discussed in the Kaggle's blog . \"Note that we only go up to the last convolutional layer --we don't include fully-connected layers. The reason is that adding the fully connected layers forces you to use a fixed input size for the model (224x224, the original ImageNet format). By only keeping the convolutional modules, our model can be adapted to arbitrary input sizes.\" I observe similar results as Keras's official blog : The first layer encode direction (looks diagonal lines) and color. Later layers mix the direction and colors together. Later layers look more complex and very staticy. In [13]: layer_names = [ \"block1_conv1\" , \"block1_conv2\" , \"block2_conv2\" , \"block3_conv3\" , \"block4_conv3\" , \"block5_conv3\" ] ## (196, 196, 3) , (96,96,3) visualizer = VisualizeImageMaximizeFmap ( pic_shape = ( 48 , 48 , 3 )) max_nfmap = 3 argimage = visualizer . find_images ( input_img , layer_names , layer_dict , max_nfmap ) visualizer . plot_images_wrapper ( argimage , n_row = 1 , scale = 3 ) Use the full VGG16 models Finding an input that maximizes a specific class In the facial landmark detection application, I did not visualize the final output layer. This is because knowing which image can maximize the x (or y) coordinate of the eye center does not give much insite about the model. On the other hand, visualizing the final output layer gives interesting insights in the classification application, because knowing the which image can maximize the probability of being in one of the specific class is interesting. Following Keras's official blog , we will find the images that maximize specific classes. Once again, I download weights from here . This weight is 10 times larger! (528 MB). This makes sense because the weights with top fully connected layer contains 138,357,544 parameters while the weights without the top layers contains 10 times less parameters (14,714,688 parameters). In [20]: ls \"vgg16_weights_tf_dim\" * vgg16_weights_tf_dim_ordering_tf_kernels.h5 * vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 * Notice that the input dimention is set to (None, 224, 224, 3), indicating that our input image needs to have this size In [63]: model = VGG16 ( include_top = True , weights = None ) model . summary () model . load_weights ( \"vgg16_weights_tf_dim_ordering_tf_kernels.h5\" ) input_img = model . layers [ 0 ] . input layer_dict = dict ([( layer . name , layer ) for layer in model . layers ]) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) (None, 224, 224, 3) 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 _________________________________________________________________ flatten (Flatten) (None, 25088) 0 _________________________________________________________________ fc1 (Dense) (None, 4096) 102764544 _________________________________________________________________ fc2 (Dense) (None, 4096) 16781312 _________________________________________________________________ predictions (Dense) (None, 1000) 4097000 ================================================================= Total params: 138,357,544 Trainable params: 138,357,544 Non-trainable params: 0 _________________________________________________________________ It would be interesting to learn what kind of image can maximize the probability of picture having cats or dogs. The visualization becomes more interesting if we know the actual label/meaning of each of the 1000 classes. So before going into the details of the visualization, I extract the labels of each 1000 classes. These classes are saved as a json object at here . This json object is used internally from the method keras.applications.vgg16.decode_predictions() . In [16]: ls imagenet_class_index . json imagenet_class_index.json * In [66]: import json CLASS_INDEX = json . load ( open ( \"imagenet_class_index.json\" )) classlabel = [] for i in range ( 1000 ): classlabel . append ( CLASS_INDEX [ str ( i )][ 1 ]) classlabel = np . array ( classlabel ) print ( len ( classlabel )) 1000 List all 1000 classes in order It is interesting to know that many of the class labels are related to dogs. As Keras's public blog says the 65th class is sea snake class and the 18th magpie. In [67]: for i , lab in enumerate ( classlabel ): print ( \"{:4.0f}: {:30}\" . format ( i , str ( lab ))), if i % 3 == 2 : print ( \"\" ) 0: tench 1: goldfish 2: great_white_shark 3: tiger_shark 4: hammerhead 5: electric_ray 6: stingray 7: cock 8: hen 9: ostrich 10: brambling 11: goldfinch 12: house_finch 13: junco 14: indigo_bunting 15: robin 16: bulbul 17: jay 18: magpie 19: chickadee 20: water_ouzel 21: kite 22: bald_eagle 23: vulture 24: great_grey_owl 25: European_fire_salamander 26: common_newt 27: eft 28: spotted_salamander 29: axolotl 30: bullfrog 31: tree_frog 32: tailed_frog 33: loggerhead 34: leatherback_turtle 35: mud_turtle 36: terrapin 37: box_turtle 38: banded_gecko 39: common_iguana 40: American_chameleon 41: whiptail 42: agama 43: frilled_lizard 44: alligator_lizard 45: Gila_monster 46: green_lizard 47: African_chameleon 48: Komodo_dragon 49: African_crocodile 50: American_alligator 51: triceratops 52: thunder_snake 53: ringneck_snake 54: hognose_snake 55: green_snake 56: king_snake 57: garter_snake 58: water_snake 59: vine_snake 60: night_snake 61: boa_constrictor 62: rock_python 63: Indian_cobra 64: green_mamba 65: sea_snake 66: horned_viper 67: diamondback 68: sidewinder 69: trilobite 70: harvestman 71: scorpion 72: black_and_gold_garden_spider 73: barn_spider 74: garden_spider 75: black_widow 76: tarantula 77: wolf_spider 78: tick 79: centipede 80: black_grouse 81: ptarmigan 82: ruffed_grouse 83: prairie_chicken 84: peacock 85: quail 86: partridge 87: African_grey 88: macaw 89: sulphur-crested_cockatoo 90: lorikeet 91: coucal 92: bee_eater 93: hornbill 94: hummingbird 95: jacamar 96: toucan 97: drake 98: red-breasted_merganser 99: goose 100: black_swan 101: tusker 102: echidna 103: platypus 104: wallaby 105: koala 106: wombat 107: jellyfish 108: sea_anemone 109: brain_coral 110: flatworm 111: nematode 112: conch 113: snail 114: slug 115: sea_slug 116: chiton 117: chambered_nautilus 118: Dungeness_crab 119: rock_crab 120: fiddler_crab 121: king_crab 122: American_lobster 123: spiny_lobster 124: crayfish 125: hermit_crab 126: isopod 127: white_stork 128: black_stork 129: spoonbill 130: flamingo 131: little_blue_heron 132: American_egret 133: bittern 134: crane 135: limpkin 136: European_gallinule 137: American_coot 138: bustard 139: ruddy_turnstone 140: red-backed_sandpiper 141: redshank 142: dowitcher 143: oystercatcher 144: pelican 145: king_penguin 146: albatross 147: grey_whale 148: killer_whale 149: dugong 150: sea_lion 151: Chihuahua 152: Japanese_spaniel 153: Maltese_dog 154: Pekinese 155: Shih-Tzu 156: Blenheim_spaniel 157: papillon 158: toy_terrier 159: Rhodesian_ridgeback 160: Afghan_hound 161: basset 162: beagle 163: bloodhound 164: bluetick 165: black-and-tan_coonhound 166: Walker_hound 167: English_foxhound 168: redbone 169: borzoi 170: Irish_wolfhound 171: Italian_greyhound 172: whippet 173: Ibizan_hound 174: Norwegian_elkhound 175: otterhound 176: Saluki 177: Scottish_deerhound 178: Weimaraner 179: Staffordshire_bullterrier 180: American_Staffordshire_terrier 181: Bedlington_terrier 182: Border_terrier 183: Kerry_blue_terrier 184: Irish_terrier 185: Norfolk_terrier 186: Norwich_terrier 187: Yorkshire_terrier 188: wire-haired_fox_terrier 189: Lakeland_terrier 190: Sealyham_terrier 191: Airedale 192: cairn 193: Australian_terrier 194: Dandie_Dinmont 195: Boston_bull 196: miniature_schnauzer 197: giant_schnauzer 198: standard_schnauzer 199: Scotch_terrier 200: Tibetan_terrier 201: silky_terrier 202: soft-coated_wheaten_terrier 203: West_Highland_white_terrier 204: Lhasa 205: flat-coated_retriever 206: curly-coated_retriever 207: golden_retriever 208: Labrador_retriever 209: Chesapeake_Bay_retriever 210: German_short-haired_pointer 211: vizsla 212: English_setter 213: Irish_setter 214: Gordon_setter 215: Brittany_spaniel 216: clumber 217: English_springer 218: Welsh_springer_spaniel 219: cocker_spaniel 220: Sussex_spaniel 221: Irish_water_spaniel 222: kuvasz 223: schipperke 224: groenendael 225: malinois 226: briard 227: kelpie 228: komondor 229: Old_English_sheepdog 230: Shetland_sheepdog 231: collie 232: Border_collie 233: Bouvier_des_Flandres 234: Rottweiler 235: German_shepherd 236: Doberman 237: miniature_pinscher 238: Greater_Swiss_Mountain_dog 239: Bernese_mountain_dog 240: Appenzeller 241: EntleBucher 242: boxer 243: bull_mastiff 244: Tibetan_mastiff 245: French_bulldog 246: Great_Dane 247: Saint_Bernard 248: Eskimo_dog 249: malamute 250: Siberian_husky 251: dalmatian 252: affenpinscher 253: basenji 254: pug 255: Leonberg 256: Newfoundland 257: Great_Pyrenees 258: Samoyed 259: Pomeranian 260: chow 261: keeshond 262: Brabancon_griffon 263: Pembroke 264: Cardigan 265: toy_poodle 266: miniature_poodle 267: standard_poodle 268: Mexican_hairless 269: timber_wolf 270: white_wolf 271: red_wolf 272: coyote 273: dingo 274: dhole 275: African_hunting_dog 276: hyena 277: red_fox 278: kit_fox 279: Arctic_fox 280: grey_fox 281: tabby 282: tiger_cat 283: Persian_cat 284: Siamese_cat 285: Egyptian_cat 286: cougar 287: lynx 288: leopard 289: snow_leopard 290: jaguar 291: lion 292: tiger 293: cheetah 294: brown_bear 295: American_black_bear 296: ice_bear 297: sloth_bear 298: mongoose 299: meerkat 300: tiger_beetle 301: ladybug 302: ground_beetle 303: long-horned_beetle 304: leaf_beetle 305: dung_beetle 306: rhinoceros_beetle 307: weevil 308: fly 309: bee 310: ant 311: grasshopper 312: cricket 313: walking_stick 314: cockroach 315: mantis 316: cicada 317: leafhopper 318: lacewing 319: dragonfly 320: damselfly 321: admiral 322: ringlet 323: monarch 324: cabbage_butterfly 325: sulphur_butterfly 326: lycaenid 327: starfish 328: sea_urchin 329: sea_cucumber 330: wood_rabbit 331: hare 332: Angora 333: hamster 334: porcupine 335: fox_squirrel 336: marmot 337: beaver 338: guinea_pig 339: sorrel 340: zebra 341: hog 342: wild_boar 343: warthog 344: hippopotamus 345: ox 346: water_buffalo 347: bison 348: ram 349: bighorn 350: ibex 351: hartebeest 352: impala 353: gazelle 354: Arabian_camel 355: llama 356: weasel 357: mink 358: polecat 359: black-footed_ferret 360: otter 361: skunk 362: badger 363: armadillo 364: three-toed_sloth 365: orangutan 366: gorilla 367: chimpanzee 368: gibbon 369: siamang 370: guenon 371: patas 372: baboon 373: macaque 374: langur 375: colobus 376: proboscis_monkey 377: marmoset 378: capuchin 379: howler_monkey 380: titi 381: spider_monkey 382: squirrel_monkey 383: Madagascar_cat 384: indri 385: Indian_elephant 386: African_elephant 387: lesser_panda 388: giant_panda 389: barracouta 390: eel 391: coho 392: rock_beauty 393: anemone_fish 394: sturgeon 395: gar 396: lionfish 397: puffer 398: abacus 399: abaya 400: academic_gown 401: accordion 402: acoustic_guitar 403: aircraft_carrier 404: airliner 405: airship 406: altar 407: ambulance 408: amphibian 409: analog_clock 410: apiary 411: apron 412: ashcan 413: assault_rifle 414: backpack 415: bakery 416: balance_beam 417: balloon 418: ballpoint 419: Band_Aid 420: banjo 421: bannister 422: barbell 423: barber_chair 424: barbershop 425: barn 426: barometer 427: barrel 428: barrow 429: baseball 430: basketball 431: bassinet 432: bassoon 433: bathing_cap 434: bath_towel 435: bathtub 436: beach_wagon 437: beacon 438: beaker 439: bearskin 440: beer_bottle 441: beer_glass 442: bell_cote 443: bib 444: bicycle-built-for-two 445: bikini 446: binder 447: binoculars 448: birdhouse 449: boathouse 450: bobsled 451: bolo_tie 452: bonnet 453: bookcase 454: bookshop 455: bottlecap 456: bow 457: bow_tie 458: brass 459: brassiere 460: breakwater 461: breastplate 462: broom 463: bucket 464: buckle 465: bulletproof_vest 466: bullet_train 467: butcher_shop 468: cab 469: caldron 470: candle 471: cannon 472: canoe 473: can_opener 474: cardigan 475: car_mirror 476: carousel 477: carpenter's_kit 478: carton 479: car_wheel 480: cash_machine 481: cassette 482: cassette_player 483: castle 484: catamaran 485: CD_player 486: cello 487: cellular_telephone 488: chain 489: chainlink_fence 490: chain_mail 491: chain_saw 492: chest 493: chiffonier 494: chime 495: china_cabinet 496: Christmas_stocking 497: church 498: cinema 499: cleaver 500: cliff_dwelling 501: cloak 502: clog 503: cocktail_shaker 504: coffee_mug 505: coffeepot 506: coil 507: combination_lock 508: computer_keyboard 509: confectionery 510: container_ship 511: convertible 512: corkscrew 513: cornet 514: cowboy_boot 515: cowboy_hat 516: cradle 517: crane 518: crash_helmet 519: crate 520: crib 521: Crock_Pot 522: croquet_ball 523: crutch 524: cuirass 525: dam 526: desk 527: desktop_computer 528: dial_telephone 529: diaper 530: digital_clock 531: digital_watch 532: dining_table 533: dishrag 534: dishwasher 535: disk_brake 536: dock 537: dogsled 538: dome 539: doormat 540: drilling_platform 541: drum 542: drumstick 543: dumbbell 544: Dutch_oven 545: electric_fan 546: electric_guitar 547: electric_locomotive 548: entertainment_center 549: envelope 550: espresso_maker 551: face_powder 552: feather_boa 553: file 554: fireboat 555: fire_engine 556: fire_screen 557: flagpole 558: flute 559: folding_chair 560: football_helmet 561: forklift 562: fountain 563: fountain_pen 564: four-poster 565: freight_car 566: French_horn 567: frying_pan 568: fur_coat 569: garbage_truck 570: gasmask 571: gas_pump 572: goblet 573: go-kart 574: golf_ball 575: golfcart 576: gondola 577: gong 578: gown 579: grand_piano 580: greenhouse 581: grille 582: grocery_store 583: guillotine 584: hair_slide 585: hair_spray 586: half_track 587: hammer 588: hamper 589: hand_blower 590: hand-held_computer 591: handkerchief 592: hard_disc 593: harmonica 594: harp 595: harvester 596: hatchet 597: holster 598: home_theater 599: honeycomb 600: hook 601: hoopskirt 602: horizontal_bar 603: horse_cart 604: hourglass 605: iPod 606: iron 607: jack-o'-lantern 608: jean 609: jeep 610: jersey 611: jigsaw_puzzle 612: jinrikisha 613: joystick 614: kimono 615: knee_pad 616: knot 617: lab_coat 618: ladle 619: lampshade 620: laptop 621: lawn_mower 622: lens_cap 623: letter_opener 624: library 625: lifeboat 626: lighter 627: limousine 628: liner 629: lipstick 630: Loafer 631: lotion 632: loudspeaker 633: loupe 634: lumbermill 635: magnetic_compass 636: mailbag 637: mailbox 638: maillot 639: maillot 640: manhole_cover 641: maraca 642: marimba 643: mask 644: matchstick 645: maypole 646: maze 647: measuring_cup 648: medicine_chest 649: megalith 650: microphone 651: microwave 652: military_uniform 653: milk_can 654: minibus 655: miniskirt 656: minivan 657: missile 658: mitten 659: mixing_bowl 660: mobile_home 661: Model_T 662: modem 663: monastery 664: monitor 665: moped 666: mortar 667: mortarboard 668: mosque 669: mosquito_net 670: motor_scooter 671: mountain_bike 672: mountain_tent 673: mouse 674: mousetrap 675: moving_van 676: muzzle 677: nail 678: neck_brace 679: necklace 680: nipple 681: notebook 682: obelisk 683: oboe 684: ocarina 685: odometer 686: oil_filter 687: organ 688: oscilloscope 689: overskirt 690: oxcart 691: oxygen_mask 692: packet 693: paddle 694: paddlewheel 695: padlock 696: paintbrush 697: pajama 698: palace 699: panpipe 700: paper_towel 701: parachute 702: parallel_bars 703: park_bench 704: parking_meter 705: passenger_car 706: patio 707: pay-phone 708: pedestal 709: pencil_box 710: pencil_sharpener 711: perfume 712: Petri_dish 713: photocopier 714: pick 715: pickelhaube 716: picket_fence 717: pickup 718: pier 719: piggy_bank 720: pill_bottle 721: pillow 722: ping-pong_ball 723: pinwheel 724: pirate 725: pitcher 726: plane 727: planetarium 728: plastic_bag 729: plate_rack 730: plow 731: plunger 732: Polaroid_camera 733: pole 734: police_van 735: poncho 736: pool_table 737: pop_bottle 738: pot 739: potter's_wheel 740: power_drill 741: prayer_rug 742: printer 743: prison 744: projectile 745: projector 746: puck 747: punching_bag 748: purse 749: quill 750: quilt 751: racer 752: racket 753: radiator 754: radio 755: radio_telescope 756: rain_barrel 757: recreational_vehicle 758: reel 759: reflex_camera 760: refrigerator 761: remote_control 762: restaurant 763: revolver 764: rifle 765: rocking_chair 766: rotisserie 767: rubber_eraser 768: rugby_ball 769: rule 770: running_shoe 771: safe 772: safety_pin 773: saltshaker 774: sandal 775: sarong 776: sax 777: scabbard 778: scale 779: school_bus 780: schooner 781: scoreboard 782: screen 783: screw 784: screwdriver 785: seat_belt 786: sewing_machine 787: shield 788: shoe_shop 789: shoji 790: shopping_basket 791: shopping_cart 792: shovel 793: shower_cap 794: shower_curtain 795: ski 796: ski_mask 797: sleeping_bag 798: slide_rule 799: sliding_door 800: slot 801: snorkel 802: snowmobile 803: snowplow 804: soap_dispenser 805: soccer_ball 806: sock 807: solar_dish 808: sombrero 809: soup_bowl 810: space_bar 811: space_heater 812: space_shuttle 813: spatula 814: speedboat 815: spider_web 816: spindle 817: sports_car 818: spotlight 819: stage 820: steam_locomotive 821: steel_arch_bridge 822: steel_drum 823: stethoscope 824: stole 825: stone_wall 826: stopwatch 827: stove 828: strainer 829: streetcar 830: stretcher 831: studio_couch 832: stupa 833: submarine 834: suit 835: sundial 836: sunglass 837: sunglasses 838: sunscreen 839: suspension_bridge 840: swab 841: sweatshirt 842: swimming_trunks 843: swing 844: switch 845: syringe 846: table_lamp 847: tank 848: tape_player 849: teapot 850: teddy 851: television 852: tennis_ball 853: thatch 854: theater_curtain 855: thimble 856: thresher 857: throne 858: tile_roof 859: toaster 860: tobacco_shop 861: toilet_seat 862: torch 863: totem_pole 864: tow_truck 865: toyshop 866: tractor 867: trailer_truck 868: tray 869: trench_coat 870: tricycle 871: trimaran 872: tripod 873: triumphal_arch 874: trolleybus 875: trombone 876: tub 877: turnstile 878: typewriter_keyboard 879: umbrella 880: unicycle 881: upright 882: vacuum 883: vase 884: vault 885: velvet 886: vending_machine 887: vestment 888: viaduct 889: violin 890: volleyball 891: waffle_iron 892: wall_clock 893: wallet 894: wardrobe 895: warplane 896: washbasin 897: washer 898: water_bottle 899: water_jug 900: water_tower 901: whiskey_jug 902: whistle 903: wig 904: window_screen 905: window_shade 906: Windsor_tie 907: wine_bottle 908: wing 909: wok 910: wooden_spoon 911: wool 912: worm_fence 913: wreck 914: yawl 915: yurt 916: web_site 917: comic_book 918: crossword_puzzle 919: street_sign 920: traffic_light 921: book_jacket 922: menu 923: plate 924: guacamole 925: consomme 926: hot_pot 927: trifle 928: ice_cream 929: ice_lolly 930: French_loaf 931: bagel 932: pretzel 933: cheeseburger 934: hotdog 935: mashed_potato 936: head_cabbage 937: broccoli 938: cauliflower 939: zucchini 940: spaghetti_squash 941: acorn_squash 942: butternut_squash 943: cucumber 944: artichoke 945: bell_pepper 946: cardoon 947: mushroom 948: Granny_Smith 949: strawberry 950: orange 951: lemon 952: fig 953: pineapple 954: banana 955: jackfruit 956: custard_apple 957: pomegranate 958: hay 959: carbonara 960: chocolate_sauce 961: dough 962: meat_loaf 963: pizza 964: potpie 965: burrito 966: red_wine 967: espresso 968: cup 969: eggnog 970: alp 971: bubble 972: cliff 973: coral_reef 974: geyser 975: lakeside 976: promontory 977: sandbar 978: seashore 979: valley 980: volcano 981: ballplayer 982: groom 983: scuba_diver 984: rapeseed 985: daisy 986: yellow_lady's_slipper 987: corn 988: acorn 989: hip 990: buckeye 991: coral_fungus 992: agaric 993: gyromitra 994: stinkhorn 995: earthstar 996: hen-of-the-woods 997: bolete 998: ear 999: toilet_tissue Is VGG16 doing its job? Before visualizing the fully-connected layer's filters, I check if VGG16 is doing its highly-reputated job. I took a picture of a dog from Google image, and see if the VGG16 can predict the picture correctly. VGG16 correctly guesses that the picture has a dog! Amazingly, it also guesses the type of the dog correctly! In [111]: from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array image = load_img ( './dog.jpg' , target_size = ( 224 , 224 , 3 )) plt . imshow ( image ) plt . show () image = img_to_array ( image ) #output Numpy-array y_pred = model . predict ( image . reshape ( 1 , image . shape [ 0 ], image . shape [ 1 ], image . shape [ 2 ])) . flatten () ## top 5 selected classes top = 5 chosen_classes = classlabel [ np . argsort ( y_pred )[:: - 1 ][: top ] ] print ( \"The top {} labels\" . format ( top )) print ( \"-----------------------------------------------------\" ) for myclass in chosen_classes : myprob = y_pred [ classlabel == myclass ][ 0 ] print ( \"{:30} prob={:4.3}\" . format ( myclass , myprob )) The top 5 labels ----------------------------------------------------- Maltese_dog prob=0.474 Dandie_Dinmont prob=0.0834 soft-coated_wheaten_terrier prob=0.0734 toy_poodle prob=0.0686 Lhasa prob=0.0521 Visualization of the filters of the final output layers Finally, I visualize the filters of output layers. Here I choose the following 4 output classes for visualization. sea snake (just like Keras's official blog ) magpie (just like Keras's official blog ) bee eater pillow The following codes look a bit complicated because of the double loop. The first loop goes over the four classes. The next while loop iterates until the gradient-ascent algorithm \"converges\". In comparisons to my Model 4, VGG16 is more complex (with about 17 times more parameters (17=138357544/8051502)). Therefore the simple gradient ascent algorithm yield quite different pictures every time it runs with different initial start image. As the solution to this optimization problem is not unique, it is expected to have a different images as the solution. However, the resulting picture should yield the probability of belonging to the corresponding class to be very high, as high as 1. Otherwise, it is most likely that the algorithm did not converge. From the trial and error, I found that the convergence (in terms of the probability of belonging to the class = 1) depends highly on the initial start image. For these reasons, my code runs until it finds the image that yieled high probability of belonging to the class. Observations I was able to find pictures with 100% probability of being sea snake, magpie, cliff or siamang! The pictures that I found look a bit different from the one found by Keras's official blog . Expectedly. The sea snake image seems to have some curvy texture? The magpie image looks to have a texture of feather ... ? some beaks? If you say so. Do I see the colourful bee eater at the center? The pillow image looks soft and fluffy.. I would not classify any of these pictures into sea snake or magpie! I wonder what would happen if I label them as not-sea-snake or not-magie, include as parts of the training data and retrain the models. (Is this the idea of GAN? Something I would love to learn.) But I guess CNN look at pictures differently. I will conclude this blog by quoting the comments from Keras's official blog : we should refrain from our natural tendency to anthropomorphize them and believe that they \"understand\", say, the concept of dog, or the appearance of a magpie, just because they are able to classify these objects with high accuracy. They don't, at least not to any any extent that would make sense to us humans. In [132]: layer_output = layer_dict [ \"predictions\" ] . output out_index = [ 65 , 18 , 92 , 721 ] for i in out_index : visualizer = VisualizeImageMaximizeFmap ( pic_shape = ( 224 , 224 , 3 )) images = [] probs = [] myprob = 0 n_alg = 0 while ( myprob < 0.9 ): myimage = visualizer . find_images_for_layer ( input_img , layer_output ,[ i ], picorig = True , n_iter = 20 ) y_pred = model . predict ( myimage [ 0 ][ 0 ]) . flatten () myprob = y_pred [ i ] n_alg += 1 print ( \"The total number of times the gradient ascent needs to run: {}\" . format ( n_alg )) argimage = { \"prediction\" :[ myimage ]} print ( \"{} probability:\" . format ( classlabel [ i ])), print ( \"{:4.3}\" . format ( myprob )), visualizer . plot_images_wrapper ( argimage , n_row = 1 , scale = 4 ) The total number of times the gradient ascent needs to run: 11 sea_snake probability: 1.0 The total number of times the gradient ascent needs to run: 50 magpie probability: 1.0 The total number of times the gradient ascent needs to run: 4 bee_eater probability: 1.0 The total number of times the gradient ascent needs to run: 10 pillow probability: 1.0 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Visualization of Filters with Keras"},{"url":"achieving-top-23-in-kaggles-facial-keypoints-detection-with-keras-tensorflow.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In this post, I will review deep learning methods for detect the location of keypoints on face images. The data is provided by Kaggle's Facial Keypoints Detection . I will use Keras framework (2.0.6) with tensorflow (1.2.1) backend. There are many nice blog posts that review this data: Daniel Nouri applied convolutional neural nets using Lasagne. Shinya Yuki more recently applied same methodologies using Keras. This post follows the same line of discussions. With several new additions by me: Experiments with the data augmentation with shifted images. Previous posts only considered mirror images for data augmentations. Submittion of the predictions to Kaggle. The final model ranks top XX out of Kaggle competition. The public score=. Preparation Under data folder, training.csv and test.csv are saved. These are doanloaded from Kaggle In [57]: import os import matplotlib.pyplot as plt import numpy as np from pandas.io.parsers import read_csv from sklearn.utils import shuffle FTRAIN = 'data/training.csv' FTEST = 'data/test.csv' FIdLookup = 'data/IdLookupTable.csv' Use a single GPU In [2]: import tensorflow as tf from keras.backend.tensorflow_backend import set_session print ( tf . __version__ ) config = tf . ConfigProto () config . gpu_options . per_process_gpu_memory_fraction = 0.95 config . gpu_options . visible_device_list = \"0\" #### 1 GPU1 #### 2 GPU2 #### 0 GPU3 #### 4 GPU4 set_session ( tf . Session ( config = config )) Using TensorFlow backend. 1.2.1 common function defenitions These functions are written by Daniel Nouri . I made very minor changes. For example, I changed the X dimention structure to have (Nsample, Nrows in frame, N columns in frame, 1) in load2d. In [3]: def plot_sample ( X , y , axs ): ''' kaggle picture is 96 by 96 y is rescaled to range between -1 and 1 ''' axs . imshow ( X . reshape ( 96 , 96 ), cmap = \"gray\" ) axs . scatter ( 48 * y [ 0 :: 2 ] + 48 , 48 * y [ 1 :: 2 ] + 48 ) def load ( test = False , cols = None ): \"\"\" load test/train data cols : a list containing landmark label names. If this is specified, only the subset of the landmark labels are extracted. for example, cols could be: [left_eye_center_x, left_eye_center_y] return: X: 2-d numpy array (Nsample, Ncol*Nrow) y: 2-d numpy array (Nsample, Nlandmarks*2) In total there are 15 landmarks. As x and y coordinates are recorded, u.shape = (Nsample,30) \"\"\" fname = FTEST if test else FTRAIN df = read_csv ( os . path . expanduser ( fname )) df [ 'Image' ] = df [ 'Image' ] . apply ( lambda im : np . fromstring ( im , sep = ' ' )) if cols : df = df [ list ( cols ) + [ 'Image' ]] myprint = df . count () myprint = myprint . reset_index () print ( myprint ) ## row with at least one NA columns are removed! df = df . dropna () X = np . vstack ( df [ 'Image' ] . values ) / 255. # changes valeus between 0 and 1 X = X . astype ( np . float32 ) if not test : # labels only exists for the training data ## standardization of the response y = df [ df . columns [: - 1 ]] . values y = ( y - 48 ) / 48 # y values are between [-1,1] X , y = shuffle ( X , y , random_state = 42 ) # shuffle data y = y . astype ( np . float32 ) else : y = None return X , y def load2d ( test = False , cols = None ): re = load ( test , cols ) X = re [ 0 ] . reshape ( - 1 , 96 , 96 , 1 ) y = re [ 1 ] return X , y def plot_loss ( hist , name , plt , RMSE_TF = False ): ''' RMSE_TF: if True, then RMSE is plotted with original scale ''' loss = hist [ 'loss' ] val_loss = hist [ 'val_loss' ] if RMSE_TF : loss = np . sqrt ( np . array ( loss )) * 48 val_loss = np . sqrt ( np . array ( val_loss )) * 48 plt . plot ( loss , \"--\" , linewidth = 3 , label = \"train:\" + name ) plt . plot ( val_loss , linewidth = 3 , label = \"val:\" + name ) load data X is a 2 dimentional numpy array (Nsample, Ncol * Nrow) Ncol = The number of columns in original picture = 96. The landmarks labels tend to be missing a lot. The right_eyebrow_outer_end_x are recorded only for 2236 pictures. We only use the pictures with all non missing landmarks (for now). How to use all the data will be discussed later. In [4]: X , y = load () print ( \"X.shape == {}; X.min == {:.3f}; X.max == {:.3f}\" . format ( X . shape , X . min (), X . max ())) print ( \"y.shape == {}; y.min == {:.3f}; y.max == {:.3f}\" . format ( y . shape , y . min (), y . max ())) index 0 0 left_eye_center_x 7039 1 left_eye_center_y 7039 2 right_eye_center_x 7036 3 right_eye_center_y 7036 4 left_eye_inner_corner_x 2271 5 left_eye_inner_corner_y 2271 6 left_eye_outer_corner_x 2267 7 left_eye_outer_corner_y 2267 8 right_eye_inner_corner_x 2268 9 right_eye_inner_corner_y 2268 10 right_eye_outer_corner_x 2268 11 right_eye_outer_corner_y 2268 12 left_eyebrow_inner_end_x 2270 13 left_eyebrow_inner_end_y 2270 14 left_eyebrow_outer_end_x 2225 15 left_eyebrow_outer_end_y 2225 16 right_eyebrow_inner_end_x 2270 17 right_eyebrow_inner_end_y 2270 18 right_eyebrow_outer_end_x 2236 19 right_eyebrow_outer_end_y 2236 20 nose_tip_x 7049 21 nose_tip_y 7049 22 mouth_left_corner_x 2269 23 mouth_left_corner_y 2269 24 mouth_right_corner_x 2270 25 mouth_right_corner_y 2270 26 mouth_center_top_lip_x 2275 27 mouth_center_top_lip_y 2275 28 mouth_center_bottom_lip_x 7016 29 mouth_center_bottom_lip_y 7016 30 Image 7049 X.shape == (2140, 9216); X.min == 0.000; X.max == 1.000 y.shape == (2140, 30); y.min == -0.920; y.max == 0.996 Single layer Feed forward network for setting the baseline performance The original is from Shinya Yuki In [5]: %% time from keras.models import Sequential from keras.layers import Dense, Activation from keras.optimizers import SGD model = Sequential() model.add(Dense(100,input_dim=X.shape[1])) model.add(Activation('relu')) model.add(Dense(30)) sgd = SGD(lr=0.01, momentum=0.9, nesterov=True) model.compile(loss='mean_squared_error', optimizer=sgd) hist = model.fit(X, y, nb_epoch=100, validation_split=0.2,verbose=False) /home/fairy/anaconda2/lib/python2.7/site-packages/keras/models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`. warnings.warn('The `nb_epoch` argument in `fit` ' CPU times: user 34.2 s, sys: 3.7 s, total: 37.9 s Wall time: 18.6 s In [6]: plot_loss ( hist . history , \"model 1\" , plt ) plt . legend () plt . grid () plt . yscale ( \"log\" ) plt . xlabel ( \"epoch\" ) plt . ylabel ( \"log loss\" ) plt . show () Test data evaluation and visualization In [7]: X_test , _ = load ( test = True ) y_test = model . predict ( X_test ) index 0 0 ImageId 1783 1 Image 1783 The simple feedfoward network shows some descent performance. but sometimes landmarks are off the face! In [10]: fig = plt . figure ( figsize = ( 7 , 7 )) fig . subplots_adjust ( hspace = 0.13 , wspace = 0.0001 , left = 0 , right = 1 , bottom = 0 , top = 1 ) Npicture = 9 count = 1 for irow in range ( Npicture ): ipic = np . random . choice ( X_test . shape [ 0 ]) ax = fig . add_subplot ( Npicture / 3 , 3 , count , xticks = [], yticks = []) plot_sample ( X_test [ ipic ], y_test [ ipic ], ax ) ax . set_title ( \"picture \" + str ( ipic )) count += 1 plt . show () Save model weights and architecture In [11]: from keras.models import model_from_json def save_model ( model , name ): ''' save model architecture and model weights ''' json_string = model . to_json () open ( name + '_architecture.json' , 'w' ) . write ( json_string ) model . save_weights ( name + '_weights.h5' ) def load_model ( name ): model = model_from_json ( open ( name + '_architecture.json' ) . read ()) model . load_weights ( name + '_weights.h5' ) return ( model ) save_model ( model , \"model1\" ) model = load_model ( \"model1\" ) Delete these data as we use differently structured data from now on In [12]: del X , y , X_test , y_test Covolusional neural network Let's make our model more complecated. Loading training data. Notice that X is now 4-d numpy array (Nsample, Nrow, Ncol, 1). In [13]: ## load data X , y = load2d () X . shape index 0 0 left_eye_center_x 7039 1 left_eye_center_y 7039 2 right_eye_center_x 7036 3 right_eye_center_y 7036 4 left_eye_inner_corner_x 2271 5 left_eye_inner_corner_y 2271 6 left_eye_outer_corner_x 2267 7 left_eye_outer_corner_y 2267 8 right_eye_inner_corner_x 2268 9 right_eye_inner_corner_y 2268 10 right_eye_outer_corner_x 2268 11 right_eye_outer_corner_y 2268 12 left_eyebrow_inner_end_x 2270 13 left_eyebrow_inner_end_y 2270 14 left_eyebrow_outer_end_x 2225 15 left_eyebrow_outer_end_y 2225 16 right_eyebrow_inner_end_x 2270 17 right_eyebrow_inner_end_y 2270 18 right_eyebrow_outer_end_x 2236 19 right_eyebrow_outer_end_y 2236 20 nose_tip_x 7049 21 nose_tip_y 7049 22 mouth_left_corner_x 2269 23 mouth_left_corner_y 2269 24 mouth_right_corner_x 2270 25 mouth_right_corner_y 2270 26 mouth_center_top_lip_x 2275 27 mouth_center_top_lip_y 2275 28 mouth_center_bottom_lip_x 7016 29 mouth_center_bottom_lip_y 7016 30 Image 7049 Out[13]: (2140, 96, 96, 1) Define simple CNN funciton as this model will be constructed several times In [40]: from keras.layers import Conv2D , MaxPooling2D , Flatten , Dropout def SimpleCNN ( withDropout = False ): ''' WithDropout: If True, then dropout regularlization is added. This feature is experimented later. ''' model = Sequential () model . add ( Conv2D ( 32 ,( 3 , 3 ), input_shape = ( 96 , 96 , 1 ))) model . add ( Activation ( 'relu' )) ## 96 - 3 + 2 model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) ## 96 - (3-1)*2 if withDropout : model . add ( Dropout ( 0.1 )) model . add ( Conv2D ( 64 ,( 2 , 2 ))) model . add ( Activation ( 'relu' )) ## model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) if withDropout : model . add ( Dropout ( 0.1 )) model . add ( Conv2D ( 128 ,( 2 , 2 ))) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) if withDropout : model . add ( Dropout ( 0.1 )) model . add ( Flatten ()) model . add ( Dense ( 500 )) model . add ( Activation ( 'relu' )) if withDropout : model . add ( Dropout ( 0.1 )) model . add ( Dense ( 500 )) model . add ( Activation ( 'relu' )) if withDropout : model . add ( Dropout ( 0.1 )) model . add ( Dense ( 30 )) sgd = SGD ( lr = 0.01 , momentum = 0.9 , nesterov = True ) model . compile ( loss = \"mean_squared_error\" , optimizer = sgd ) return ( model ) In [15]: %% time model2 = SimpleCNN() hist2 = model2.fit(X,y,nb_epoch=1000,validation_split=0.2,verbose=False) CPU times: user 13min 3s, sys: 1min 28s, total: 14min 31s Wall time: 10min 52s plot validation loss, train loss In [16]: plt . figure ( figsize = ( 8 , 8 )) plot_loss ( hist . history , \"model 1\" , plt ) plot_loss ( hist2 . history , \"model 2\" , plt ) plt . legend () plt . grid () plt . yscale ( \"log\" ) plt . xlabel ( \"epoch\" ) plt . ylabel ( \"loss\" ) plt . show () In [17]: sample1 , _ = load ( test = True ) sample2 , _ = load2d ( test = True ) y_pred1 = model . predict ( sample1 ) y_pred2 = model2 . predict ( sample2 ) index 0 0 ImageId 1783 1 Image 1783 index 0 0 ImageId 1783 1 Image 1783 Compare the model performance fully connected layers vs simple CNN In [18]: fig = plt . figure ( figsize = ( 4 , 10 )) fig . subplots_adjust ( hspace = 0.001 , wspace = 0.001 , left = 0 , right = 1 , bottom = 0 , top = 1 ) Npicture = 5 count = 1 for irow in range ( Npicture ): ipic = np . random . choice ( sample2 . shape [ 0 ]) ax = fig . add_subplot ( Npicture , 2 , count , xticks = [], yticks = []) plot_sample ( sample1 [ ipic ], y_pred1 [ ipic ], ax ) if count < 3 : ax . set_title ( \"model 1\" ) count += 1 ax = fig . add_subplot ( Npicture , 2 , count , xticks = [], yticks = []) plot_sample ( sample2 [ ipic ], y_pred2 [ ipic ], ax ) if count < 3 : ax . set_title ( \"model 2\" ) count += 1 plt . show () Data augmentation flipping pictures FlippedImageDataGenerator is written by Shinya . This class extends the keras.preprocessing.image.mageDataGenerator and overwrites \"next\" method. When I try using this class, it was not correctly generating flipped pictures. *This could be because of the different version of the keras? It seems ImageDataGenerator.flow() is not internally calling next() method). So I take more manual approach and explicitly write codes to modify each batch within each iteration/epoch (just as in the first example of keras documentation ). Why not just using ImageDataGenerator? Keras has exisiting ImageDataGenerator but this only flip the data and not the land marks. I guess ImageDataGenerator can be used for image classification purpose where the classification labels do not need to be flipped. (e.g., Dog picture is a dog picture even if the picutre is flipped.) In [19]: class DataModifier ( object ): def fit ( self , X_ , y_ ): return ( NotImplementedError ) class FlipPic ( DataModifier ): def __init__ ( self , flip_indices = None ): if flip_indices is None : flip_indices = [ ( 0 , 2 ), ( 1 , 3 ), ( 4 , 8 ), ( 5 , 9 ), ( 6 , 10 ), ( 7 , 11 ), ( 12 , 16 ), ( 13 , 17 ), ( 14 , 18 ), ( 15 , 19 ), ( 22 , 24 ), ( 23 , 25 ) ] self . flip_indices = flip_indices def fit ( self , X_batch , y_batch ): batch_size = X_batch . shape [ 0 ] indices = np . random . choice ( batch_size , batch_size / 2 , replace = False ) X_batch [ indices ] = X_batch [ indices , :, :: - 1 ,:] y_batch [ indices , :: 2 ] = y_batch [ indices , :: 2 ] * - 1 # flip left eye to right eye, left mouth to right mouth and so on .. for a , b in self . flip_indices : y_batch [ indices , a ], y_batch [ indices , b ] = ( y_batch [ indices , b ], y_batch [ indices , a ] ) return X_batch , y_batch manually splitting training and validation data In [20]: from sklearn.model_selection import train_test_split X_train , X_val , y_train , y_val = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) print ( X_train . shape ) (1712, 96, 96, 1) Make sure that the pictures show up in both directions In [21]: from keras.preprocessing.image import ImageDataGenerator generator = ImageDataGenerator () modifier = FlipPic () fig = plt . figure ( figsize = ( 7 , 7 )) count = 1 for batch in generator . flow ( X_train [: 2 ], y_train [: 2 ]): X_batch , y_batch = modifier . fit ( * batch ) ax = fig . add_subplot ( 3 , 3 , count , xticks = [], yticks = []) plot_sample ( X_batch [ 0 ], y_batch [ 0 ], ax ) count += 1 if count == 10 : break plt . show () training As previously discussed, we manually write fit function. Our fit function allows \"Early stopping\", which means that the back propagration algorithm will terminate if the validation loss does not decrease for conseqtive epochs In [22]: def fit ( model , modifier , train , validation , batch_size = 32 , epochs = 2000 , print_every = 10 , patience = np . Inf ): ''' model : keras model object Modifier: DataModifier() object train: tuple containing two numpy arrays (X_train,y_train) validation: tuple containing two numpy arrays (X_val,y_val) patience: The back propagation algorithm will stop if the val_loss does not decrease after epochs ''' ## manually write fit method X_train , y_train = train X_val , y_val = validation generator = ImageDataGenerator () history = { \"loss\" :[], \"val_loss\" :[]} for e in range ( epochs ): if e % print_every == 0 : print ( 'Epoch {:4}:' . format ( e )), ## -------- ## ## training ## -------- ## batches = 0 loss_epoch = [] for X_batch , y_batch in generator . flow ( X_train , y_train , batch_size = batch_size ): X_batch , y_batch = modifier . fit ( X_batch , y_batch ) hist = model . fit ( X_batch , y_batch , verbose = False , epochs = 1 ) loss_epoch . extend ( hist . history [ \"loss\" ]) batches += 1 if batches >= len ( X_train ) / batch_size : # we need to break the loop by hand because # the generator loops indefinitely break loss = np . mean ( loss_epoch ) history [ \"loss\" ] . append ( loss ) ## --------- ## ## validation ## --------- ## y_pred = model . predict ( X_val ) val_loss = np . mean (( y_pred - y_val ) ** 2 ) history [ \"val_loss\" ] . append ( val_loss ) if e % print_every == 0 : print ( \"loss - {:6.5f}, val_loss - {:6.5f}\" . format ( loss , val_loss )) min_val_loss = np . min ( history [ \"val_loss\" ]) ## Early stopping if patience is not np . Inf : if np . all ( min_val_loss < np . array ( history [ \"val_loss\" ])[ - patience :]): break return ( history ) In [23]: %% time #X, y = load2d() model3 = SimpleCNN() hist3 = fit(model3,modifier, train=(X_train,y_train), validation=(X_val,y_val), batch_size=32,epochs=2000,print_every=100 ) Epoch 0: loss - 0.03308, val_loss - 0.00749 Epoch 100: loss - 0.00289, val_loss - 0.00274 Epoch 200: loss - 0.00174, val_loss - 0.00176 Epoch 300: loss - 0.00141, val_loss - 0.00152 Epoch 400: loss - 0.00120, val_loss - 0.00139 Epoch 500: loss - 0.00106, val_loss - 0.00131 Epoch 600: loss - 0.00097, val_loss - 0.00125 Epoch 700: loss - 0.00087, val_loss - 0.00122 Epoch 800: loss - 0.00080, val_loss - 0.00118 Epoch 900: loss - 0.00074, val_loss - 0.00114 Epoch 1000: loss - 0.00068, val_loss - 0.00113 Epoch 1100: loss - 0.00063, val_loss - 0.00110 Epoch 1200: loss - 0.00059, val_loss - 0.00109 Epoch 1300: loss - 0.00055, val_loss - 0.00108 Epoch 1400: loss - 0.00052, val_loss - 0.00107 Epoch 1500: loss - 0.00048, val_loss - 0.00107 Epoch 1600: loss - 0.00046, val_loss - 0.00106 Epoch 1700: loss - 0.00043, val_loss - 0.00106 Epoch 1800: loss - 0.00040, val_loss - 0.00106 Epoch 1900: loss - 0.00038, val_loss - 0.00106 CPU times: user 26min 35s, sys: 2min 40s, total: 29min 16s Wall time: 23min plot the training and validation losses Data augmentation with flipped pictures help improving the model prediction accuracy. One potential issue: \"train:model 3\" (final value: 0.00038) is by far less than the \"val:model 3\" (final value: 0.00106), indicating that the model might have overfitted. In [24]: plt . figure ( figsize = ( 8 , 8 )) plot_loss ( hist . history , \"model 1\" , plt ) plot_loss ( hist2 . history , \"model 2\" , plt ) plot_loss ( hist3 , \"model 3\" , plt ) plt . legend () plt . grid () plt . yscale ( \"log\" ) plt . xlabel ( \"epoch\" ) plt . ylabel ( \"loss\" ) plt . show () data augmentation shifting pictures Flipping pictures can double the number of pictures twice. If we allow the pictures to shift by some pixcels within frames, this can increase the number of pictures substantially! Here is my code to randomly shift the pictures to left, right, top, bottom by prespecified proportion. In [25]: class ShiftFlipPic ( FlipPic ): def __init__ ( self , flip_indices = None , prop = 0.1 ): super ( ShiftFlipPic , self ) . __init__ ( flip_indices ) self . prop = prop def fit ( self , X , y ): X , y = super ( ShiftFlipPic , self ) . fit ( X , y ) X , y = self . shift_image ( X , y , prop = self . prop ) return ( X , y ) def random_shift ( self , shift_range , n = 96 ): ''' :param shift_range: The maximum number of columns/rows to shift :return: keep(0): minimum row/column index to keep keep(1): maximum row/column index to keep assign(0): minimum row/column index to assign assign(1): maximum row/column index to assign shift: amount to shift the landmark assign(1) - assign(0) == keep(1) - keep(0) ''' shift = np . random . randint ( - shift_range , shift_range ) def shift_left ( n , shift ): shift = np . abs ( shift ) return ( 0 , n - shift ) def shift_right ( n , shift ): shift = np . abs ( shift ) return ( shift , n ) if shift < 0 : keep = shift_left ( n , shift ) assign = shift_right ( n , shift ) else : assign = shift_left ( n , shift ) ## less than 96 keep = shift_right ( n , shift ) return (( keep , assign , shift )) def shift_single_image ( self , x_ , y_ , prop = 0.1 ): ''' :param x_: a single picture array (96, 96, 1) :param y_: 15 landmark locations [0::2] contains x axis values [1::2] contains y axis values :param prop: proportion of random horizontal and vertical shift relative to the number of columns e.g. prop = 0.1 then the picture is moved at least by 0.1*96 = 8 columns/rows :return: x_, y_ ''' w_shift_max = int ( x_ . shape [ 0 ] * prop ) h_shift_max = int ( x_ . shape [ 1 ] * prop ) w_keep , w_assign , w_shift = self . random_shift ( w_shift_max ) h_keep , h_assign , h_shift = self . random_shift ( h_shift_max ) x_ [ w_assign [ 0 ]: w_assign [ 1 ], h_assign [ 0 ]: h_assign [ 1 ],:] = x_ [ w_keep [ 0 ]: w_keep [ 1 ], h_keep [ 0 ]: h_keep [ 1 ],:] y_ [ 0 :: 2 ] = y_ [ 0 :: 2 ] - h_shift / float ( x_ . shape [ 0 ] / 2. ) y_ [ 1 :: 2 ] = y_ [ 1 :: 2 ] - w_shift / float ( x_ . shape [ 1 ] / 2. ) return ( x_ , y_ ) def shift_image ( self , X , y , prop = 0.1 ): ## This function may be modified to be more efficient e.g. get rid of loop? for irow in range ( X . shape [ 0 ]): x_ = X [ irow ] y_ = y [ irow ] X [ irow ], y [ irow ] = self . shift_single_image ( x_ , y_ , prop = prop ) return ( X , y ) Following codes plot the generated pictures. Observations The landmarks are shiftting together with the picture frame. Notice that if you shift the pictures too much, then the landmarks go outside of the frame. (For now we are going to ignore this potential problem.) In [26]: from keras.preprocessing.image import ImageDataGenerator generator = ImageDataGenerator () shiftFlipPic = ShiftFlipPic ( prop = 0.1 ) fig = plt . figure ( figsize = ( 7 , 7 )) count = 1 for batch in generator . flow ( X_train [: 2 ], y_train [: 2 ]): X_batch , y_batch = shiftFlipPic . fit ( * batch ) ax = fig . add_subplot ( 3 , 3 , count , xticks = [], yticks = []) plot_sample ( X_batch [ 0 ], y_batch [ 0 ], ax ) count += 1 if count == 10 : break plt . show () training In [42]: %% time model4 = SimpleCNN() hist4 = fit(model4,shiftFlipPic, train=(X_train,y_train), validation=(X_val,y_val), batch_size=32,epochs=3000,print_every=50,patience=100) Epoch 0: loss - 0.04738, val_loss - 0.00727 Epoch 50: loss - 0.00461, val_loss - 0.00318 Epoch 100: loss - 0.00343, val_loss - 0.00225 Epoch 150: loss - 0.00293, val_loss - 0.00195 Epoch 200: loss - 0.00250, val_loss - 0.00177 Epoch 250: loss - 0.00230, val_loss - 0.00166 Epoch 300: loss - 0.00225, val_loss - 0.00155 Epoch 350: loss - 0.00213, val_loss - 0.00151 Epoch 400: loss - 0.00201, val_loss - 0.00142 Epoch 450: loss - 0.00192, val_loss - 0.00138 Epoch 500: loss - 0.00182, val_loss - 0.00134 Epoch 550: loss - 0.00173, val_loss - 0.00131 Epoch 600: loss - 0.00169, val_loss - 0.00131 Epoch 650: loss - 0.00166, val_loss - 0.00125 Epoch 700: loss - 0.00158, val_loss - 0.00123 Epoch 750: loss - 0.00154, val_loss - 0.00121 Epoch 800: loss - 0.00148, val_loss - 0.00117 Epoch 850: loss - 0.00147, val_loss - 0.00116 Epoch 900: loss - 0.00141, val_loss - 0.00113 Epoch 950: loss - 0.00137, val_loss - 0.00111 Epoch 1000: loss - 0.00136, val_loss - 0.00112 Epoch 1050: loss - 0.00135, val_loss - 0.00110 Epoch 1100: loss - 0.00128, val_loss - 0.00108 Epoch 1150: loss - 0.00131, val_loss - 0.00106 Epoch 1200: loss - 0.00130, val_loss - 0.00106 Epoch 1250: loss - 0.00124, val_loss - 0.00104 Epoch 1300: loss - 0.00120, val_loss - 0.00102 Epoch 1350: loss - 0.00122, val_loss - 0.00102 Epoch 1400: loss - 0.00118, val_loss - 0.00100 Epoch 1450: loss - 0.00115, val_loss - 0.00100 Epoch 1500: loss - 0.00116, val_loss - 0.00100 Epoch 1550: loss - 0.00114, val_loss - 0.00099 Epoch 1600: loss - 0.00114, val_loss - 0.00097 Epoch 1650: loss - 0.00109, val_loss - 0.00096 Epoch 1700: loss - 0.00108, val_loss - 0.00096 Epoch 1750: loss - 0.00108, val_loss - 0.00095 Epoch 1800: loss - 0.00106, val_loss - 0.00094 Epoch 1850: loss - 0.00102, val_loss - 0.00095 Epoch 1900: loss - 0.00104, val_loss - 0.00093 Epoch 1950: loss - 0.00101, val_loss - 0.00093 Epoch 2000: loss - 0.00101, val_loss - 0.00092 Epoch 2050: loss - 0.00098, val_loss - 0.00093 Epoch 2100: loss - 0.00097, val_loss - 0.00090 Epoch 2150: loss - 0.00096, val_loss - 0.00091 Epoch 2200: loss - 0.00096, val_loss - 0.00089 Epoch 2250: loss - 0.00093, val_loss - 0.00089 Epoch 2300: loss - 0.00095, val_loss - 0.00088 Epoch 2350: loss - 0.00094, val_loss - 0.00088 Epoch 2400: loss - 0.00092, val_loss - 0.00087 Epoch 2450: loss - 0.00090, val_loss - 0.00088 Epoch 2500: loss - 0.00090, val_loss - 0.00087 Epoch 2550: loss - 0.00089, val_loss - 0.00086 Epoch 2600: loss - 0.00089, val_loss - 0.00086 Epoch 2650: loss - 0.00088, val_loss - 0.00084 Epoch 2700: loss - 0.00089, val_loss - 0.00086 Epoch 2750: loss - 0.00088, val_loss - 0.00085 Epoch 2800: loss - 0.00085, val_loss - 0.00084 Epoch 2850: loss - 0.00086, val_loss - 0.00084 Epoch 2900: loss - 0.00087, val_loss - 0.00084 Epoch 2950: loss - 0.00083, val_loss - 0.00085 CPU times: user 42min 39s, sys: 4min 4s, total: 46min 44s Wall time: 37min 56s plot the training and validation losses Data augmentation with flipped + shifted pictures help improving the model prediction accuracy. The val_loss is below 0.001 Model 3 had overfitting issue when the number of epoch was 2000 (\"train:model 3\" << \"val:model 3\"). Model 4 seems to have an opposite situation, underfitting, at epoch = 2000 as (\"train:model 4\" > \"val:model 4\"). As \"val: model4\" seems still decreasing at epoch = 2000, we increase the number of epoch to 3000. By epoch = 3000 \"train:model 4\" ~= \"val:model 4\". In practice, it is better to stop the training when training loss is slightly less than the validation, but I stop training here. In [43]: plt . figure ( figsize = ( 8 , 8 )) plot_loss ( hist . history , \"model 1\" , plt ) plot_loss ( hist2 . history , \"model 2\" , plt ) plot_loss ( hist3 , \"model 3\" , plt ) plot_loss ( hist4 , \"model 4\" , plt ) plt . legend () plt . grid () plt . yscale ( \"log\" ) plt . xlabel ( \"epoch\" ) plt . ylabel ( \"loss\" ) plt . show () Save the model object as model 4 seems to be the best so far In [45]: save_model ( model4 , \"model4\" ) More complex model Shinya's blog also experimented with adding Dropout regulaziation layer. You can do this by setting model5 = SimpleCNN(True). As the data augmentation with the random shifting already served the purpose of raguralization, I did not see much improvement by adding Dropout layer. Create separate models for different landmarks separately. Up to this point, we are only using about 20% (X.shape[0] = 2140) of the original data. This is quite a waste as for some landmarks (e.g., left_eye_center, right_eye_center), more than 7000 frames are available. If I separately create model for separate set of landmarks, I can use more data for training. This motivated Danile Nouri to create seprate models. I follow this blog and create 6 separate models. The 15 landmarks are devided into 6 separate groups as shown in the list \"SPECIALIST_SETTINGS\" below. All 6 models contains the same CNN architecture but the final output layer is adjusted for different number of outputs: for example we have a model for left eye and right eye center landmark prediction. As there are are x and y coordinates for both eye centers, we have 4 nodes in the output layer of this model. Remind you that it took me 38 minutes to train Model 4 with epochs = 3000. If I train all 6 models from a scratch, it could take about 4 hours (38 minutes x 6). Instead, I will take advantage of the knowledge (i.e., weights) from model 4. We will use the weights from model 4 and only train the weights from the final output layer. This is the idea of transfer learning. About 1000 epochs would be enough to train the weights from the final output layers. However, we will just train 500 epochs due to the lack of time. In [194]: SPECIALIST_SETTINGS = [ dict ( columns = ( 'left_eye_center_x' , 'left_eye_center_y' , 'right_eye_center_x' , 'right_eye_center_y' , ), flip_indices = (( 0 , 2 ), ( 1 , 3 )), ), dict ( columns = ( 'nose_tip_x' , 'nose_tip_y' , ), flip_indices = (), ), dict ( columns = ( 'mouth_left_corner_x' , 'mouth_left_corner_y' , 'mouth_right_corner_x' , 'mouth_right_corner_y' , 'mouth_center_top_lip_x' , 'mouth_center_top_lip_y' , ), flip_indices = (( 0 , 2 ), ( 1 , 3 )), ), dict ( columns = ( 'mouth_center_bottom_lip_x' , 'mouth_center_bottom_lip_y' , ), flip_indices = (), ), dict ( columns = ( 'left_eye_inner_corner_x' , 'left_eye_inner_corner_y' , 'right_eye_inner_corner_x' , 'right_eye_inner_corner_y' , 'left_eye_outer_corner_x' , 'left_eye_outer_corner_y' , 'right_eye_outer_corner_x' , 'right_eye_outer_corner_y' , ), flip_indices = (( 0 , 2 ), ( 1 , 3 ), ( 4 , 6 ), ( 5 , 7 )), ), dict ( columns = ( 'left_eyebrow_inner_end_x' , 'left_eyebrow_inner_end_y' , 'right_eyebrow_inner_end_x' , 'right_eyebrow_inner_end_y' , 'left_eyebrow_outer_end_x' , 'left_eyebrow_outer_end_y' , 'right_eyebrow_outer_end_x' , 'right_eyebrow_outer_end_y' , ), flip_indices = (( 0 , 2 ), ( 1 , 3 ), ( 4 , 6 ), ( 5 , 7 )), ), ] from collections import OrderedDict def fit_specialists ( freeze = True , print_every = 50 , epochs = 3000 , prop = 0.1 , name_transfer_model = \"model4\" ): specialists = OrderedDict () for setting in SPECIALIST_SETTINGS : cols = setting [ 'columns' ] flip_indices = setting [ 'flip_indices' ] X , y = load2d ( cols = cols ) X_train , X_val , y_train , y_val = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) model = load_model ( name_transfer_model ) if freeze : for layer in model . layers : layer . trainable = False model . layers . pop () # get rid of output layer model . outputs = [ model . layers [ - 1 ] . output ] model . layers [ - 1 ] . outbound_nodes = [] model . add ( Dense ( len ( cols ))) # add new output layer model . compile ( loss = 'mean_squared_error' , optimizer = \"adam\" ) shiftFlipPic = ShiftFlipPic ( flip_indices = flip_indices , prop = prop ) ## print(model.summary()) hist = fit ( model , shiftFlipPic , train = ( X_train , y_train ), validation = ( X_val , y_val ), batch_size = 32 , epochs = epochs , print_every = print_every ) specialists [ cols ] = { \"model\" : model , \"hist\" : hist } return ( specialists ) Set freeze = True for training the weights only from the final outputlayers. If freeze = False then the weights from the transfer model are used only for the initialization of the model and all the weights are re-trained. epochs = 500 seems to be enough (checked by looking at the plot of val_loss vs epochs) use model4 as the transfer model In [47]: %% time specialists1 = fit_specialists(freeze=True, print_every=50, epochs=500, name_transfer_model=\"model4\") index 0 0 left_eye_center_x 7039 1 left_eye_center_y 7039 2 right_eye_center_x 7036 3 right_eye_center_y 7036 4 Image 7049 Epoch 0: loss - 0.01192, val_loss - 0.00540 Epoch 50: loss - 0.00252, val_loss - 0.00319 Epoch 100: loss - 0.00250, val_loss - 0.00319 Epoch 150: loss - 0.00260, val_loss - 0.00313 Epoch 200: loss - 0.00260, val_loss - 0.00309 Epoch 250: loss - 0.00261, val_loss - 0.00317 Epoch 300: loss - 0.00254, val_loss - 0.00311 Epoch 350: loss - 0.00253, val_loss - 0.00312 Epoch 400: loss - 0.00249, val_loss - 0.00310 Epoch 450: loss - 0.00258, val_loss - 0.00335 index 0 0 nose_tip_x 7049 1 nose_tip_y 7049 2 Image 7049 Epoch 0: loss - 0.01236, val_loss - 0.00851 Epoch 50: loss - 0.00565, val_loss - 0.00483 Epoch 100: loss - 0.00573, val_loss - 0.00476 Epoch 150: loss - 0.00557, val_loss - 0.00490 Epoch 200: loss - 0.00575, val_loss - 0.00469 Epoch 250: loss - 0.00563, val_loss - 0.00466 Epoch 300: loss - 0.00565, val_loss - 0.00476 Epoch 350: loss - 0.00569, val_loss - 0.00534 Epoch 400: loss - 0.00555, val_loss - 0.00462 Epoch 450: loss - 0.00572, val_loss - 0.00487 index 0 0 mouth_left_corner_x 2269 1 mouth_left_corner_y 2269 2 mouth_right_corner_x 2270 3 mouth_right_corner_y 2270 4 mouth_center_top_lip_x 2275 5 mouth_center_top_lip_y 2275 6 Image 7049 Epoch 0: loss - 0.05808, val_loss - 0.00793 Epoch 50: loss - 0.00175, val_loss - 0.00156 Epoch 100: loss - 0.00157, val_loss - 0.00144 Epoch 150: loss - 0.00146, val_loss - 0.00139 Epoch 200: loss - 0.00162, val_loss - 0.00133 Epoch 250: loss - 0.00149, val_loss - 0.00130 Epoch 300: loss - 0.00149, val_loss - 0.00143 Epoch 350: loss - 0.00151, val_loss - 0.00134 Epoch 400: loss - 0.00152, val_loss - 0.00135 Epoch 450: loss - 0.00142, val_loss - 0.00137 index 0 0 mouth_center_bottom_lip_x 7016 1 mouth_center_bottom_lip_y 7016 2 Image 7049 Epoch 0: loss - 0.01435, val_loss - 0.00720 Epoch 50: loss - 0.00476, val_loss - 0.00393 Epoch 100: loss - 0.00467, val_loss - 0.00372 Epoch 150: loss - 0.00478, val_loss - 0.00377 Epoch 200: loss - 0.00476, val_loss - 0.00389 Epoch 250: loss - 0.00490, val_loss - 0.00366 Epoch 300: loss - 0.00474, val_loss - 0.00390 Epoch 350: loss - 0.00471, val_loss - 0.00402 Epoch 400: loss - 0.00455, val_loss - 0.00366 Epoch 450: loss - 0.00473, val_loss - 0.00363 index 0 0 left_eye_inner_corner_x 2271 1 left_eye_inner_corner_y 2271 2 right_eye_inner_corner_x 2268 3 right_eye_inner_corner_y 2268 4 left_eye_outer_corner_x 2267 5 left_eye_outer_corner_y 2267 6 right_eye_outer_corner_x 2268 7 right_eye_outer_corner_y 2268 8 Image 7049 Epoch 0: loss - 0.02120, val_loss - 0.00348 Epoch 50: loss - 0.00123, val_loss - 0.00081 Epoch 100: loss - 0.00098, val_loss - 0.00074 Epoch 150: loss - 0.00094, val_loss - 0.00073 Epoch 200: loss - 0.00094, val_loss - 0.00068 Epoch 250: loss - 0.00095, val_loss - 0.00070 Epoch 300: loss - 0.00089, val_loss - 0.00069 Epoch 350: loss - 0.00103, val_loss - 0.00070 Epoch 400: loss - 0.00090, val_loss - 0.00068 Epoch 450: loss - 0.00092, val_loss - 0.00070 index 0 0 left_eyebrow_inner_end_x 2270 1 left_eyebrow_inner_end_y 2270 2 right_eyebrow_inner_end_x 2270 3 right_eyebrow_inner_end_y 2270 4 left_eyebrow_outer_end_x 2225 5 left_eyebrow_outer_end_y 2225 6 right_eyebrow_outer_end_x 2236 7 right_eyebrow_outer_end_y 2236 8 Image 7049 Epoch 0: loss - 0.04013, val_loss - 0.00667 Epoch 50: loss - 0.00195, val_loss - 0.00117 Epoch 100: loss - 0.00186, val_loss - 0.00113 Epoch 150: loss - 0.00166, val_loss - 0.00111 Epoch 200: loss - 0.00185, val_loss - 0.00118 Epoch 250: loss - 0.00164, val_loss - 0.00108 Epoch 300: loss - 0.00160, val_loss - 0.00106 Epoch 350: loss - 0.00166, val_loss - 0.00106 Epoch 400: loss - 0.00167, val_loss - 0.00104 Epoch 450: loss - 0.00170, val_loss - 0.00108 CPU times: user 1h 2min 46s, sys: 5min 11s, total: 1h 7min 57s Wall time: 48min 20s plot validation losses of each specialist model You might think that the model performance is relatively poor especially for the nose tip models, and month center models, because their losses are above 0.03. However, you should not compare these validation losses with the validation losses from model 4, because the data size has increased almost 4 times for these data. In [218]: def plot_specialist ( specialists1 , plt ): i = 1 for key , value in specialists1 . items (): plot_loss ( value [ \"hist\" ], key [ 0 ] + str ( len ( key )), plt ) i += 1 plt . legend () plt . grid () plt . set_yscale ( \"log\" ) plt . set_xlabel ( \"epoch\" ) plt . set_ylabel ( \"loss\" ) fig = plt . figure ( figsize = ( 10 , 10 )) ax = fig . add_subplot ( 1 , 1 , 1 ) ax . set_ylim ( 7 ** ( - 4 ), 10 ** ( - 2 )) plot_specialist ( specialists1 , ax ) plt . show () Where do I stand in the kaggle competition? We predict the landmarks of testing set, using model4 and specialist model. In [201]: from pandas import DataFrame , concat X_test , _ = load2d ( test = True ) ## prediction with model 4 y_pred4 = model4 . predict ( X_test ) landmark_nm = read_csv ( os . path . expanduser ( FTRAIN )) . columns [: - 1 ] . values df_y_pred4 = DataFrame ( y_pred4 , columns = landmark_nm ) ## prediction with specialist model def predict_specialist ( specialists1 , X_test ): y_pred_s = [] for columns , value in specialists1 . items (): smodel = value [ \"model\" ] y_pred = smodel . predict ( X_test ) y_pred = DataFrame ( y_pred , columns = columns ) y_pred_s . append ( y_pred ) df_y_pred_s = concat ( y_pred_s , axis = 1 ) return ( df_y_pred_s ) df_y_pred_s = predict_specialist ( specialists1 , X_test ) y_pred_s = df_y_pred_s . values index 0 0 ImageId 1783 1 Image 1783 Create .csv files to submit to keggle (Late submission) In [143]: IdLookup = read_csv ( os . path . expanduser ( FIdLookup )) def prepare_submission ( y_pred4 , filename ): ''' save a .csv file that can be submitted to kaggle ''' ImageId = IdLookup [ \"ImageId\" ] FeatureName = IdLookup [ \"FeatureName\" ] RowId = IdLookup [ \"RowId\" ] submit = [] for rowId , irow , landmark in zip ( RowId , ImageId , FeatureName ): submit . append ([ rowId , y_pred4 [ landmark ] . iloc [ irow - 1 ]]) submit = DataFrame ( submit , columns = [ \"RowId\" , \"Location\" ]) ## adjust the scale submit [ \"Location\" ] = submit [ \"Location\" ] * 48 + 48 print ( submit . shape ) loc = \"result/\" + filename + \".csv\" submit . to_csv ( loc , index = False ) print ( \"File is saved at:\" + loc ) prepare_submission ( df_y_pred4 , \"model4\" ) prepare_submission ( df_y_pred_s , \"special\" ) (27124, 2) File is saved at:result/model4.csv (27124, 2) File is saved at:result/special.csv kaggle Results model4 scores 2.86 (Private score) and 2.93 (Public score) special scores 2.26819 (Private score) and 2.53439 (Public score) This means that the special model rank top 50 out of 175 teams. i.e., top 30%! at 2018, January 4th. In [175]: ## reorder the columns of df_y_pred_s df_y_pred_s = df_y_pred_s [ df_y_pred4 . columns ] df_compare = {} df_compare [ \"difference\" ] = (( df_y_pred_s - df_y_pred4 ) ** 2 ) . mean ( axis = 1 ) df_compare [ \"RowId\" ] = range ( df_y_pred_s . shape [ 0 ]) df_compare = DataFrame ( df_compare ) df_compare = df_compare . sort_values ( \"difference\" , ascending = False ) Intuitively, when landmarks are harder to detect, the two model performances are different. Therefore it makes sense to learn about the frames where the two models detections are similar/desimilar. Plot the model performance of the best 13 and worst 13 pictures. \"best\" - The two model results agree the most. \"worst\" - The two model results disgree the most. In [192]: fig = plt . figure ( figsize = ( 12 , 35 )) Nsample = 13 pic_index = df_compare [ \"RowId\" ] . iloc [: Nsample ] . values pic_index_good = df_compare [ \"RowId\" ] . iloc [ - Nsample :] . values count = 1 for ipic_g , ipic in zip ( pic_index_good , pic_index ): ## bad model 4 ax = fig . add_subplot ( Nsample , 4 , count , xticks = [], yticks = []) count += 1 plot_sample ( X_test [ ipic_g ], y_pred4 [ ipic_g ], ax ) ax . set_title ( \"Good:model4:pic\" + str ( ipic_g )) ## bad special ax = fig . add_subplot ( Nsample , 4 , count , xticks = [], yticks = []) count += 1 plot_sample ( X_test [ ipic_g ], y_pred_s [ ipic_g ], ax ) ax . set_title ( \"Good:special:pic\" + str ( ipic_g )) ## bad model 4 ax = fig . add_subplot ( Nsample , 4 , count , xticks = [], yticks = []) count += 1 plot_sample ( X_test [ ipic ], y_pred4 [ ipic ], ax ) ax . set_title ( \"Bad:model4:pic\" + str ( ipic )) ## bad special ax = fig . add_subplot ( Nsample , 4 , count , xticks = [], yticks = []) count += 1 plot_sample ( X_test [ ipic ], y_pred_s [ ipic ], ax ) ax . set_title ( \"Bad:special:pic\" + str ( ipic )) plt . show () Specialist model revisited From the plots above, it is clear that the model performances are good when the faces are located at the center and facing front. The model performances are poor when the faces are looking side or not at the center. So the next step to improve the model prediction performance would be to 1: increase the shifting proportion (to improve the model performance of pic750) 2: use the weights from the transfer model only as the initial weights and re-train all the weights. 3: augment the data to include more data with different shear angle (to improve the model performance of pic1234 or pic962) with different zooming (to improve the model performance of pic1064) We will only conider the 1st and 2nd points. Unexpectedly, we actually do not need too many epochs for the model to converge even when the weights are not frozen! In [198]: %% time specialists2 = fit_specialists(freeze=False, print_every=50, epochs=300, prop=0.2, name_transfer_model=\"model4\") index 0 0 left_eye_center_x 7039 1 left_eye_center_y 7039 2 right_eye_center_x 7036 3 right_eye_center_y 7036 4 Image 7049 Epoch 0: loss - 0.05922, val_loss - 0.00636 Epoch 50: loss - 0.00197, val_loss - 0.00240 Epoch 100: loss - 0.00178, val_loss - 0.00312 Epoch 150: loss - 0.00123, val_loss - 0.00299 Epoch 200: loss - 0.00125, val_loss - 0.00262 Epoch 250: loss - 0.00105, val_loss - 0.00263 index 0 0 nose_tip_x 7049 1 nose_tip_y 7049 2 Image 7049 Epoch 0: loss - 0.08784, val_loss - 0.01316 Epoch 50: loss - 0.00389, val_loss - 0.00341 Epoch 100: loss - 0.00291, val_loss - 0.00262 Epoch 150: loss - 0.00264, val_loss - 0.00270 Epoch 200: loss - 0.00229, val_loss - 0.00236 Epoch 250: loss - 0.00219, val_loss - 0.00223 index 0 0 mouth_left_corner_x 2269 1 mouth_left_corner_y 2269 2 mouth_right_corner_x 2270 3 mouth_right_corner_y 2270 4 mouth_center_top_lip_x 2275 5 mouth_center_top_lip_y 2275 6 Image 7049 Epoch 0: loss - 0.08622, val_loss - 0.01097 Epoch 50: loss - 0.00209, val_loss - 0.00181 Epoch 100: loss - 0.00149, val_loss - 0.00173 Epoch 150: loss - 0.00124, val_loss - 0.00140 Epoch 200: loss - 0.00104, val_loss - 0.00118 Epoch 250: loss - 0.00087, val_loss - 0.00118 index 0 0 mouth_center_bottom_lip_x 7016 1 mouth_center_bottom_lip_y 7016 2 Image 7049 Epoch 0: loss - 0.08083, val_loss - 0.00932 Epoch 50: loss - 0.00308, val_loss - 0.00310 Epoch 100: loss - 0.00259, val_loss - 0.00326 Epoch 150: loss - 0.00209, val_loss - 0.00311 Epoch 200: loss - 0.00196, val_loss - 0.00299 Epoch 250: loss - 0.00211, val_loss - 0.00299 index 0 0 left_eye_inner_corner_x 2271 1 left_eye_inner_corner_y 2271 2 right_eye_inner_corner_x 2268 3 right_eye_inner_corner_y 2268 4 left_eye_outer_corner_x 2267 5 left_eye_outer_corner_y 2267 6 right_eye_outer_corner_x 2268 7 right_eye_outer_corner_y 2268 8 Image 7049 Epoch 0: loss - 0.05546, val_loss - 0.00585 Epoch 50: loss - 0.00107, val_loss - 0.00089 Epoch 100: loss - 0.00075, val_loss - 0.00071 Epoch 150: loss - 0.00083, val_loss - 0.00084 Epoch 200: loss - 0.00056, val_loss - 0.00058 Epoch 250: loss - 0.00050, val_loss - 0.00060 index 0 0 left_eyebrow_inner_end_x 2270 1 left_eyebrow_inner_end_y 2270 2 right_eyebrow_inner_end_x 2270 3 right_eyebrow_inner_end_y 2270 4 left_eyebrow_outer_end_x 2225 5 left_eyebrow_outer_end_y 2225 6 right_eyebrow_outer_end_x 2236 7 right_eyebrow_outer_end_y 2236 8 Image 7049 Epoch 0: loss - 0.06381, val_loss - 0.01177 Epoch 50: loss - 0.00224, val_loss - 0.00147 Epoch 100: loss - 0.00140, val_loss - 0.00133 Epoch 150: loss - 0.00117, val_loss - 0.00122 Epoch 200: loss - 0.00097, val_loss - 0.00108 Epoch 250: loss - 0.00092, val_loss - 0.00107 CPU times: user 59min 53s, sys: 5min 59s, total: 1h 5min 52s Wall time: 53min 44s In [222]: ylim = ( 7 ** ( - 4 ), 10 ** ( - 2 )) fig = plt . figure ( figsize = ( 20 , 10 )) ax = fig . add_subplot ( 1 , 2 , 1 ) ax . set_ylim ( ylim ) ax . set_title ( \"specialist 1\" ) plot_specialist ( specialists1 , ax ) ax = fig . add_subplot ( 1 , 2 , 2 ) ax . set_ylim ( ylim ) ax . set_title ( \"specialist 2\" ) plot_specialist ( specialists2 , ax ) plt . show () Kaggle results I am proud to say that this specialist model 2 achieves: private score 1.71181 public score 2.04064 top 40 out of 175 teams ( top 23% ) Pretty good! In [223]: df_y_pred_s2 = predict_specialist ( specialists2 , X_test ) prepare_submission ( df_y_pred_s2 , \"special2\" ) (27124, 2) File is saved at:result/special2.csv finally I look at the model performances of specialist 2 in the worst performing 23 pictures as before. Notice that the model performance of the pic 750, pic 962 and many others with off-the-center faces substantially improved. In [233]: fig = plt . figure ( figsize = ( 7 , 35 )) y_pred_s2 = df_y_pred_s2 . values Nsample = 13 pic_index = df_compare [ \"RowId\" ] . iloc [: Nsample ] . values pic_index_good = df_compare [ \"RowId\" ] . iloc [ - Nsample :] . values count = 1 for ipic_g , ipic in zip ( pic_index_good , pic_index ): ax = fig . add_subplot ( Nsample , 2 , count , xticks = [], yticks = []) count += 1 plot_sample ( X_test [ ipic ], y_pred_s [ ipic ], ax ) ax . set_title ( \"Bad:special1:pic\" + str ( ipic )) ax = fig . add_subplot ( Nsample , 2 , count , xticks = [], yticks = []) count += 1 plot_sample ( X_test [ ipic ], y_pred_s2 [ ipic ], ax ) ax . set_title ( \"Bad:special2:pic\" + str ( ipic )) plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Achieving Top 23% in Kaggle's Facial Keypoints Detection with Keras + Tensorflow"},{"url":"Learn-about-ImageDataGenerator.html","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } Goal: learn ImagedataGenerator This script shows randomly generated images using various values of ImagedataGenerator from keras.preprocessing.image Reference The Keras Blog Keras Documentations Read in the function that read in the original image, generate manuplated images and save them in a specified folder, In [119]: import os from keras.preprocessing.image import ImageDataGenerator , img_to_array , load_img def generate_plot_pics ( datagen , orig_img ): dir_augmented_data = \"data/preview\" try : ## if the preview folder does not exist, create os . mkdir ( dir_augmented_data ) except : ## if the preview folder exists, then remove ## the contents (pictures) in the folder for item in os . listdir ( dir_augmented_data ): os . remove ( dir_augmented_data + \"/\" + item ) ## convert the original image to array x = img_to_array ( orig_img ) ## reshape (Sampke, Nrow, Ncol, 3) 3 = R, G or B x = x . reshape (( 1 ,) + x . shape ) ## -------------------------- ## ## randomly generate pictures ## -------------------------- ## i = 0 Nplot = 8 for batch in datagen . flow ( x , batch_size = 1 , save_to_dir = dir_augmented_data , save_prefix = \"pic\" , save_format = 'jpeg' ): i += 1 if i > Nplot - 1 : ## generate 8 pictures break ## -------------------------- ## ## plot the generated data ## -------------------------- ## fig = plt . figure ( figsize = ( 8 , 6 )) fig . subplots_adjust ( hspace = 0.02 , wspace = 0.01 , left = 0 , right = 1 , bottom = 0 , top = 1 ) ## original picture ax = fig . add_subplot ( 3 , 3 , 1 , xticks = [], yticks = []) ax . imshow ( orig_img ) ax . set_title ( \"original\" ) i = 2 for imgnm in os . listdir ( dir_augmented_data ): ax = fig . add_subplot ( 3 , 3 , i , xticks = [], yticks = []) img = load_img ( dir_augmented_data + \"/\" + imgnm ) ax . imshow ( img ) i += 1 plt . show () We will use Taylor Swift's picture as an original picture In [118]: orig_img = load_img ( \"data/TAYLOR-SWIFT.jpg\" ) In [120]: ## rotation_range: Int. Degree range for random rotations. datagen = ImageDataGenerator ( rotation_range = 180 ) generate_plot_pics ( datagen , orig_img ) In [127]: ## rotation_range: Int. Degree range for random rotations. datagen = ImageDataGenerator ( rotation_range = 20 ) generate_plot_pics ( datagen , orig_img ) In [130]: ## width_shift_range: Float (fraction of total width). Range for random horizontal shifts. datagen = ImageDataGenerator ( width_shift_range = 1 ) generate_plot_pics ( datagen , orig_img ) In [132]: datagen = ImageDataGenerator ( width_shift_range = 0.25 ) generate_plot_pics ( datagen , orig_img ) In [141]: ## height_shift_range: Float (fraction of total height). Range for random vertical shifts. ## fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}. Points outside the boundaries of the input are filled according to the given mode: ## \"constant\": kkkkkkkk|abcd|kkkkkkkk (cval=k) ## \"nearest\": aaaaaaaa|abcd|dddddddd ## \"reflect\": abcddcba|abcd|dcbaabcd ## \"wrap\": abcdabcd|abcd|abcdabcd datagen = ImageDataGenerator ( height_shift_range = 0.2 , fill_mode = \"constant\" ) generate_plot_pics ( datagen , orig_img ) In [142]: datagen = ImageDataGenerator ( height_shift_range = 0.2 , fill_mode = \"nearest\" ) generate_plot_pics ( datagen , orig_img ) In [143]: datagen = ImageDataGenerator ( height_shift_range = 0.2 , fill_mode = \"reflect\" ) generate_plot_pics ( datagen , orig_img ) In [144]: datagen = ImageDataGenerator ( height_shift_range = 0.2 , fill_mode = \"wrap\" ) generate_plot_pics ( datagen , orig_img ) In [124]: ## shear_range: Float. Shear Intensity (Shear angle in counter-clockwise direction as radians) datagen = ImageDataGenerator ( shear_range = 0.2 ) generate_plot_pics ( datagen , orig_img ) In [125]: ## zoom_range: Float or [lower, upper]. Range for random zoom. ## If a float, [lower, upper] = [1-zoom_range, 1+zoom_range]. datagen = ImageDataGenerator ( zoom_range = 0.2 ) generate_plot_pics ( datagen , orig_img ) In [133]: datagen = ImageDataGenerator ( horizontal_flip = True ) generate_plot_pics ( datagen , orig_img ) In [145]: datagen = ImageDataGenerator ( vertical_flip = True ) generate_plot_pics ( datagen , orig_img ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Blog","title":"Learn about ImageDataGenerator"}]}